{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "from math import sqrt\n",
    "import matplotlib.pyplot as plt\n",
    "import random\n",
    "import env_sim_complete_params2\n",
    "import env6\n",
    "import os\n",
    "import numpy as np\n",
    "import torch\n",
    "from torchsummary import summary\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from copy import deepcopy\n",
    "\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "plt.ioff()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ExperienceDataset(Dataset):\n",
    "    def __init__(self, data):\n",
    "        super().__init__()\n",
    "        self.data = data\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data[0])\n",
    "\n",
    "    def __getitem__(self, ind):\n",
    "        return tuple(map(lambda t: t[ind], self.data))\n",
    "\n",
    "def create_shuffled_dataloader(data, batch_size):\n",
    "    ds = ExperienceDataset(data)\n",
    "    return DataLoader(ds, batch_size = batch_size, shuffle = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PolicyNetwork(torch.nn.Module):\n",
    "    def __init__(self,state_dims, n_actions):\n",
    "        super(PolicyNetwork, self).__init__()\n",
    "        self.actor = torch.nn.Sequential(\n",
    "            torch.nn.Linear(state_dims, 64),\n",
    "            torch.nn.Tanh(),\n",
    "            torch.nn.Linear(64, 32),\n",
    "            torch.nn.Tanh(),\n",
    "            torch.nn.Linear(32, n_actions),\n",
    "            torch.nn.Softmax(dim=-1)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        policy = self.actor(x)\n",
    "        return policy\n",
    "    \n",
    "\n",
    "class ValueNetwork(torch.nn.Module):\n",
    "    def __init__(self,state_dims):\n",
    "        super(ValueNetwork, self).__init__()\n",
    "        self.critic = torch.nn.Sequential(\n",
    "            torch.nn.Linear(state_dims, 64),\n",
    "            torch.nn.Tanh(),\n",
    "            torch.nn.Linear(64, 32),\n",
    "            torch.nn.Tanh(),\n",
    "            torch.nn.Linear(32, 1)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        value = self.critic(x)\n",
    "        return value\n",
    "        \n",
    "def weights_init(m):\n",
    "    if isinstance(m,torch.nn.Linear):\n",
    "        m.bias.data.fill_(0)\n",
    "        torch.nn.init.kaiming_uniform_(m.weight)\n",
    "\n",
    "def np_to_tensor(x):\n",
    "    return torch.tensor(x).to(torch.float32)\n",
    "\n",
    "    \n",
    "policy_model = PolicyNetwork(state_dims=10, n_actions = 10)\n",
    "value_model = ValueNetwork(state_dims=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(policy_model,value_model)\n",
    "summary(policy_model, input_size=(1, 10))\n",
    "summary(value_model, input_size=(1, 10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PPO:\n",
    "    def __init__(self, state_dim=6, action_dim=6, policy_lr=0.0005, value_lr=0.001, \n",
    "                 gamma=0.99, lam=0.95, beta_s=0.01,epsilon_clip=0.2, value_clip=0.4, \n",
    "                 epochs=5,batch_size=50,save_path='Models',env=None):\n",
    "        \n",
    "        self.policy_net = PolicyNetwork(state_dim, action_dim).to(device)\n",
    "        self.value_net = ValueNetwork(state_dim).to(device)\n",
    "        self.policy_net.apply(weights_init)\n",
    "        self.value_net.apply(weights_init)\n",
    "\n",
    "        self.policy_optimizer = torch.optim.Adam(self.policy_net.parameters(), lr=policy_lr)\n",
    "        self.value_optimizer = torch.optim.Adam(self.value_net.parameters(), lr=value_lr)\n",
    "\n",
    "        self.gamma = gamma\n",
    "        self.lamda = lam\n",
    "        self.epsilon_clip = epsilon_clip\n",
    "        self.value_clip = value_clip\n",
    "        self.beta_s = beta_s\n",
    "        self.epochs = epochs\n",
    "        self.action_dim = action_dim\n",
    "        self.batch_size = batch_size\n",
    "\n",
    "        self.env = env\n",
    "        self.env.run()\n",
    "\n",
    "        self.save_path = save_path\n",
    "        self.env_name = \"auto-docking-iss\"\n",
    "\n",
    "        if not os.path.exists(self.save_path): os.makedirs(self.save_path)\n",
    "        self.path = str(self.env_name)+'_PPO_'\n",
    "        self.model_name = os.path.join(self.save_path, self.path)\n",
    "\n",
    "        self.trajectory = {'states':[],'actions':[], 'rewards':[], 'action_logs':[],\n",
    "                           'values':[],'done':[]}\n",
    "        \n",
    "        self.scores, self.episodes, self.average = [],[],[]\n",
    "        self.value_loss_mean, self.policy_loss_mean = [], []\n",
    "        self.rewards,self.average_reward = [],[]\n",
    "\n",
    "    def save(self,episode,score):\n",
    "        torch.save({\n",
    "            'actor': self.policy_net.state_dict(),\n",
    "            'critic': self.value_net.state_dict()\n",
    "        }, f'./'+ self.model_name + '_' + str(episode) + '_' + str(round(score,4)) + '.pt')\n",
    "\n",
    "    def load(self,name):\n",
    "        print(f'./{name}')\n",
    "        try:\n",
    "            data = torch.load(f'./{name}',weights_only=True)\n",
    "            self.policy_net.load_state_dict(data['actor'])\n",
    "            self.value_net.load_state_dict(data['critic'])\n",
    "        except Exception as e:\n",
    "            print(e)\n",
    "\n",
    "    # Función para visualizar la evolución del entrenamiento\n",
    "    def plot_model(self, score, episode,num_episodes):\n",
    "        self.scores.append(score)\n",
    "        self.episodes.append(episode)\n",
    "        self.average.append(sum(self.scores[-10:]) / len(self.scores[-10:]))\n",
    "        if (episode % 10 == 0 and episode > 0) or episode == num_episodes:#str(episode)[-2:] == \"00\":# much faster than episode % 100\n",
    "            plt.figure(1,figsize=(10, 5))\n",
    "            plt.plot(self.episodes, self.scores, 'b')\n",
    "            plt.plot(self.episodes, self.average, 'r')\n",
    "            plt.ylabel('Score', fontsize=12)\n",
    "            plt.xlabel('Steps', fontsize=12)\n",
    "            try:\n",
    "                plt.savefig(self.model_name+\"scores.png\")\n",
    "            except OSError:\n",
    "                pass\n",
    "\n",
    "        return self.average[-1]\n",
    "\n",
    "    def plot_reward(self,reward,episode,num_episodes):\n",
    "        self.rewards.append(reward)\n",
    "        self.average_reward.append(sum(self.rewards[-10:]) / len(self.rewards[-10:]))     \n",
    "        if (episode % 10 == 0 and episode > 0) or episode == num_episodes:#str(episode)[-2:] == \"00\":# much faster than episode % 100\n",
    "            plt.figure(2,figsize=(10, 5))\n",
    "            plt.plot(self.episodes, self.rewards, 'b')\n",
    "            plt.plot(self.episodes, self.average_reward, 'r')\n",
    "            plt.ylabel('Reward', fontsize=12)\n",
    "            plt.xlabel('Steps', fontsize=12)\n",
    "            try:\n",
    "                plt.savefig(self.model_name+\"last-state-reward.png\")\n",
    "            except OSError:\n",
    "                pass\n",
    "\n",
    "    \n",
    "    def plot_loss(self,episode,num_episodes):        \n",
    "        if (episode % 10 == 0 and episode > 0) or episode == num_episodes:#str(episode)[-2:] == \"00\":# much faster than episode % 100\n",
    "            plt.figure(3,figsize=(10, 5))\n",
    "            plt.plot(self.episodes, self.policy_loss_mean, 'b')\n",
    "            plt.plot(self.episodes, self.value_loss_mean, 'r')\n",
    "            plt.ylabel('Loss', fontsize=12)\n",
    "            plt.xlabel('Steps', fontsize=12)\n",
    "            try:\n",
    "                plt.savefig(self.model_name+\"loss.png\")\n",
    "            except OSError:\n",
    "                pass\n",
    "    \n",
    "    def save_data_to_file(self,data,file):\n",
    "        with open(self.model_name+file, 'a') as archivo:\n",
    "            archivo.write(data)\n",
    "\n",
    "    #Almacenamiento de todas las variables que definen una transición para PPO\n",
    "    def collect_trayectory(self,state,action,action_logs,reward,done,value):\n",
    "        self.trajectory['states'].append(state)\n",
    "        self.trajectory['actions'].append(action)\n",
    "        self.trajectory['rewards'].append(reward)\n",
    "        self.trajectory['values'].append(value)\n",
    "        self.trajectory['action_logs'].append(action_logs)\n",
    "        self.trajectory['done'].append(done)\n",
    "    \n",
    "    def clear_trayectory(self):\n",
    "        for key in self.trajectory.keys():\n",
    "            self.trajectory[key].clear()\n",
    "    \n",
    "    def select_action(self,state):\n",
    "        action_probs = self.policy_net(state).squeeze().detach()\n",
    "        dist = torch.distributions.Categorical(action_probs)\n",
    "        action = dist.sample()\n",
    "        action_log_probs = dist.log_prob(action)\n",
    "        \n",
    "        return action.item(),action_log_probs,dist.entropy().item()\n",
    "\n",
    "    \n",
    "    def generalized_advantage_estimation(self,next_state):\n",
    "        advantages = np.zeros_like(self.trajectory['rewards'])\n",
    "        last_gae_lambda = 0\n",
    "        self.trajectory['values'].append(self.value_net(next_state).squeeze().detach())\n",
    "\n",
    "        for t in reversed(range(len(self.trajectory['rewards']))):\n",
    "            delta = self.trajectory['rewards'][t]+ self.gamma * self.trajectory['values'][t + 1].item() - self.trajectory['values'][t].item()\n",
    "            advantages[t] = last_gae_lambda = delta + self.gamma * self.lamda * last_gae_lambda\n",
    "        \n",
    "        return np_to_tensor(advantages)\n",
    "\n",
    "\n",
    "    def learn_from_episode_trajectory(self,next_state):\n",
    "        self.policy_net.train()\n",
    "        self.value_net.train()\n",
    "\n",
    "        states = deepcopy(self.trajectory['states'])\n",
    "        actions = deepcopy(self.trajectory['actions'])\n",
    "        # rewards = deepcopy(self.trajectory['rewards'])\n",
    "        values = deepcopy(self.trajectory['values'])\n",
    "        action_logs = deepcopy(self.trajectory['action_logs'])\n",
    "        # done = deepcopy(self.trajectory['done'])\n",
    "\n",
    "        advantages = self.generalized_advantage_estimation(next_state).to(device)\n",
    "\n",
    "        # prepare dataloader for policy phase training\n",
    "        dl = create_shuffled_dataloader([states, actions, action_logs, advantages, values], self.batch_size)\n",
    "\n",
    "        policy_loss_array = []\n",
    "        value_loss_array = []\n",
    "\n",
    "        for i in range(self.epochs):\n",
    "            for states, actions, action_log, advantages, values in dl:\n",
    "                #Update policy_net\n",
    "                new_action_probs = self.policy_net(states).squeeze()\n",
    "                dist = torch.distributions.Categorical(new_action_probs)\n",
    "\n",
    "                new_action_logs = dist.log_prob(actions)\n",
    "                entropy = dist.entropy()\n",
    "\n",
    "                # compute PPO-Clip\n",
    "                ratios = torch.exp(new_action_logs - action_log)\n",
    "                surr1 = ratios * advantages\n",
    "                surr2 = ratios.clamp(1 - self.epsilon_clip, 1 + self.epsilon_clip) * advantages\n",
    "                policy_loss = -torch.mean(torch.min(surr1, surr2) - self.beta_s * entropy)\n",
    "                policy_loss_array.append(policy_loss.detach().numpy())\n",
    "\n",
    "                self.policy_optimizer.zero_grad() # Clean gradients\n",
    "                policy_loss.mean().backward() # Compute gradients\n",
    "                self.policy_optimizer.step() #Update weights\n",
    "\n",
    "                #Update value_net\n",
    "                new_values = self.value_net(states).squeeze()\n",
    "                value_clipped = values + (new_values - values).clamp(-self.value_clip, self.value_clip)\n",
    "                value_loss_1 = (value_clipped.flatten() - advantages) ** 2\n",
    "                value_loss_2 = (values.flatten() - advantages) ** 2\n",
    "                value_loss = torch.mean(torch.max(value_loss_1, value_loss_2))\n",
    "                value_loss_array.append(value_loss.detach().numpy())\n",
    "\n",
    "                self.value_optimizer.zero_grad() # Clean gradients\n",
    "                value_loss.backward() # Compute gradients \n",
    "                self.value_optimizer.step() #Update weights\n",
    "\n",
    "        self.value_loss_mean.append(sum(value_loss_array)/self.epochs)\n",
    "        self.policy_loss_mean.append(sum(policy_loss_array)/self.epochs)\n",
    "\n",
    "        # print(f\"\\n Mean Policy loss: {self.value_loss_mean[-1]:.4f}. Mean Value loss: {self.policy_loss_mean[-1]:.4f}\")\n",
    "        \n",
    "        self.policy_net.eval()\n",
    "        self.value_net.eval()\n",
    "\n",
    "\n",
    "    # Función para resetear el entorno tras acabar trayectoria\n",
    "    def reset(self):\n",
    "        state = self.env.reset()\n",
    "        return np_to_tensor(self.process_state(state))\n",
    "    \n",
    "     # Función para interacción agente-entorno\n",
    "    def step(self, action,step,max_steps):\n",
    "        next_state, reward, done = self.env.step(action,step,max_steps)\n",
    "        next_state = self.process_state(next_state)\n",
    "        return np_to_tensor(next_state), reward, done\n",
    "\n",
    "    def process_state(self,state):\n",
    "        state_processed = np.array(state[1:3]+state[4:])\n",
    "        state_processed[0] = state_processed[0] / 1.0 # y\n",
    "        state_processed[1] = state_processed[1] / 1.0 # z\n",
    "        state_processed[4] = state_processed[4] / 1.0 # roll\n",
    "        state_processed[5] = state_processed[5] / 1.0 # pith\n",
    "        state_processed[6] = state_processed[6] / 1.0 # yaw\n",
    "        return state_processed\n",
    "    \n",
    "    def train(self,init_episode=1,num_episodes=200,max_steps=200,dt=0.01):\n",
    "        average = 0\n",
    "        self.policy_net.eval()\n",
    "        self.value_net.eval()\n",
    "\n",
    "        max_score_average = 1000\n",
    "\n",
    "        num_episodes += init_episode\n",
    "\n",
    "        for episode in range(init_episode,num_episodes):\n",
    "            self.clear_trayectory()\n",
    "\n",
    "            state = self.reset()\n",
    "            score = 0\n",
    "\n",
    "            SAVING = \"\"\n",
    "\n",
    "            for step in range(1,max_steps+1):\n",
    "                action_probs = self.policy_net(state).squeeze().detach()\n",
    "                dist = torch.distributions.Categorical(action_probs)\n",
    "                action = dist.sample()\n",
    "                action_log = dist.log_prob(action)\n",
    "\n",
    "                value = self.value_net(state).squeeze().detach()\n",
    "                \n",
    "                next_state,reward,done = self.step(action.item()+2,step,max_steps)    \n",
    "                self.collect_trayectory(state,action,action_log,reward,done,value)\n",
    "\n",
    "                state = next_state\n",
    "\n",
    "                score += reward\n",
    "\n",
    "                # print(state)\n",
    "                # print(f'\\rSteps: {step}/{max_steps}. Action: {action}. Reward/Value: {round(reward,2)}/{round(value.item(),4)}. Done: {done}', end='', flush=True)\n",
    "\n",
    "                if done:\n",
    "                    break\n",
    "                \n",
    "                # time.sleep(dt)\n",
    "\n",
    "            self.learn_from_episode_trajectory(next_state)\n",
    "            self.clear_trayectory()\n",
    "\n",
    "            average = self.plot_model(score, episode,num_episodes)\n",
    "            self.plot_loss(episode,num_episodes)\n",
    "            #self.plot_reward(reward,episode,num_episodes)\n",
    "            \n",
    "            if episode > 40 and average >= max_score_average:\n",
    "                max_score_average = average\n",
    "                self.save(episode,reward)\n",
    "                SAVING = \"SAVING\"\n",
    "            # elif episode % 10 == 0:\n",
    "            #     self.save(episode,average)\n",
    "            #     SAVING = \"SAVING\"\n",
    "            else:\n",
    "                SAVING = \"\"\n",
    "            \n",
    "            # print(f\"\\rEpisode: {episode}/{num_episodes-1}. Steps: {step}/{max_steps}. Score/Average/reward: {score:.2f}/{average:.2f}/{reward:.2f} {SAVING}      \", end='', flush=True)\n",
    "            s_print =  np.around(self.env.state[1:3]+self.env.state[6:9],4).tolist()\n",
    "            print(f\"\\rEpisode: {episode}/{num_episodes-1}. Steps: {step}/{max_steps}. Score/Average: {score:.2f}/{average:.2f}. Done: {done}.\\t State: {s_print}      \", end='', flush=True)\n",
    "\n",
    "            self.save_data_to_file(file='train.txt',data=f\"Episode: {episode}/{num_episodes-1}. Steps: {step}/{max_steps}. Score/Average: {score:.2f}/{average:.2f}. Done: {done}.\\t State: {s_print}\\n\")\n",
    "\n",
    "\n",
    "        # close environemnt when finish training\n",
    "        self.save(episode,average)\n",
    "\n",
    "    def test(self,max_steps):\n",
    "        self.policy_net.eval()\n",
    "        self.value_net.eval()\n",
    "\n",
    "        self.clear_trayectory()\n",
    "        state = self.reset()\n",
    "        score = 0\n",
    "\n",
    "        # y,z,roll,pitch,yaw = [],[],[],[],[]\n",
    "\n",
    "        for step in range(1,max_steps+1):\n",
    "            action_probabilities = self.policy_net(state)\n",
    "            action = torch.argmax(action_probabilities, dim=-1).item()\n",
    "\n",
    "            next_state,reward,done = self.step(action+2,step,max_steps) \n",
    "\n",
    "            # y.append(self.env.state[1])\n",
    "            # z.append(self.env.state[2])\n",
    "            # roll.append(self.env.state[6])\n",
    "            # pitch.append(self.env.state[7])\n",
    "            # yaw.append(self.env.state[8])\n",
    "            \n",
    "            state = next_state\n",
    "            score += reward\n",
    "\n",
    "            if done:\n",
    "                break\n",
    "\n",
    "            #time.sleep(dt)\n",
    "        # plt.figure(4,figsize=(10, 5))\n",
    "        # plt.plot([x for x in range(len(y))], y, 'b')\n",
    "        # plt.plot([x for x in range(len(z))], z, 'r')\n",
    "        # plt.plot([x for x in range(len(z))], roll, 'g')\n",
    "        # plt.plot([x for x in range(len(z))], pitch, 'y')\n",
    "        # plt.plot([x for x in range(len(z))], yaw, 'k')\n",
    "        # plt.ylabel('Errors', fontsize=12)\n",
    "        # plt.xlabel('Steps', fontsize=12)\n",
    "        \n",
    "        print(f'\\rSteps: {step}/{max_steps}. Score/reward: {score:.2f}/{reward:.2f}. Done: {done}       ', end='', flush=True)\n",
    "\n",
    "        return done"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Experiment 1 - No terminal reward\n",
    "dt = 0.2\n",
    "max_steps = 500\n",
    "max_state = [30.0,10.0,10.0,10.0,10.0,10.0]\n",
    "objective_state = [0.2,0.2,0.2,0.2,0.2,0.2]\n",
    "range_state = [[15,20],6.0,6.0,6.0,6.0,6.0]\n",
    "reward_terminal = [-0.0,0.0]\n",
    "env = env_sim_complete_params2.Env(\n",
    "    max_state=max_state,\n",
    "    objective_state=objective_state,\n",
    "    range_state=range_state,\n",
    "    reward_terminal=reward_terminal,\n",
    "    dt=dt)\n",
    "agent = PPO(save_path='Models/Models16-4/Exp1',env=env,state_dim=10, action_dim=10)\n",
    "agent.train(init_episode=1,num_episodes=400,max_steps=int(max_steps/dt))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Experiment 2 - Terminal reward neg\n",
    "dt = 0.2\n",
    "max_steps = 500\n",
    "max_state = [30.0,10.0,10.0,10.0,10.0,10.0]\n",
    "objective_state = [0.2,0.2,0.2,0.2,0.2,0.2]\n",
    "range_state = [[15,20],6.0,6.0,6.0,6.0,6.0]\n",
    "reward_terminal = [-5000.0,0.0]\n",
    "env = env_sim_complete_params2.Env(\n",
    "    max_state=max_state,\n",
    "    objective_state=objective_state,\n",
    "    range_state=range_state,\n",
    "    reward_terminal=reward_terminal,\n",
    "    dt=dt)\n",
    "agent = PPO(save_path='Models/Models16-4/Exp2',env=env,state_dim=10, action_dim=10)\n",
    "agent.train(init_episode=1,num_episodes=400,max_steps=int(max_steps/dt))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Experiment 2 - Test\n",
    "dt = 0.2\n",
    "max_steps = 500\n",
    "max_state = [30.0,10.0,10.0,10.0,10.0,10.0]\n",
    "objective_state = [0.2,0.2,0.2,0.2,0.2,0.2]\n",
    "range_state = [[15,20],6.0,6.0,6.0,6.0,6.0]\n",
    "reward_terminal = [-5000.0,0.0]\n",
    "env = env_sim_complete_params2.Env(\n",
    "    max_state=max_state,\n",
    "    objective_state=objective_state,\n",
    "    range_state=range_state,\n",
    "    reward_terminal=reward_terminal,\n",
    "    dt=dt)\n",
    "agent = PPO(save_path='Models/Models16-4/Exp2',env=env,state_dim=10, action_dim=10)\n",
    "agent.load(\"Models/Models16-4/Exp2/auto-docking-iss_PPO__323_4.0998.pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "done = []\n",
    "for x in range(100):\n",
    "    done.append(agent.test(int(max_steps/dt)))\n",
    "\n",
    "ocurrencias = dict((i, done.count(i)) for i in done)\n",
    "print(\"\\n\",ocurrencias)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Experiment 3 - Terminal neg and pos reward\n",
    "dt = 0.2\n",
    "max_steps = 500\n",
    "max_state = [30.0,10.0,10.0,10.0,10.0,10.0]\n",
    "objective_state = [0.2,0.2,0.2,0.2,0.2,0.2]\n",
    "range_state = [[15,20],6.0,6.0,6.0,6.0,6.0]\n",
    "reward_terminal = [-5000.0,5000.0]\n",
    "env = env_sim_complete_params2.Env(\n",
    "    max_state=max_state,\n",
    "    objective_state=objective_state,\n",
    "    range_state=range_state,\n",
    "    reward_terminal=reward_terminal,\n",
    "    dt=dt)\n",
    "agent = PPO(save_path='Models/Models16-4/Exp3',env=env,state_dim=10, action_dim=10)\n",
    "agent.train(init_episode=1,num_episodes=400,max_steps=int(max_steps/dt))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "agent.train(init_episode=601,num_episodes=200,max_steps=int(max_steps/dt))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Experiment 3 - Test\n",
    "dt = 0.2\n",
    "max_steps = 500\n",
    "max_state = [30.0,10.0,10.0,10.0,10.0,10.0]\n",
    "objective_state = [0.2,0.2,0.2,0.2,0.2,0.2]\n",
    "range_state = [[15,20],6.0,6.0,6.0,6.0,6.0]\n",
    "reward_terminal = [-5000.0,5000.0]\n",
    "env = env_sim_complete_params2.Env(\n",
    "    max_state=max_state,\n",
    "    objective_state=objective_state,\n",
    "    range_state=range_state,\n",
    "    reward_terminal=reward_terminal,\n",
    "    dt=dt)\n",
    "agent = PPO(save_path='Models/Models16-4/Exp3',env=env,state_dim=10, action_dim=10)\n",
    "agent.load(\"Models/Models16-4/Exp3/auto-docking-iss_PPO__375_5003.8768.pt\")\n",
    "\n",
    "done = []\n",
    "for x in range(100):\n",
    "    done.append(agent.test(int(max_steps/dt)))\n",
    "\n",
    "ocurrencias = dict((i, done.count(i)) for i in done)\n",
    "print(\"\\n\",ocurrencias)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "miar_tfm",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "from math import sqrt\n",
    "import matplotlib.pyplot as plt\n",
    "import random\n",
    "from env6 import Env\n",
    "import os\n",
    "import numpy as np\n",
    "import torch\n",
    "from torchsummary import summary\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from copy import deepcopy\n",
    "\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ExperienceDataset(Dataset):\n",
    "    def __init__(self, data):\n",
    "        super().__init__()\n",
    "        self.data = data\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data[0])\n",
    "\n",
    "    def __getitem__(self, ind):\n",
    "        return tuple(map(lambda t: t[ind], self.data))\n",
    "\n",
    "def create_shuffled_dataloader(data, batch_size,shuffle=True):\n",
    "    ds = ExperienceDataset(data)\n",
    "    return DataLoader(ds, batch_size = batch_size, shuffle = shuffle)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PolicyNetwork(torch.nn.Module):\n",
    "    def __init__(self,state_dims, n_actions, hidden_size=64, num_layers=1):\n",
    "        super(PolicyNetwork, self).__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "        self.num_layers = num_layers\n",
    "\n",
    "        self.lstm = torch.nn.LSTM(state_dims, hidden_size, num_layers, batch_first=True)\n",
    "        self.fc1 = torch.nn.Linear(hidden_size, 32)\n",
    "        torch.nn.init.kaiming_uniform_(self.fc1.weight)\n",
    "        self.fc2 = torch.nn.Linear(32, n_actions)\n",
    "        torch.nn.init.kaiming_uniform_(self.fc2.weight)\n",
    "\n",
    "\n",
    "    def forward(self, x, hidden_state=None):\n",
    "        # Initialize the hidden state if not provided\n",
    "        if hidden_state is None:\n",
    "            h_0 = torch.zeros(self.num_layers, x.size(0), self.hidden_size)\n",
    "            c_0 = torch.zeros(self.num_layers, x.size(0), self.hidden_size)\n",
    "            hidden_state = (h_0, c_0)\n",
    "        \n",
    "        # Pass through LSTM layer\n",
    "        lstm_out, hidden_state = self.lstm(x, hidden_state)\n",
    "        \n",
    "        # Take the output of the last LSTM cell\n",
    "        lstm_out = lstm_out[:, -1, :]\n",
    "\n",
    "        actor_out = torch.tanh(self.fc1(lstm_out))\n",
    "        actor_out = torch.tanh(self.fc2(actor_out))\n",
    "        policy = torch.nn.functional.softmax(actor_out, dim=-1) \n",
    "\n",
    "        return policy, hidden_state\n",
    "    \n",
    "\n",
    "class ValueNetwork(torch.nn.Module):\n",
    "    def __init__(self,state_dims, hidden_size=64, num_layers=1):\n",
    "        super(ValueNetwork, self).__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "        self.num_layers = num_layers\n",
    "\n",
    "        self.lstm = torch.nn.LSTM(state_dims, hidden_size, num_layers, batch_first=True)\n",
    "        self.fc1 = torch.nn.Linear(hidden_size, 32)\n",
    "        torch.nn.init.kaiming_uniform_(self.fc1.weight)\n",
    "        self.fc2 = torch.nn.Linear(32, 1)\n",
    "        torch.nn.init.kaiming_uniform_(self.fc2.weight)\n",
    "\n",
    "\n",
    "    def forward(self, x, hidden_state=None):\n",
    "        # Initialize the hidden state if not provided\n",
    "        if hidden_state is None:\n",
    "            h_0 = torch.zeros(self.num_layers, x.size(0), self.hidden_size)\n",
    "            c_0 = torch.zeros(self.num_layers, x.size(0), self.hidden_size)\n",
    "            hidden_state = (h_0, c_0)\n",
    "        \n",
    "        # Pass through LSTM layer\n",
    "        lstm_out, hidden_state = self.lstm(x, hidden_state)\n",
    "        \n",
    "        # Take the output of the last LSTM cell\n",
    "        lstm_out = lstm_out[:, -1, :]\n",
    "\n",
    "        critic_out = torch.tanh(self.fc1(lstm_out))\n",
    "        value = self.fc2(critic_out)\n",
    "\n",
    "        return value, hidden_state\n",
    "        \n",
    "def weights_init(m):\n",
    "    if isinstance(m,torch.nn.Linear):\n",
    "        m.bias.data.fill_(0)\n",
    "        torch.nn.init.kaiming_uniform_(m.weight)\n",
    "\n",
    "def np_to_tensor(x):\n",
    "    return torch.tensor(x).to(torch.float32)\n",
    "\n",
    "    \n",
    "policy_model = PolicyNetwork(state_dims=6, n_actions = 6)\n",
    "value_model = ValueNetwork(state_dims=6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PolicyNetwork(\n",
      "  (lstm): LSTM(6, 64, batch_first=True)\n",
      "  (fc1): Linear(in_features=64, out_features=32, bias=True)\n",
      "  (fc2): Linear(in_features=32, out_features=6, bias=True)\n",
      ") ValueNetwork(\n",
      "  (lstm): LSTM(6, 64, batch_first=True)\n",
      "  (fc1): Linear(in_features=64, out_features=32, bias=True)\n",
      "  (fc2): Linear(in_features=32, out_features=1, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "print(policy_model,value_model)\n",
    "# summary(policy_model, input_size=(1, 6))\n",
    "# summary(value_model, input_size=(1, 6))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PPO:\n",
    "    def __init__(self, state_dim=6, action_dim=6, policy_lr=0.0005, value_lr=0.001, \n",
    "                 gamma=0.99, lam=0.95, beta_s=0.01,epsilon_clip=0.2, value_clip=0.4, \n",
    "                 epochs=5,batch_size=50,save_path='Models'):\n",
    "        \n",
    "        self.policy_net = PolicyNetwork(state_dim, action_dim).to(device)\n",
    "        self.value_net = ValueNetwork(state_dim).to(device)\n",
    "        # self.policy_net.apply(weights_init)\n",
    "        # self.value_net.apply(weights_init)\n",
    "\n",
    "        self.policy_optimizer = torch.optim.Adam(self.policy_net.parameters(), lr=policy_lr)\n",
    "        self.value_optimizer = torch.optim.Adam(self.value_net.parameters(), lr=value_lr)\n",
    "\n",
    "        self.gamma = gamma\n",
    "        self.lamda = lam\n",
    "        self.epsilon_clip = epsilon_clip\n",
    "        self.value_clip = value_clip\n",
    "        self.beta_s = beta_s\n",
    "        self.epochs = epochs\n",
    "        self.action_dim = action_dim\n",
    "        self.batch_size = batch_size\n",
    "\n",
    "        self.env = Env(port=5555)\n",
    "        self.env.run()\n",
    "\n",
    "        self.save_path = save_path\n",
    "        self.env_name = \"auto-docking-iss\"\n",
    "\n",
    "        if not os.path.exists(self.save_path): os.makedirs(self.save_path)\n",
    "        self.path = str(self.env_name)+'_PPO_'\n",
    "        self.model_name = os.path.join(self.save_path, self.path)\n",
    "\n",
    "        self.trajectory = {'states':[],'actions':[], 'rewards':[], 'action_logs':[],\n",
    "                           'values':[],'done':[]}\n",
    "        \n",
    "        self.scores, self.episodes, self.average = [],[],[]\n",
    "        self.value_loss_mean, self.policy_loss_mean = [], []\n",
    "\n",
    "    def save(self,episode,score):\n",
    "        torch.save({\n",
    "            'actor': self.policy_net.state_dict(),\n",
    "            'critic': self.value_net.state_dict()\n",
    "        }, f'./'+ self.model_name + '_' + str(episode) + '_' + str(round(score,4)) + '.pt')\n",
    "\n",
    "    def load(self,name):\n",
    "        print(f'./{name}')\n",
    "        try:\n",
    "            data = torch.load(f'./{name}')\n",
    "            self.policy_net.load_state_dict(data['actor'])\n",
    "            self.value_net.load_state_dict(data['critic'])\n",
    "        except Exception as e:\n",
    "            print(e)\n",
    "\n",
    "    # Función para visualizar la evolución del entrenamiento\n",
    "    def plot_model(self, score, episode,num_episodes):\n",
    "        self.scores.append(score)\n",
    "        self.episodes.append(episode)\n",
    "        self.average.append(sum(self.scores[-10:]) / len(self.scores[-10:]))\n",
    "        if (episode % 10 == 0 and episode > 0) or episode == num_episodes:#str(episode)[-2:] == \"00\":# much faster than episode % 100\n",
    "            plt.figure(1,figsize=(18, 9))\n",
    "            plt.plot(self.episodes, self.scores, 'b')\n",
    "            plt.plot(self.episodes, self.average, 'r')\n",
    "            plt.ylabel('Score', fontsize=18)\n",
    "            plt.xlabel('Steps', fontsize=18)\n",
    "            try:\n",
    "                plt.savefig(self.model_name+\"scores.png\")\n",
    "            except OSError:\n",
    "                pass\n",
    "\n",
    "        return self.average[-1]\n",
    "    \n",
    "    def plot_loss(self,episode,num_episodes):        \n",
    "        if (episode % 10 == 0 and episode > 0) or episode == num_episodes:#str(episode)[-2:] == \"00\":# much faster than episode % 100\n",
    "            plt.figure(2,figsize=(18, 9))\n",
    "            plt.plot(self.episodes, self.policy_loss_mean, 'b')\n",
    "            plt.plot(self.episodes, self.value_loss_mean, 'r')\n",
    "            plt.ylabel('Loss', fontsize=18)\n",
    "            plt.xlabel('Steps', fontsize=18)\n",
    "            try:\n",
    "                plt.savefig(self.model_name+\"loss.png\")\n",
    "            except OSError:\n",
    "                pass\n",
    "    \n",
    "\n",
    "    #Almacenamiento de todas las variables que definen una transición para PPO\n",
    "    def collect_trayectory(self,state,action,action_logs,reward,done,value):\n",
    "        self.trajectory['states'].append(state)\n",
    "        self.trajectory['actions'].append(action)\n",
    "        self.trajectory['rewards'].append(reward)\n",
    "        self.trajectory['values'].append(value)\n",
    "        self.trajectory['action_logs'].append(action_logs)\n",
    "        self.trajectory['done'].append(done)\n",
    "    \n",
    "    def clear_trayectory(self):\n",
    "        for key in self.trajectory.keys():\n",
    "            self.trajectory[key].clear()\n",
    "\n",
    "    \n",
    "    def generalized_advantage_estimation(self,next_state):\n",
    "        advantages = np.zeros_like(self.trajectory['rewards'])\n",
    "        last_gae_lambda = 0\n",
    "        value,hidden_state = self.value_net(next_state.unsqueeze(0).unsqueeze(0),None)\n",
    "        self.trajectory['values'].append(value.squeeze().detach())\n",
    "\n",
    "        for t in reversed(range(len(self.trajectory['rewards']))):\n",
    "            delta = self.trajectory['rewards'][t]+ self.gamma * self.trajectory['values'][t + 1].item() - self.trajectory['values'][t].item()\n",
    "            advantages[t] = last_gae_lambda = delta + self.gamma * self.lamda * last_gae_lambda\n",
    "        \n",
    "        return np_to_tensor(advantages)\n",
    "\n",
    "\n",
    "    def learn_from_episode_trajectory(self,next_state):\n",
    "        self.policy_net.train()\n",
    "        self.value_net.train()\n",
    "\n",
    "        states = deepcopy(self.trajectory['states'])\n",
    "        actions = deepcopy(self.trajectory['actions'])\n",
    "        # rewards = deepcopy(self.trajectory['rewards'])\n",
    "        values = deepcopy(self.trajectory['values'])\n",
    "        action_logs = deepcopy(self.trajectory['action_logs'])\n",
    "        # done = deepcopy(self.trajectory['done'])\n",
    "\n",
    "        advantages = self.generalized_advantage_estimation(next_state).to(device)\n",
    "\n",
    "        # prepare dataloader for policy phase training\n",
    "        dl = create_shuffled_dataloader([states, actions, action_logs, advantages, values], self.batch_size,shuffle=False)\n",
    "\n",
    "        policy_loss_array = []\n",
    "        value_loss_array = []\n",
    "\n",
    "        for i in range(self.epochs):\n",
    "            policy_hidden_state = None\n",
    "            value_hidden_state = None\n",
    "            for states, actions, action_log, advantages, values in dl:\n",
    "                #Update policy_net\n",
    "                new_action_probs,policy_hidden_state = self.policy_net(states.unsqueeze(0),policy_hidden_state)\n",
    "                new_action_probs = new_action_probs.squeeze()\n",
    "                dist = torch.distributions.Categorical(new_action_probs)\n",
    "\n",
    "                # print(\"\")\n",
    "                # print(actions)\n",
    "                # print(new_action_probs)\n",
    "                new_action_logs = dist.log_prob(actions)\n",
    "                entropy = dist.entropy()\n",
    "\n",
    "                # compute PPO-Clip\n",
    "                ratios = torch.exp(new_action_logs - action_log)\n",
    "                surr1 = ratios * advantages\n",
    "                surr2 = ratios.clamp(1 - self.epsilon_clip, 1 + self.epsilon_clip) * advantages\n",
    "                policy_loss = -torch.mean(torch.min(surr1, surr2) - self.beta_s * entropy)\n",
    "                policy_loss_array.append(policy_loss.detach().numpy())\n",
    "\n",
    "                self.policy_optimizer.zero_grad() # Clean gradients\n",
    "                policy_loss.mean().backward() # Compute gradients\n",
    "                self.policy_optimizer.step() #Update weights\n",
    "\n",
    "                #Update value_net\n",
    "                new_values,value_hidden_state = self.value_net(states.unsqueeze(0),value_hidden_state)\n",
    "                new_values = new_values.squeeze()\n",
    "                value_clipped = values + (new_values - values).clamp(-self.value_clip, self.value_clip)\n",
    "                value_loss_1 = (value_clipped.flatten() - advantages) ** 2\n",
    "                value_loss_2 = (values.flatten() - advantages) ** 2\n",
    "                value_loss = torch.mean(torch.max(value_loss_1, value_loss_2))\n",
    "                value_loss_array.append(value_loss.detach().numpy())\n",
    "\n",
    "                self.value_optimizer.zero_grad() # Clean gradients\n",
    "                value_loss.backward() # Compute gradients \n",
    "                self.value_optimizer.step() #Update weights\n",
    "\n",
    "        self.value_loss_mean.append(sum(value_loss_array)/self.epochs)\n",
    "        self.policy_loss_mean.append(sum(policy_loss_array)/self.epochs)\n",
    "\n",
    "        print(f\"\\n Mean Policy loss: {self.value_loss_mean[-1]:.4f}. Mean Value loss: {self.policy_loss_mean[-1]:.4f}\")\n",
    "        \n",
    "        self.policy_net.eval()\n",
    "        self.value_net.eval()\n",
    "\n",
    "\n",
    "    # Función para resetear el entorno tras acabar trayectoria\n",
    "    def reset(self):\n",
    "        state = self.env.reset()\n",
    "        return np_to_tensor(self.process_state(state))\n",
    "    \n",
    "     # Función para interacción agente-entorno\n",
    "    def step(self, action):\n",
    "        next_state, reward, done = self.env.step(action)\n",
    "        next_state = self.process_state(next_state)\n",
    "        return np_to_tensor(next_state), reward, done\n",
    "\n",
    "    def process_state(self,state):\n",
    "        state_processed = np.array(state)\n",
    "        state_processed[0] = state_processed[0] / 30 # Position array\n",
    "        state_processed[1] = state_processed[1] / 3 # Position array\n",
    "        state_processed[2] = state_processed[2] / 3 # Position array\n",
    "        return state_processed\n",
    "    \n",
    "    def train(self,num_episodes=200,max_steps=200):\n",
    "        average = 0\n",
    "        self.policy_net.eval()\n",
    "        self.value_net.eval()\n",
    "\n",
    "        max_score_average = -5000\n",
    "\n",
    "        for episode in range(1,num_episodes+1):\n",
    "            self.clear_trayectory()\n",
    "\n",
    "            state = self.reset()\n",
    "            score = 0\n",
    "\n",
    "            SAVING = \"\"\n",
    "            policy_hidden_state = None\n",
    "            value_hidden_state = None\n",
    "\n",
    "            for step in range(1,max_steps+1):\n",
    "                \n",
    "                action_probs,policy_hidden_state = self.policy_net(state.unsqueeze(0).unsqueeze(0),policy_hidden_state)\n",
    "                action_probs = action_probs.squeeze().detach()\n",
    "                dist = torch.distributions.Categorical(action_probs)\n",
    "                action = dist.sample()\n",
    "                action_log = dist.log_prob(action)\n",
    "\n",
    "                value,value_hidden_state = self.value_net(state.unsqueeze(0).unsqueeze(0),value_hidden_state)\n",
    "                value = value.squeeze().detach()\n",
    "                \n",
    "                next_state,reward,done = self.step(action.item())    \n",
    "                self.collect_trayectory(state,action,action_log,reward,done,value)\n",
    "\n",
    "                state = next_state\n",
    "\n",
    "                score += reward\n",
    "\n",
    "                # print(state)\n",
    "                print(f'\\rSteps: {step}/{max_steps}. Action: {action}. Reward/Value: {round(reward,2)}/{round(value.item(),4)}. Done: {done}', end='', flush=True)\n",
    "\n",
    "                if done:\n",
    "                    break\n",
    "                \n",
    "                time.sleep(1.0)\n",
    "\n",
    "            self.learn_from_episode_trajectory(next_state)\n",
    "            self.clear_trayectory()\n",
    "\n",
    "            average = self.plot_model(score, episode,num_episodes)\n",
    "            self.plot_loss(episode,num_episodes)\n",
    "            \n",
    "            if average >= max_score_average and episode > 5:\n",
    "                max_score_average = average\n",
    "                self.save(episode,average)\n",
    "                SAVING = \"SAVING\"\n",
    "            elif episode % 10 == 0:\n",
    "                self.save(episode,average)\n",
    "                SAVING = \"SAVING\"\n",
    "            else:\n",
    "                SAVING = \"\"\n",
    "            \n",
    "            print(f\"Episode: {episode}/{num_episodes}, score/average: {score:.4f}/{average:.4f} {SAVING}\\n\")\n",
    "\n",
    "\n",
    "        # close environemnt when finish training\n",
    "        self.save(episode,average)\n",
    "\n",
    "    def test(self,max_steps):\n",
    "        self.policy_net.eval()\n",
    "        self.value_net.eval()\n",
    "\n",
    "        self.clear_trayectory()\n",
    "        state = self.reset()\n",
    "        score = 0\n",
    "\n",
    "        start = time.time()\n",
    "        end = time.time()\n",
    "        step = 0\n",
    "\n",
    "        while True:\n",
    "            end = time.time()\n",
    "            if end - start >= 1.0:\n",
    "                step += 1\n",
    "                action_probabilities = self.policy_net(state)\n",
    "                action = torch.argmax(action_probabilities, dim=-1).item()#np.argmax(self.policy_net(state).squeeze().detach().numpy())\n",
    "                value = self.value_net(state).squeeze().detach()\n",
    "                \n",
    "                next_state,reward,done = self.step(action)    \n",
    "\n",
    "                state = next_state\n",
    "\n",
    "                score += reward\n",
    "\n",
    "                # print(action_probabilities)\n",
    "                print(f'\\rSteps: {step}/{max_steps}. Action: {action}. Score/Value: {round(score,2)}/{round(value.item(),4)}. Done: {done}', end='', flush=True)\n",
    "                start = time.time()\n",
    "\n",
    "            if self.env.done or step > max_steps:\n",
    "                break\n",
    "\n",
    "        state_end = self.env.get_state()\n",
    "        \n",
    "        print(f'\\nNext state: {state_end}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "(11477) wsgi starting up on http://127.0.0.1:5555\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "(11477) accepted ('127.0.0.1', 58882)\n",
      "(11477) accepted ('127.0.0.1', 58898)\n",
      "127.0.0.1 - - [10/Aug/2024 14:07:17] \"GET / HTTP/1.1\" 304 243 0.006165\n",
      "127.0.0.1 - - [10/Aug/2024 14:07:17] \"GET /css/normalize.css HTTP/1.1\" 304 201 0.000765\n",
      "(11477) accepted ('127.0.0.1', 58904)\n",
      "(11477) accepted ('127.0.0.1', 58908)\n",
      "(11477) accepted ('127.0.0.1', 58914)\n",
      "(11477) accepted ('127.0.0.1', 58920)\n",
      "(11477) accepted ('127.0.0.1', 58932)\n",
      "(11477) accepted ('127.0.0.1', 58948)\n",
      "127.0.0.1 - - [10/Aug/2024 14:07:17] \"GET /css/layout.css HTTP/1.1\" 304 198 0.000494\n",
      "127.0.0.1 - - [10/Aug/2024 14:07:17] \"GET /js/three/three.js HTTP/1.1\" 304 196 0.000380\n",
      "127.0.0.1 - - [10/Aug/2024 14:07:17] \"GET /js/three/WebGL.js HTTP/1.1\" 304 196 0.000334\n",
      "127.0.0.1 - - [10/Aug/2024 14:07:17] \"GET /js/three/GLTFLoader.js HTTP/1.1\" 304 201 0.000448\n",
      "127.0.0.1 - - [10/Aug/2024 14:07:17] \"GET /js/three/CSS2DRenderer.js HTTP/1.1\" 304 204 0.000487\n",
      "127.0.0.1 - - [10/Aug/2024 14:07:17] \"GET /js/gsap3/gsap.js HTTP/1.1\" 304 195 0.000435\n",
      "(11477) accepted ('127.0.0.1', 58954)\n",
      "(11477) accepted ('127.0.0.1', 58956)\n",
      "127.0.0.1 - - [10/Aug/2024 14:07:17] \"GET /js/gsap3/DrawSVGPlugin.js HTTP/1.1\" 304 204 0.000573\n",
      "127.0.0.1 - - [10/Aug/2024 14:07:17] \"GET /js/gsap3/ScrambleTextPlugin.js HTTP/1.1\" 304 209 0.000539\n",
      "(11477) accepted ('127.0.0.1', 58958)\n",
      "127.0.0.1 - - [10/Aug/2024 14:07:17] \"GET /img/hud-darken.png HTTP/1.1\" 304 202 0.000921\n",
      "(11477) accepted ('127.0.0.1', 58964)\n",
      "127.0.0.1 - - [10/Aug/2024 14:07:17] \"GET /img/hud-ring.png HTTP/1.1\" 304 200 0.000802\n",
      "(11477) accepted ('127.0.0.1', 58980)\n",
      "127.0.0.1 - - [10/Aug/2024 14:07:17] \"GET /img/hud-ring-inner.png HTTP/1.1\" 304 206 0.000529\n",
      "(11477) accepted ('127.0.0.1', 58990)\n",
      "127.0.0.1 - - [10/Aug/2024 14:07:17] \"GET /img/shadow.png HTTP/1.1\" 304 198 0.000567\n",
      "(11477) accepted ('127.0.0.1', 59004)\n",
      "127.0.0.1 - - [10/Aug/2024 14:07:17] \"GET /js/threed.js HTTP/1.1\" 304 197 0.000865\n",
      "(11477) accepted ('127.0.0.1', 59016)\n",
      "127.0.0.1 - - [10/Aug/2024 14:07:17] \"GET /img/instructions/step1_6.jpg HTTP/1.1\" 304 199 0.000595\n",
      "(11477) accepted ('127.0.0.1', 59018)\n",
      "(11477) accepted ('127.0.0.1', 59022)\n",
      "127.0.0.1 - - [10/Aug/2024 14:07:17] \"GET /img/instructions/step2.jpg HTTP/1.1\" 304 197 0.000662\n",
      "127.0.0.1 - - [10/Aug/2024 14:07:17] \"GET /img/instructions/step3.jpg HTTP/1.1\" 304 197 0.000384\n",
      "(11477) accepted ('127.0.0.1', 59030)\n",
      "127.0.0.1 - - [10/Aug/2024 14:07:17] \"GET /img/instructions/step4.jpg HTTP/1.1\" 304 197 0.000594\n",
      "(11477) accepted ('127.0.0.1', 59032)\n",
      "(11477) accepted ('127.0.0.1', 59042)\n",
      "127.0.0.1 - - [10/Aug/2024 14:07:17] \"GET /img/instructions/step5.jpg HTTP/1.1\" 304 197 0.000637\n",
      "127.0.0.1 - - [10/Aug/2024 14:07:17] \"GET /img/instructions/step7.jpg HTTP/1.1\" 304 197 0.000438\n",
      "(11477) accepted ('127.0.0.1', 59058)\n",
      "127.0.0.1 - - [10/Aug/2024 14:07:17] \"GET /socket.io/?EIO=4&transport=polling&t=P4yBvLe HTTP/1.1\" 200 278 0.000342\n",
      "127.0.0.1 - - [10/Aug/2024 14:07:17] \"GET /img/hud/ForwardViewSprites.json HTTP/1.1\" 304 211 0.000663\n",
      "(11477) accepted ('127.0.0.1', 59060)\n",
      "127.0.0.1 - - [10/Aug/2024 14:07:17] \"GET /3d/iss.glb HTTP/1.1\" 304 195 0.000813\n",
      "(11477) accepted ('127.0.0.1', 59072)\n",
      "127.0.0.1 - - [10/Aug/2024 14:07:17] \"GET /img/earth.jpg HTTP/1.1\" 304 197 0.000724\n",
      "(11477) accepted ('127.0.0.1', 59074)\n",
      "(11477) accepted ('127.0.0.1', 59078)\n",
      "(11477) accepted ('127.0.0.1', 59092)\n",
      "(11477) accepted ('127.0.0.1', 59096)\n",
      "127.0.0.1 - - [10/Aug/2024 14:07:17] \"GET /img/navball.png HTTP/1.1\" 304 199 0.000515\n",
      "127.0.0.1 - - [10/Aug/2024 14:07:17] \"GET /img/texture_fire.jpg HTTP/1.1\" 304 204 0.000339\n",
      "127.0.0.1 - - [10/Aug/2024 14:07:17] \"GET /img/texture_wormhole.jpg HTTP/1.1\" 304 208 0.000526\n",
      "127.0.0.1 - - [10/Aug/2024 14:07:17] \"GET /img/texture_star.jpg HTTP/1.1\" 304 204 0.000442\n",
      "(11477) accepted ('127.0.0.1', 59106)\n",
      "127.0.0.1 - - [10/Aug/2024 14:07:17] \"GET /img/hud/ForwardViewSprites2.png HTTP/1.1\" 304 211 0.000953\n",
      "(11477) accepted ('127.0.0.1', 59116)\n",
      "127.0.0.1 - - [10/Aug/2024 14:07:17] \"POST /socket.io/?EIO=4&transport=polling&t=P4yBvOq&sid=CdGBqEBomE8L17xIAAAA HTTP/1.1\" 200 219 0.000541\n",
      "(11477) accepted ('127.0.0.1', 59132)\n",
      "(11477) accepted ('127.0.0.1', 59144)\n",
      "127.0.0.1 - - [10/Aug/2024 14:07:17] \"GET /socket.io/?EIO=4&transport=polling&t=P4yBvOr&sid=CdGBqEBomE8L17xIAAAA HTTP/1.1\" 200 181 0.000077\n",
      "127.0.0.1 - - [10/Aug/2024 14:07:17] \"GET /socket.io/?EIO=4&transport=polling&t=P4yBvQV&sid=CdGBqEBomE8L17xIAAAA HTTP/1.1\" 200 181 0.000140\n",
      "127.0.0.1 - - [10/Aug/2024 14:07:17] \"GET /socket.io/?EIO=4&transport=polling&t=P4yBvQx&sid=CdGBqEBomE8L17xIAAAA HTTP/1.1\" 200 181 0.000171\n",
      "127.0.0.1 - - [10/Aug/2024 14:07:17] \"GET /img/favicon.ico HTTP/1.1\" 304 199 0.000957\n"
     ]
    }
   ],
   "source": [
    "agent = PPO(save_path='Models10')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reset\n",
      "Steps: 146/250. Action: 5. Reward/Value: -201.41/-0.1787. Done: -1"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "Trying to backward through the graph a second time (or directly access saved tensors after they have already been freed). Saved intermediate values of the graph are freed when you call .backward() or autograd.grad(). Specify retain_graph=True if you need to backward through the graph a second time or if you need to access saved tensors after calling backward.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[7], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43magent\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnum_episodes\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m200\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43mmax_steps\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m250\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[5], line 240\u001b[0m, in \u001b[0;36mPPO.train\u001b[0;34m(self, num_episodes, max_steps)\u001b[0m\n\u001b[1;32m    236\u001b[0m         \u001b[38;5;28;01mbreak\u001b[39;00m\n\u001b[1;32m    238\u001b[0m     time\u001b[38;5;241m.\u001b[39msleep(\u001b[38;5;241m1.0\u001b[39m)\n\u001b[0;32m--> 240\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlearn_from_episode_trajectory\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnext_state\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    241\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mclear_trayectory()\n\u001b[1;32m    243\u001b[0m average \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mplot_model(score, episode,num_episodes)\n",
      "Cell \u001b[0;32mIn[5], line 154\u001b[0m, in \u001b[0;36mPPO.learn_from_episode_trajectory\u001b[0;34m(self, next_state)\u001b[0m\n\u001b[1;32m    151\u001b[0m policy_loss_array\u001b[38;5;241m.\u001b[39mappend(policy_loss\u001b[38;5;241m.\u001b[39mdetach()\u001b[38;5;241m.\u001b[39mnumpy())\n\u001b[1;32m    153\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpolicy_optimizer\u001b[38;5;241m.\u001b[39mzero_grad() \u001b[38;5;66;03m# Clean gradients\u001b[39;00m\n\u001b[0;32m--> 154\u001b[0m \u001b[43mpolicy_loss\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmean\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;66;03m# Compute gradients\u001b[39;00m\n\u001b[1;32m    155\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpolicy_optimizer\u001b[38;5;241m.\u001b[39mstep() \u001b[38;5;66;03m#Update weights\u001b[39;00m\n\u001b[1;32m    157\u001b[0m \u001b[38;5;66;03m#Update value_net\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/envs/miar_tfm/lib/python3.10/site-packages/torch/_tensor.py:525\u001b[0m, in \u001b[0;36mTensor.backward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    515\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_unary(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    516\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[1;32m    517\u001b[0m         Tensor\u001b[38;5;241m.\u001b[39mbackward,\n\u001b[1;32m    518\u001b[0m         (\u001b[38;5;28mself\u001b[39m,),\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    523\u001b[0m         inputs\u001b[38;5;241m=\u001b[39minputs,\n\u001b[1;32m    524\u001b[0m     )\n\u001b[0;32m--> 525\u001b[0m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mautograd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    526\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgradient\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs\u001b[49m\n\u001b[1;32m    527\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/miar_tfm/lib/python3.10/site-packages/torch/autograd/__init__.py:267\u001b[0m, in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    262\u001b[0m     retain_graph \u001b[38;5;241m=\u001b[39m create_graph\n\u001b[1;32m    264\u001b[0m \u001b[38;5;66;03m# The reason we repeat the same comment below is that\u001b[39;00m\n\u001b[1;32m    265\u001b[0m \u001b[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[1;32m    266\u001b[0m \u001b[38;5;66;03m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[0;32m--> 267\u001b[0m \u001b[43m_engine_run_backward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    268\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtensors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    269\u001b[0m \u001b[43m    \u001b[49m\u001b[43mgrad_tensors_\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    270\u001b[0m \u001b[43m    \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    271\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    272\u001b[0m \u001b[43m    \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    273\u001b[0m \u001b[43m    \u001b[49m\u001b[43mallow_unreachable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    274\u001b[0m \u001b[43m    \u001b[49m\u001b[43maccumulate_grad\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    275\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/miar_tfm/lib/python3.10/site-packages/torch/autograd/graph.py:744\u001b[0m, in \u001b[0;36m_engine_run_backward\u001b[0;34m(t_outputs, *args, **kwargs)\u001b[0m\n\u001b[1;32m    742\u001b[0m     unregister_hooks \u001b[38;5;241m=\u001b[39m _register_logging_hooks_on_whole_graph(t_outputs)\n\u001b[1;32m    743\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 744\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mVariable\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_execution_engine\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun_backward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[1;32m    745\u001b[0m \u001b[43m        \u001b[49m\u001b[43mt_outputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\n\u001b[1;32m    746\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# Calls into the C++ engine to run the backward pass\u001b[39;00m\n\u001b[1;32m    747\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m    748\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m attach_logging_hooks:\n",
      "\u001b[0;31mRuntimeError\u001b[0m: Trying to backward through the graph a second time (or directly access saved tensors after they have already been freed). Saved intermediate values of the graph are freed when you call .backward() or autograd.grad(). Specify retain_graph=True if you need to backward through the graph a second time or if you need to access saved tensors after calling backward."
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "127.0.0.1 - - [10/Aug/2024 14:10:19] \"GET /socket.io/?EIO=4&transport=websocket&sid=CdGBqEBomE8L17xIAAAA HTTP/1.1\" 200 0 182.493003\n",
      "(11477) accepted ('127.0.0.1', 46412)\n",
      "127.0.0.1 - - [10/Aug/2024 14:10:19] \"GET / HTTP/1.1\" 304 243 0.000824\n",
      "127.0.0.1 - - [10/Aug/2024 14:10:19] \"GET /css/normalize.css HTTP/1.1\" 304 201 0.000705\n",
      "(11477) accepted ('127.0.0.1', 46420)\n",
      "(11477) accepted ('127.0.0.1', 46428)\n",
      "(11477) accepted ('127.0.0.1', 46436)\n",
      "(11477) accepted ('127.0.0.1', 46440)\n",
      "127.0.0.1 - - [10/Aug/2024 14:10:19] \"GET /css/layout.css HTTP/1.1\" 304 198 0.000476\n",
      "127.0.0.1 - - [10/Aug/2024 14:10:19] \"GET /js/three/three.js HTTP/1.1\" 304 196 0.000516\n",
      "127.0.0.1 - - [10/Aug/2024 14:10:19] \"GET /js/three/WebGL.js HTTP/1.1\" 304 196 0.000338\n",
      "127.0.0.1 - - [10/Aug/2024 14:10:19] \"GET /js/three/GLTFLoader.js HTTP/1.1\" 304 201 0.000403\n",
      "(11477) accepted ('127.0.0.1', 46442)\n",
      "(11477) accepted ('127.0.0.1', 46450)\n",
      "(11477) accepted ('127.0.0.1', 46460)\n",
      "127.0.0.1 - - [10/Aug/2024 14:10:19] \"GET /js/three/CSS2DRenderer.js HTTP/1.1\" 304 204 0.000327\n",
      "127.0.0.1 - - [10/Aug/2024 14:10:19] \"GET /js/gsap3/gsap.js HTTP/1.1\" 304 195 0.000406\n",
      "(11477) accepted ('127.0.0.1', 46472)\n",
      "127.0.0.1 - - [10/Aug/2024 14:10:19] \"GET /js/gsap3/DrawSVGPlugin.js HTTP/1.1\" 304 204 0.000556\n",
      "127.0.0.1 - - [10/Aug/2024 14:10:19] \"GET /js/gsap3/ScrambleTextPlugin.js HTTP/1.1\" 304 209 0.000415\n",
      "(11477) accepted ('127.0.0.1', 46478)\n",
      "127.0.0.1 - - [10/Aug/2024 14:10:19] \"GET /img/hud-darken.png HTTP/1.1\" 304 202 0.000532\n",
      "(11477) accepted ('127.0.0.1', 46484)\n",
      "127.0.0.1 - - [10/Aug/2024 14:10:19] \"GET /img/hud-ring.png HTTP/1.1\" 304 200 0.000535\n",
      "(11477) accepted ('127.0.0.1', 46500)\n",
      "(11477) accepted ('127.0.0.1', 46504)\n",
      "127.0.0.1 - - [10/Aug/2024 14:10:19] \"GET /img/hud-ring-inner.png HTTP/1.1\" 304 206 0.000547\n",
      "127.0.0.1 - - [10/Aug/2024 14:10:19] \"GET /img/shadow.png HTTP/1.1\" 304 198 0.000567\n",
      "(11477) accepted ('127.0.0.1', 46516)\n",
      "127.0.0.1 - - [10/Aug/2024 14:10:19] \"GET /js/threed.js HTTP/1.1\" 304 197 0.000651\n",
      "(11477) accepted ('127.0.0.1', 46522)\n",
      "(11477) accepted ('127.0.0.1', 46524)\n",
      "(11477) accepted ('127.0.0.1', 46530)\n",
      "(11477) accepted ('127.0.0.1', 46536)\n",
      "(11477) accepted ('127.0.0.1', 46538)\n",
      "127.0.0.1 - - [10/Aug/2024 14:10:19] \"GET /img/instructions/step1_6.jpg HTTP/1.1\" 304 199 0.000435\n",
      "127.0.0.1 - - [10/Aug/2024 14:10:19] \"GET /img/instructions/step2.jpg HTTP/1.1\" 304 197 0.000468\n",
      "127.0.0.1 - - [10/Aug/2024 14:10:19] \"GET /img/instructions/step3.jpg HTTP/1.1\" 304 197 0.000372\n",
      "127.0.0.1 - - [10/Aug/2024 14:10:19] \"GET /img/instructions/step4.jpg HTTP/1.1\" 304 197 0.000630\n",
      "127.0.0.1 - - [10/Aug/2024 14:10:19] \"GET /img/instructions/step5.jpg HTTP/1.1\" 304 197 0.000298\n",
      "(11477) accepted ('127.0.0.1', 46544)\n",
      "(11477) accepted ('127.0.0.1', 46556)\n",
      "127.0.0.1 - - [10/Aug/2024 14:10:19] \"GET /img/instructions/step7.jpg HTTP/1.1\" 304 197 0.000827\n",
      "127.0.0.1 - - [10/Aug/2024 14:10:19] \"GET /img/shadow.png HTTP/1.1\" 304 198 0.000619\n",
      "(11477) accepted ('127.0.0.1', 46572)\n",
      "127.0.0.1 - - [10/Aug/2024 14:10:19] \"GET /img/instructions/step1_6.jpg HTTP/1.1\" 304 199 0.000568\n",
      "(11477) accepted ('127.0.0.1', 46576)\n",
      "127.0.0.1 - - [10/Aug/2024 14:10:19] \"GET /socket.io/?EIO=4&transport=polling&t=P4yCb-h HTTP/1.1\" 200 278 0.000450\n",
      "127.0.0.1 - - [10/Aug/2024 14:10:19] \"GET /img/hud/ForwardViewSprites.json HTTP/1.1\" 304 211 0.000801\n",
      "(11477) accepted ('127.0.0.1', 46578)\n",
      "127.0.0.1 - - [10/Aug/2024 14:10:20] \"GET /3d/iss.glb HTTP/1.1\" 304 195 0.000919\n",
      "(11477) accepted ('127.0.0.1', 46586)\n",
      "(11477) accepted ('127.0.0.1', 46596)\n",
      "(11477) accepted ('127.0.0.1', 46598)\n",
      "(11477) accepted ('127.0.0.1', 46606)\n",
      "127.0.0.1 - - [10/Aug/2024 14:10:20] \"GET /img/earth.jpg HTTP/1.1\" 304 197 0.000570\n",
      "127.0.0.1 - - [10/Aug/2024 14:10:20] \"GET /img/navball.png HTTP/1.1\" 304 199 0.000463\n",
      "127.0.0.1 - - [10/Aug/2024 14:10:20] \"GET /img/texture_fire.jpg HTTP/1.1\" 304 204 0.000396\n",
      "127.0.0.1 - - [10/Aug/2024 14:10:20] \"GET /img/texture_wormhole.jpg HTTP/1.1\" 304 208 0.000330\n",
      "(11477) accepted ('127.0.0.1', 46614)\n",
      "127.0.0.1 - - [10/Aug/2024 14:10:20] \"GET /img/texture_star.jpg HTTP/1.1\" 304 204 0.000501\n",
      "(11477) accepted ('127.0.0.1', 46630)\n",
      "(11477) accepted ('127.0.0.1', 46632)\n",
      "127.0.0.1 - - [10/Aug/2024 14:10:20] \"POST /socket.io/?EIO=4&transport=polling&t=P4yCc2t&sid=id5sYyy79nUkT7d1AAAC HTTP/1.1\" 200 219 0.000437\n",
      "127.0.0.1 - - [10/Aug/2024 14:10:20] \"GET /img/hud/ForwardViewSprites2.png HTTP/1.1\" 304 211 0.000599\n",
      "(11477) accepted ('127.0.0.1', 46646)\n",
      "(11477) accepted ('127.0.0.1', 46658)\n",
      "127.0.0.1 - - [10/Aug/2024 14:10:20] \"GET /socket.io/?EIO=4&transport=polling&t=P4yCc2u&sid=id5sYyy79nUkT7d1AAAC HTTP/1.1\" 200 181 0.000107\n",
      "127.0.0.1 - - [10/Aug/2024 14:10:20] \"GET /socket.io/?EIO=4&transport=polling&t=P4yCc4v&sid=id5sYyy79nUkT7d1AAAC HTTP/1.1\" 200 181 0.000161\n",
      "127.0.0.1 - - [10/Aug/2024 14:10:20] \"GET /img/favicon.ico HTTP/1.1\" 304 199 0.000780\n"
     ]
    }
   ],
   "source": [
    "agent.train(num_episodes=200,max_steps=250)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "agent.load(\"Models8/auto-docking-iss_PPO__148_-226.6446.pt\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "agent.test(500)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pyautogui\n",
    "\n",
    "# Simular pulsación de Ctrl+S\n",
    "pyautogui.hotkey('ctrl', 's')\n",
    "\n",
    "os.system('sudo shutdown')\n",
    "# os.system('sudo shutdown -c')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "miar_tfm",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

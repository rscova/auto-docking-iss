{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import random\n",
    "from math import sqrt\n",
    "import torch\n",
    "from torchsummary import summary\n",
    "import matplotlib.pyplot as plt\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from env import Env\n",
    "import os\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ActorCritic(torch.nn.Module):\n",
    "    def __init__(self,state_dims, n_actions):\n",
    "        super(ActorCritic, self).__init__()\n",
    "\n",
    "        self.fc1 = torch.nn.Linear(state_dims, 124)\n",
    "        torch.nn.init.kaiming_uniform_(self.fc1.weight)\n",
    "\n",
    "        self.fc2 = torch.nn.Linear(124, 64)\n",
    "        torch.nn.init.kaiming_uniform_(self.fc2.weight)\n",
    "\n",
    "        self.actor = torch.nn.Linear(64, n_actions)\n",
    "        torch.nn.init.kaiming_uniform_(self.actor.weight)\n",
    "        self.critic = torch.nn.Linear(64, 1)\n",
    "        torch.nn.init.kaiming_uniform_(self.critic.weight)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = torch.nn.functional.relu(self.fc1(x))\n",
    "        x = torch.nn.functional.relu(self.fc2(x))\n",
    "\n",
    "        policy = self.actor(x)\n",
    "        value = self.critic(x)\n",
    "\n",
    "        return policy, value\n",
    "\n",
    "def np_to_tensor(x):\n",
    "    return torch.tensor(x).to(torch.float32)\n",
    "    \n",
    "model = ActorCritic(state_dims=9,n_actions = 7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ActorCritic(\n",
      "  (fc1): Linear(in_features=9, out_features=124, bias=True)\n",
      "  (fc2): Linear(in_features=124, out_features=64, bias=True)\n",
      "  (actor): Linear(in_features=64, out_features=7, bias=True)\n",
      "  (critic): Linear(in_features=64, out_features=1, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------------------------------------------\n",
      "        Layer (type)               Output Shape         Param #\n",
      "================================================================\n",
      "            Linear-1               [-1, 1, 124]           1,240\n",
      "            Linear-2                [-1, 1, 64]           8,000\n",
      "            Linear-3                 [-1, 1, 7]             455\n",
      "            Linear-4                 [-1, 1, 1]              65\n",
      "================================================================\n",
      "Total params: 9,760\n",
      "Trainable params: 9,760\n",
      "Non-trainable params: 0\n",
      "----------------------------------------------------------------\n",
      "Input size (MB): 0.00\n",
      "Forward/backward pass size (MB): 0.00\n",
      "Params size (MB): 0.04\n",
      "Estimated Total Size (MB): 0.04\n",
      "----------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "summary(model, input_size=(1, 9))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<Figure size 1800x900 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "class PPOAgent:\n",
    "    # Algoritmo de optimización de PPO\n",
    "    def __init__(self, env_name):\n",
    "        # Initialization del entorno y parametros de PPO\n",
    "        self.env_name = env_name\n",
    "        self.env = Env()\n",
    "        self.env.run()\n",
    "        self.action_size = 6\n",
    "        self.EPISODES, self.max_average = 500, -500000 # specific for pong\n",
    "        self.lr = 0.001\n",
    "        self.bs = 10000\n",
    "        self.gamma = 0.99\n",
    "\n",
    "        # Hyperparametros para pre-procesamiento obs -> state\n",
    "        self.REM_STEP = 3\n",
    "\n",
    "        # Hyperparametros propios de PPO\n",
    "        self.LOSS_CLIPPING = 0.2\n",
    "        self.ENTROPY_LOSS = 5e-3\n",
    "        self.epochs = 10\n",
    "\n",
    "        # Inicialización de la memoria\n",
    "        self.states, self.actions, self.rewards, self.values, self.actions_probs = [], [], [], [], []\n",
    "        self.scores, self.episodes, self.average = [], [], []\n",
    "\n",
    "        # Preparar rutas de almacenamiento de estados y resultados\n",
    "        self.Save_Path = 'Models'\n",
    "        self.state_size = 3*self.REM_STEP\n",
    "        self.state_memory = np.zeros(self.state_size)\n",
    "\n",
    "        self.image_memory = np.zeros(self.state_size)\n",
    "\n",
    "        if not os.path.exists(self.Save_Path): os.makedirs(self.Save_Path)\n",
    "        self.path = '{}_PPO_{}'.format(self.env_name, self.lr)\n",
    "        self.Model_name = os.path.join(self.Save_Path, self.path)\n",
    "\n",
    "        # Crear el modelo\n",
    "        self.model = ActorCritic(state_dims=self.state_size,n_actions = self.action_size)\n",
    "        # Preparar optimizador.\n",
    "        self.optimizer = torch.optim.Adam(self.model.parameters(), lr=self.lr)\n",
    "\n",
    "\n",
    "    def remember(self, state, action, reward, value, action_prob):\n",
    "        # Almacenamiento de todas las variables que definen una transición para PPO\n",
    "        self.states.append(state)\n",
    "        action_onehot = np.zeros([self.action_size])\n",
    "        action_onehot[action] = 1\n",
    "        self.actions.append(action_onehot)\n",
    "        self.rewards.append(reward)\n",
    "        self.values.append(value)\n",
    "        self.actions_probs.append(action_prob)\n",
    "\n",
    "    def discount_rewards(self, reward):\n",
    "        # Computamos los gamma-discounted rewards sobre un episodio\n",
    "        running_add = 0\n",
    "        discounted_r = np.zeros_like(reward)\n",
    "        for i in reversed(range(0,len(reward))):\n",
    "            if reward[i] != 0: # reseteamos debido a re-inicio de partida (pong specific!)\n",
    "                running_add = 0\n",
    "            running_add = running_add * self.gamma + reward[i]\n",
    "            discounted_r[i] = running_add\n",
    "\n",
    "        # Normalización para reducir la varianza\n",
    "        discounted_r -= np.mean(discounted_r)\n",
    "        discounted_r /= np.std(discounted_r)\n",
    "        return discounted_r\n",
    "\n",
    "    def fit_actor(self, states, actions, advantages, actions_probs, verbose=False):\n",
    "        self.model.train()\n",
    "\n",
    "        idx, track_loss = 0, 0\n",
    "        indexes = np.arange(0, states.shape[0])\n",
    "        np.random.shuffle(indexes)\n",
    "        for i_epoch in range(states.shape[0]//self.bs):\n",
    "            indexes_batch = indexes[idx:idx+self.bs]\n",
    "\n",
    "            # Seleccionamos un batch de la trayectoria\n",
    "            states_batch = np_to_tensor(states[indexes_batch,:])\n",
    "            actions_batch = np_to_tensor(actions[indexes_batch,:].squeeze())\n",
    "            advantages_batch = np_to_tensor(advantages[indexes_batch])\n",
    "            actions_probs_batch = np_to_tensor(actions_probs[indexes_batch])\n",
    "\n",
    "            # Hacemos forward al actor-critic dado el estado actual\n",
    "            logits, _ = self.model(states_batch)\n",
    "\n",
    "            # Obtenemos las probabilidades de la acción a partir de los logits\n",
    "            prob = torch.nn.functional.softmax(logits, -1)\n",
    "\n",
    "            # Seleccionamos probabilidad de acción realizada y antigua\n",
    "            prob = torch.sum(prob * actions_batch, -1)\n",
    "            old_prob = torch.sum(actions_probs_batch * actions_batch, -1)\n",
    "\n",
    "            # Calculamos el ratio\n",
    "            r = prob/(old_prob + 1e-10)\n",
    "\n",
    "            # Ponderación por función ventaja\n",
    "            p1 = r * advantages_batch\n",
    "\n",
    "            # Clip\n",
    "            p2 = torch.clip(r, min=1 - self.LOSS_CLIPPING, max=1 + self.LOSS_CLIPPING) * advantages_batch\n",
    "\n",
    "            # Calculate loss\n",
    "            ppo_loss = -torch.mean(torch.minimum(p1, p2) + self.ENTROPY_LOSS * -(prob * torch.log(prob + 1e-10)))\n",
    "\n",
    "            # Computamos gradientes\n",
    "            ppo_loss.backward()\n",
    "            # Actualizamos los pesos\n",
    "            self.optimizer.step()\n",
    "            # Limpiamos gradientes del modelo\n",
    "            self.optimizer.zero_grad()\n",
    "            # Actualizamos iterador de batch\n",
    "            idx += self.bs\n",
    "            track_loss += ppo_loss.item()/(states.shape[0]//self.bs)\n",
    "\n",
    "            if verbose:\n",
    "                print(str(i_epoch) + \"/\" + str(states.shape[0]//self.bs) + \" - loss: \" + str(ppo_loss.item()), end=\"\\r\")\n",
    "        if verbose:\n",
    "            print(str(i_epoch) + \"/\" + str(states.shape[0]//self.bs) + \" - loss: \" + str(track_loss), end=\"\\n\")\n",
    "\n",
    "        self.model.eval()\n",
    "\n",
    "    def fit_critic(self, states, discounted_r, verbose=False):\n",
    "        self.model.train()\n",
    "\n",
    "        idx, track_loss = 0, 0\n",
    "        indexes = np.arange(0, states.shape[0])\n",
    "        np.random.shuffle(indexes)\n",
    "        for i_epoch in range(states.shape[0]//self.bs):\n",
    "            indexes_batch = indexes[idx:idx+self.bs]\n",
    "\n",
    "            # Seleccionamos un batch de la trayectoria\n",
    "            states_batch = np_to_tensor(states[indexes_batch,:])\n",
    "            discounted_r_batch = np_to_tensor(discounted_r[indexes_batch])\n",
    "\n",
    "            # Hacemos forward al actor-critic dado el estado actual\n",
    "            _, values = self.model(states_batch)\n",
    "\n",
    "            # Obtenemos criterios de optimización\n",
    "            critic_loss = torch.mean((discounted_r_batch.detach() - values).pow(2))\n",
    "\n",
    "            # Computamos gradientes\n",
    "            critic_loss.backward()\n",
    "            # Actualizamos los pesos\n",
    "            self.optimizer.step()\n",
    "            # Limpiamos gradientes del modelo\n",
    "            self.optimizer.zero_grad()\n",
    "            # Actualizamos iterador de batch\n",
    "            idx += self.bs\n",
    "            track_loss += critic_loss.item()/(states.shape[0]//self.bs)\n",
    "            if verbose:\n",
    "                print(str(i_epoch) + \"/\" + str(states.shape[0]//self.bs) + \" - loss: \" + str(critic_loss.item()), end=\"\\r\")\n",
    "        if verbose:\n",
    "            print(str(i_epoch) + \"/\" + str(states.shape[0]//self.bs) + \" - loss: \" + str(track_loss), end=\"\\n\")\n",
    "\n",
    "        self.model.eval()\n",
    "\n",
    "    # Función principal de entrenamiento\n",
    "    def replay(self):\n",
    "        # Pasamos la memoria de listas a arrays\n",
    "        states = np.vstack(self.states)\n",
    "        actions = np.vstack(self.actions)\n",
    "        values = np.array(self.values)\n",
    "        actions_probs = np.array(self.actions_probs)\n",
    "\n",
    "        # Compute discounted rewards\n",
    "        discounted_r = self.discount_rewards(self.rewards)\n",
    "\n",
    "        # Compute advantages\n",
    "        advantages = discounted_r - values\n",
    "\n",
    "        print(\"Trajectory length: \", len(self.actions))\n",
    "        # Training Actor and Critic networks\n",
    "        for i in range(self.epochs):\n",
    "            print(f'Fit actor. Epoch:{i}',end='\\r')\n",
    "            self.fit_actor(states, actions, advantages, actions_probs)\n",
    "        print(\"\")\n",
    "        for i in range(self.epochs):\n",
    "            print(f\"Fit Critic. Epoch: \", i,end='\\r')\n",
    "            self.fit_critic(states, discounted_r)\n",
    "        print(\"\")\n",
    "\n",
    "        # reset training memory\n",
    "        self.states, self.actions, self.rewards, self.values, self.actions_probs = [], [], [], [], []\n",
    "\n",
    "    def load(self, model_name):\n",
    "        self.model.load_state_dict(torch.load(model_name))\n",
    "\n",
    "    def save(self,episode,score):\n",
    "        torch.save(self.model.state_dict(), self.Model_name + '_' + str(episode) + '_' + str(score) + '.pth')\n",
    "\n",
    "    # Función para visualizar la evolución del entrenamiento\n",
    "    plt.figure(figsize=(18, 9))\n",
    "    def PlotModel(self, score, episode):\n",
    "        self.scores.append(score)\n",
    "        self.episodes.append(episode)\n",
    "        self.average.append(sum(self.scores[-50:]) / len(self.scores[-50:]))\n",
    "        if str(episode)[-2:] == \"00\":# much faster than episode % 100\n",
    "            plt.plot(self.episodes, self.scores, 'b')\n",
    "            plt.plot(self.episodes, self.average, 'r')\n",
    "            plt.ylabel('Score', fontsize=18)\n",
    "            plt.xlabel('Steps', fontsize=18)\n",
    "            try:\n",
    "                plt.savefig(self.path+\".png\")\n",
    "            except OSError:\n",
    "                pass\n",
    "\n",
    "        return self.average[-1]\n",
    "\n",
    "    def process_state(self,state):\n",
    "        self.state_memory = np.concatenate((np.array(state) / 40, self.state_memory))[:self.state_size]\n",
    "        return self.state_memory\n",
    "\n",
    "        return np.array(state[:3] + state[6:9])\n",
    "\n",
    "    # Función de pre-procesamiento observación --> estado\n",
    "    def GetImage(self, frame):\n",
    "        # Cropeamos la información de la imagen\n",
    "        frame_cropped = frame[35:195:2, ::2,:]\n",
    "        # if frame_cropped.shape[0] != self.COLS or frame_cropped.shape[1] != self.ROWS:\n",
    "        #     # OpenCV resize function\n",
    "        #     frame_cropped = cv2.resize(frame, (self.COLS, self.ROWS), interpolation=cv2.INTER_CUBIC)\n",
    "\n",
    "        # Convertimos a rgb\n",
    "        frame_rgb = 0.299*frame_cropped[:,:,0] + 0.587*frame_cropped[:,:,1] + 0.114*frame_cropped[:,:,2]\n",
    "\n",
    "        # Convertimos a máscara binaria para acelerar el entrenamiento\n",
    "        frame_rgb[frame_rgb < 100] = 0\n",
    "        frame_rgb[frame_rgb >= 100] = 255\n",
    "\n",
    "        # Normalizamos los valroes de intensidad\n",
    "        new_frame = np.array(frame_rgb).astype(np.float32) / 255.0\n",
    "\n",
    "        # push our data by 1 frame, similar as deq() function work\n",
    "        self.image_memory = np.roll(self.image_memory, 1, axis = 0)\n",
    "\n",
    "        # inserting new frame to free space\n",
    "        self.image_memory[0,:,:] = new_frame\n",
    "\n",
    "        return np.expand_dims(self.image_memory, axis=0)\n",
    "\n",
    "    # Función para resetear el entorno tras acabar trayectoria\n",
    "    def reset(self):\n",
    "        frame = self.env.reset()\n",
    "        self.state_memory = np.zeros(self.state_size)\n",
    "        return self.process_state(frame)\n",
    "\n",
    "    # Función para interacción agente-entorno\n",
    "    def step(self, action):\n",
    "        next_state, reward, done = self.env.step(action)\n",
    "        next_state = self.process_state(next_state)\n",
    "        return next_state, reward, done\n",
    "\n",
    "    # Función principal de exploración + entrenamiento\n",
    "    def run(self):\n",
    "        # unique_actions_list = []\n",
    "        # Bucle de episodios de entrenamiento\n",
    "        for e in range(self.EPISODES):\n",
    "            self.model.eval()\n",
    "            state = self.reset()\n",
    "            done, score, SAVING = False, 0, ''\n",
    "            # Bucle para recopilar la trayectoria\n",
    "            while not done:\n",
    "                time.sleep(0.02)\n",
    "                # Elección de acción por parte del actor\n",
    "                action, value = self.model(np_to_tensor(state))\n",
    "                # Pasamos a distribución de probabilidad\n",
    "                action_prob = torch.softmax(action, -1).squeeze().detach().numpy()\n",
    "                # Hacemos un sampling de la acción - exploraicón\n",
    "                action = np.random.choice(self.action_size, p=action_prob)\n",
    "                value = value.squeeze().detach().numpy()\n",
    "                # Dada la acción seleccionado, interaccionar con el entorno y recibir nuevo estado y recompensa\n",
    "                # if action not in unique_actions_list:\n",
    "                #     unique_actions_list.append(action)\n",
    "                next_state, reward, done = self.step(action)\n",
    "                # print(next_state)\n",
    "                # Almacenamiento de memoria para posterior entrenamiento\n",
    "                self.remember(state, action, reward, value, action_prob)\n",
    "                # Update current state\n",
    "                state = next_state\n",
    "                score += reward\n",
    "\n",
    "                if done: # Estado terminal\n",
    "                    # Al acabar la trayectoria, almacenamos KPIs para curva de entrenamiento y almacenamos mejor modelo\n",
    "                    average = self.PlotModel(score, e)\n",
    "                    if average >= self.max_average:\n",
    "                        self.max_average = average\n",
    "                        self.save(e,score)\n",
    "                        SAVING = \"SAVING\"\n",
    "                    else:\n",
    "                        SAVING = \"\"\n",
    "                    print(\"episode: {}/{}, score: {}, average: {:.2f} {}\".format(e, self.EPISODES, score, average, SAVING))\n",
    "\n",
    "                    # print(unique_actions_list)\n",
    "                    # Entrenamos un modelo haciendo un replay de la trayectoria\n",
    "                    self.replay()\n",
    "                    print(\"End buffer replay\")\n",
    "\n",
    "        # close environemnt when finish training\n",
    "        self.env.close()\n",
    "\n",
    "    # Función de testeo\n",
    "    def test(self, Actor_name, Critic_name):\n",
    "        self.load(model_name)\n",
    "        self.model.eval()\n",
    "\n",
    "        for e in range(100):\n",
    "            state = self.reset()\n",
    "            done = False\n",
    "            score = 0\n",
    "            while not done:\n",
    "                # Al testear, seleccionamos la acción con mayor probabilidad (no muestreo)\n",
    "                action, value = self.model(np_to_tensor(state))\n",
    "                state, reward, done = self.step(action)\n",
    "                score += reward\n",
    "                if done:\n",
    "                    print(\"episode: {}/{}, score: {}\".format(e, self.EPISODES, score))\n",
    "                    break\n",
    "        self.env.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "(7675) wsgi starting up on http://0.0.0.0:5000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "(7675) accepted ('127.0.0.1', 33286)\n",
      "(7675) accepted ('127.0.0.1', 33298)\n",
      "127.0.0.1 - - [22/Jul/2024 16:56:19] \"GET / HTTP/1.1\" 200 47839 0.003519\n",
      "(7675) accepted ('127.0.0.1', 33302)\n",
      "(7675) accepted ('127.0.0.1', 33314)\n",
      "(7675) accepted ('127.0.0.1', 33326)\n",
      "(7675) accepted ('127.0.0.1', 33342)\n",
      "127.0.0.1 - - [22/Jul/2024 16:56:19] \"GET /js/three/three.js HTTP/1.1\" 200 945109 0.001809\n",
      "127.0.0.1 - - [22/Jul/2024 16:56:19] \"GET /js/three/WebGL.js HTTP/1.1\" 200 1857 0.000783\n",
      "127.0.0.1 - - [22/Jul/2024 16:56:19] \"GET /js/three/GLTFLoader.js HTTP/1.1\" 200 83998 0.000806\n",
      "127.0.0.1 - - [22/Jul/2024 16:56:19] \"GET /js/three/CSS2DRenderer.js HTTP/1.1\" 200 3782 0.000626\n",
      "127.0.0.1 - - [22/Jul/2024 16:56:19] \"GET /css/normalize.css HTTP/1.1\" 200 7122 0.000570\n",
      "127.0.0.1 - - [22/Jul/2024 16:56:19] \"GET /css/layout.css HTTP/1.1\" 200 42604 0.000527\n",
      "127.0.0.1 - - [22/Jul/2024 16:56:19] \"GET /js/gsap3/gsap.js HTTP/1.1\" 200 114391 0.000522\n",
      "127.0.0.1 - - [22/Jul/2024 16:56:19] \"GET /js/gsap3/DrawSVGPlugin.js HTTP/1.1\" 200 7652 0.000711\n",
      "127.0.0.1 - - [22/Jul/2024 16:56:19] \"GET /js/gsap3/ScrambleTextPlugin.js HTTP/1.1\" 200 15853 0.000530\n",
      "127.0.0.1 - - [22/Jul/2024 16:56:19] \"GET /img/hud-darken.png HTTP/1.1\" 200 274031 0.001489\n",
      "127.0.0.1 - - [22/Jul/2024 16:56:19] \"GET /img/hud-ring.png HTTP/1.1\" 200 18623 0.000763\n",
      "127.0.0.1 - - [22/Jul/2024 16:56:19] \"GET /img/hud-ring-inner.png HTTP/1.1\" 200 8780 0.000521\n",
      "127.0.0.1 - - [22/Jul/2024 16:56:19] \"GET /img/shadow.png HTTP/1.1\" 200 54074 0.000485\n",
      "127.0.0.1 - - [22/Jul/2024 16:56:19] \"GET /js/threed.js HTTP/1.1\" 200 88854 0.000546\n",
      "127.0.0.1 - - [22/Jul/2024 16:56:19] \"GET /img/instructions/step1_6.jpg HTTP/1.1\" 200 65956 0.000767\n",
      "127.0.0.1 - - [22/Jul/2024 16:56:19] \"GET /img/instructions/step2.jpg HTTP/1.1\" 200 91224 0.000867\n",
      "127.0.0.1 - - [22/Jul/2024 16:56:19] \"GET /img/instructions/step3.jpg HTTP/1.1\" 200 97891 0.000664\n",
      "127.0.0.1 - - [22/Jul/2024 16:56:19] \"GET /img/instructions/step4.jpg HTTP/1.1\" 200 77950 0.000559\n",
      "127.0.0.1 - - [22/Jul/2024 16:56:19] \"GET /img/instructions/step7.jpg HTTP/1.1\" 200 47249 0.000499\n",
      "127.0.0.1 - - [22/Jul/2024 16:56:19] \"GET /img/instructions/step5.jpg HTTP/1.1\" 200 80103 0.000521\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/media/saul/Datos/Asignaturas/09MIAR  - TFM/auto-docking-iss/iss-sim.spacex.com js/three/three.js\n",
      "###########\n",
      "/media/saul/Datos/Asignaturas/09MIAR  - TFM/auto-docking-iss/iss-sim.spacex.com js/three/WebGL.js\n",
      "###########\n",
      "/media/saul/Datos/Asignaturas/09MIAR  - TFM/auto-docking-iss/iss-sim.spacex.com js/three/GLTFLoader.js\n",
      "###########\n",
      "/media/saul/Datos/Asignaturas/09MIAR  - TFM/auto-docking-iss/iss-sim.spacex.com js/three/CSS2DRenderer.js\n",
      "###########\n",
      "/media/saul/Datos/Asignaturas/09MIAR  - TFM/auto-docking-iss/iss-sim.spacex.com css/normalize.css\n",
      "###########\n",
      "/media/saul/Datos/Asignaturas/09MIAR  - TFM/auto-docking-iss/iss-sim.spacex.com css/layout.css\n",
      "###########\n",
      "/media/saul/Datos/Asignaturas/09MIAR  - TFM/auto-docking-iss/iss-sim.spacex.com js/gsap3/gsap.js\n",
      "###########\n",
      "/media/saul/Datos/Asignaturas/09MIAR  - TFM/auto-docking-iss/iss-sim.spacex.com js/gsap3/DrawSVGPlugin.js\n",
      "###########\n",
      "/media/saul/Datos/Asignaturas/09MIAR  - TFM/auto-docking-iss/iss-sim.spacex.com js/gsap3/ScrambleTextPlugin.js\n",
      "###########\n",
      "/media/saul/Datos/Asignaturas/09MIAR  - TFM/auto-docking-iss/iss-sim.spacex.com img/hud-darken.png\n",
      "###########\n",
      "/media/saul/Datos/Asignaturas/09MIAR  - TFM/auto-docking-iss/iss-sim.spacex.com img/hud-ring.png\n",
      "###########\n",
      "/media/saul/Datos/Asignaturas/09MIAR  - TFM/auto-docking-iss/iss-sim.spacex.com img/hud-ring-inner.png\n",
      "###########\n",
      "/media/saul/Datos/Asignaturas/09MIAR  - TFM/auto-docking-iss/iss-sim.spacex.com img/shadow.png\n",
      "###########\n",
      "/media/saul/Datos/Asignaturas/09MIAR  - TFM/auto-docking-iss/iss-sim.spacex.com js/threed.js\n",
      "###########\n",
      "/media/saul/Datos/Asignaturas/09MIAR  - TFM/auto-docking-iss/iss-sim.spacex.com img/instructions/step1_6.jpg\n",
      "###########\n",
      "/media/saul/Datos/Asignaturas/09MIAR  - TFM/auto-docking-iss/iss-sim.spacex.com img/instructions/step2.jpg\n",
      "###########\n",
      "/media/saul/Datos/Asignaturas/09MIAR  - TFM/auto-docking-iss/iss-sim.spacex.com img/instructions/step3.jpg\n",
      "###########\n",
      "/media/saul/Datos/Asignaturas/09MIAR  - TFM/auto-docking-iss/iss-sim.spacex.com img/instructions/step4.jpg\n",
      "###########\n",
      "/media/saul/Datos/Asignaturas/09MIAR  - TFM/auto-docking-iss/iss-sim.spacex.com img/instructions/step7.jpg\n",
      "###########\n",
      "/media/saul/Datos/Asignaturas/09MIAR  - TFM/auto-docking-iss/iss-sim.spacex.com img/instructions/step5.jpg\n",
      "###########\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "127.0.0.1 - - [22/Jul/2024 16:56:19] \"GET /socket.io/?EIO=4&transport=polling&t=P3QyOE7 HTTP/1.1\" 200 278 0.000699\n",
      "127.0.0.1 - - [22/Jul/2024 16:56:19] \"GET /img/hud/ForwardViewSprites.json HTTP/1.1\" 200 52530 0.001128\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/media/saul/Datos/Asignaturas/09MIAR  - TFM/auto-docking-iss/iss-sim.spacex.com img/hud/ForwardViewSprites.json\n",
      "###########\n",
      "/media/saul/Datos/Asignaturas/09MIAR  - TFM/auto-docking-iss/iss-sim.spacex.com 3d/iss.glb\n",
      "###########\n",
      "/media/saul/Datos/Asignaturas/09MIAR  - TFM/auto-docking-iss/iss-sim.spacex.com img/texture_wormhole.jpg\n",
      "###########\n",
      "/media/saul/Datos/Asignaturas/09MIAR  - TFM/auto-docking-iss/iss-sim.spacex.com img/navball.png\n",
      "###########\n",
      "/media/saul/Datos/Asignaturas/09MIAR  - TFM/auto-docking-iss/iss-sim.spacex.com img/earth.jpg\n",
      "###########\n",
      "/media/saul/Datos/Asignaturas/09MIAR  - TFM/auto-docking-iss/iss-sim.spacex.com img/texture_fire.jpg\n",
      "###########\n",
      "/media/saul/Datos/Asignaturas/09MIAR  - TFM/auto-docking-iss/iss-sim.spacex.com img/texture_star.jpg\n",
      "###########\n",
      "/media/saul/Datos/Asignaturas/09MIAR  - TFM/auto-docking-iss/iss-sim.spacex.com img/hud/ForwardViewSprites2.png\n",
      "###########\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "127.0.0.1 - - [22/Jul/2024 16:56:19] \"GET /img/texture_wormhole.jpg HTTP/1.1\" 200 33695 0.000840\n",
      "127.0.0.1 - - [22/Jul/2024 16:56:19] \"GET /img/navball.png HTTP/1.1\" 200 141901 0.000775\n",
      "127.0.0.1 - - [22/Jul/2024 16:56:19] \"GET /img/earth.jpg HTTP/1.1\" 200 961168 0.001553\n",
      "127.0.0.1 - - [22/Jul/2024 16:56:19] \"GET /img/texture_fire.jpg HTTP/1.1\" 200 17345 0.000564\n",
      "127.0.0.1 - - [22/Jul/2024 16:56:19] \"GET /img/texture_star.jpg HTTP/1.1\" 200 1939 0.000473\n",
      "127.0.0.1 - - [22/Jul/2024 16:56:19] \"GET /img/hud/ForwardViewSprites2.png HTTP/1.1\" 200 43810 0.000486\n",
      "127.0.0.1 - - [22/Jul/2024 16:56:19] \"POST /socket.io/?EIO=4&transport=polling&t=P3QyOIb&sid=U2R2fYJsPOBDuhZIAAAA HTTP/1.1\" 200 217 0.000608\n",
      "(7675) accepted ('127.0.0.1', 33354)\n",
      "127.0.0.1 - - [22/Jul/2024 16:56:19] \"GET /socket.io/?EIO=4&transport=polling&t=P3QyOIc&sid=U2R2fYJsPOBDuhZIAAAA HTTP/1.1\" 200 181 0.000134\n",
      "127.0.0.1 - - [22/Jul/2024 16:56:19] \"GET /socket.io/?EIO=4&transport=polling&t=P3QyOKO&sid=U2R2fYJsPOBDuhZIAAAA HTTP/1.1\" 200 181 0.000139\n",
      "127.0.0.1 - - [22/Jul/2024 16:56:19] \"GET /img/favicon.ico HTTP/1.1\" 200 15716 0.001017\n",
      "127.0.0.1 - - [22/Jul/2024 16:56:19] \"GET /3d/iss.glb HTTP/1.1\" 200 19160374 0.158133\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/media/saul/Datos/Asignaturas/09MIAR  - TFM/auto-docking-iss/iss-sim.spacex.com img/favicon.ico\n",
      "###########\n"
     ]
    }
   ],
   "source": [
    "agent = PPOAgent(\"iss_auto\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reset\n",
      "Stopped\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([ 0.1304075, -0.00744  , -0.014815 ,  0.       ,  0.       ,\n",
       "        0.       ,  0.       ,  0.       ,  0.       ])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "agent.reset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "agent.run()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "miar_tfm",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import random\n",
    "from math import sqrt\n",
    "import torch\n",
    "from torchsummary import summary\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from env import Env\n",
    "import os\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ActorCritic(torch.nn.Module):\n",
    "    def __init__(self,state_dims, n_actions):\n",
    "        super(ActorCritic, self).__init__()\n",
    "\n",
    "        self.fc1 = torch.nn.Linear(state_dims, 124)\n",
    "        torch.nn.init.kaiming_uniform_(self.fc1.weight)\n",
    "\n",
    "        self.fc2 = torch.nn.Linear(124, 64)\n",
    "        torch.nn.init.kaiming_uniform_(self.fc2.weight)\n",
    "\n",
    "        self.actor = torch.nn.Linear(64, n_actions)\n",
    "        torch.nn.init.kaiming_uniform_(self.actor.weight)\n",
    "        self.critic = torch.nn.Linear(64, 1)\n",
    "        torch.nn.init.kaiming_uniform_(self.critic.weight)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = torch.nn.functional.relu(self.fc1(x))\n",
    "        x = torch.nn.functional.relu(self.fc2(x))\n",
    "\n",
    "        policy = self.actor(x)\n",
    "        value = self.critic(x)\n",
    "\n",
    "        return policy, value\n",
    "\n",
    "def np_to_tensor(x):\n",
    "    return torch.tensor(x).to(torch.float32)\n",
    "    \n",
    "model = ActorCritic(state_dims=9,n_actions = 7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "summary(model, input_size=(1, 9))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PPOAgent:\n",
    "    # Algoritmo de optimización de PPO\n",
    "    def __init__(self, env_name):\n",
    "        # Initialization del entorno y parametros de PPO\n",
    "        self.env_name = env_name\n",
    "        self.env = Env()\n",
    "        self.env.run()\n",
    "        self.action_size = 6\n",
    "        self.EPISODES, self.max_average = 500, -500000 # specific for pong\n",
    "        self.lr = 0.001\n",
    "        self.bs = 10000\n",
    "        self.gamma = 0.99\n",
    "\n",
    "        # Hyperparametros para pre-procesamiento obs -> state\n",
    "        self.REM_STEP = 3\n",
    "\n",
    "        # Hyperparametros propios de PPO\n",
    "        self.LOSS_CLIPPING = 0.2\n",
    "        self.ENTROPY_LOSS = 5e-3\n",
    "        self.epochs = 10\n",
    "\n",
    "        # Inicialización de la memoria\n",
    "        self.states, self.actions, self.rewards, self.values, self.actions_probs = [], [], [], [], []\n",
    "        self.scores, self.episodes, self.average = [], [], []\n",
    "\n",
    "        # Preparar rutas de almacenamiento de estados y resultados\n",
    "        self.Save_Path = 'Models'\n",
    "        self.state_size = 3*self.REM_STEP\n",
    "        self.state_memory = np.zeros(self.state_size)\n",
    "\n",
    "        self.image_memory = np.zeros(self.state_size)\n",
    "\n",
    "        if not os.path.exists(self.Save_Path): os.makedirs(self.Save_Path)\n",
    "        self.path = '{}_PPO_{}'.format(self.env_name, self.lr)\n",
    "        self.Model_name = os.path.join(self.Save_Path, self.path)\n",
    "\n",
    "        # Crear el modelo\n",
    "        self.model = ActorCritic(state_dims=self.state_size,n_actions = self.action_size)\n",
    "        # Preparar optimizador.\n",
    "        self.optimizer = torch.optim.Adam(self.model.parameters(), lr=self.lr)\n",
    "\n",
    "\n",
    "    def remember(self, state, action, reward, value, action_prob):\n",
    "        # Almacenamiento de todas las variables que definen una transición para PPO\n",
    "        self.states.append(state)\n",
    "        action_onehot = np.zeros([self.action_size])\n",
    "        action_onehot[action] = 1\n",
    "        self.actions.append(action_onehot)\n",
    "        self.rewards.append(reward)\n",
    "        self.values.append(value)\n",
    "        self.actions_probs.append(action_prob)\n",
    "\n",
    "    def discount_rewards(self, reward):\n",
    "        # Computamos los gamma-discounted rewards sobre un episodio\n",
    "        running_add = 0\n",
    "        discounted_r = np.zeros_like(reward)\n",
    "        for i in reversed(range(0,len(reward))):\n",
    "            if reward[i] != 0: # reseteamos debido a re-inicio de partida (pong specific!)\n",
    "                running_add = 0\n",
    "            running_add = running_add * self.gamma + reward[i]\n",
    "            discounted_r[i] = running_add\n",
    "\n",
    "        # Normalización para reducir la varianza\n",
    "        discounted_r -= np.mean(discounted_r)\n",
    "        discounted_r /= np.std(discounted_r)\n",
    "        return discounted_r\n",
    "\n",
    "    def fit_actor(self, states, actions, advantages, actions_probs, verbose=False):\n",
    "        self.model.train()\n",
    "\n",
    "        idx, track_loss = 0, 0\n",
    "        indexes = np.arange(0, states.shape[0])\n",
    "        np.random.shuffle(indexes)\n",
    "        for i_epoch in range(states.shape[0]//self.bs):\n",
    "            indexes_batch = indexes[idx:idx+self.bs]\n",
    "\n",
    "            # Seleccionamos un batch de la trayectoria\n",
    "            states_batch = np_to_tensor(states[indexes_batch,:])\n",
    "            actions_batch = np_to_tensor(actions[indexes_batch,:].squeeze())\n",
    "            advantages_batch = np_to_tensor(advantages[indexes_batch])\n",
    "            actions_probs_batch = np_to_tensor(actions_probs[indexes_batch])\n",
    "\n",
    "            # Hacemos forward al actor-critic dado el estado actual\n",
    "            logits, _ = self.model(states_batch)\n",
    "\n",
    "            # Obtenemos las probabilidades de la acción a partir de los logits\n",
    "            prob = torch.nn.functional.softmax(logits, -1)\n",
    "\n",
    "            # Seleccionamos probabilidad de acción realizada y antigua\n",
    "            prob = torch.sum(prob * actions_batch, -1)\n",
    "            old_prob = torch.sum(actions_probs_batch * actions_batch, -1)\n",
    "\n",
    "            # Calculamos el ratio\n",
    "            r = prob/(old_prob + 1e-10)\n",
    "\n",
    "            # Ponderación por función ventaja\n",
    "            p1 = r * advantages_batch\n",
    "\n",
    "            # Clip\n",
    "            p2 = torch.clip(r, min=1 - self.LOSS_CLIPPING, max=1 + self.LOSS_CLIPPING) * advantages_batch\n",
    "\n",
    "            # Calculate loss\n",
    "            ppo_loss = -torch.mean(torch.minimum(p1, p2) + self.ENTROPY_LOSS * -(prob * torch.log(prob + 1e-10)))\n",
    "\n",
    "            # Computamos gradientes\n",
    "            ppo_loss.backward()\n",
    "            # Actualizamos los pesos\n",
    "            self.optimizer.step()\n",
    "            # Limpiamos gradientes del modelo\n",
    "            self.optimizer.zero_grad()\n",
    "            # Actualizamos iterador de batch\n",
    "            idx += self.bs\n",
    "            track_loss += ppo_loss.item()/(states.shape[0]//self.bs)\n",
    "\n",
    "            if verbose:\n",
    "                print(str(i_epoch) + \"/\" + str(states.shape[0]//self.bs) + \" - loss: \" + str(ppo_loss.item()), end=\"\\r\")\n",
    "        if verbose:\n",
    "            print(str(i_epoch) + \"/\" + str(states.shape[0]//self.bs) + \" - loss: \" + str(track_loss), end=\"\\n\")\n",
    "\n",
    "        self.model.eval()\n",
    "\n",
    "    def fit_critic(self, states, discounted_r, verbose=False):\n",
    "        self.model.train()\n",
    "\n",
    "        idx, track_loss = 0, 0\n",
    "        indexes = np.arange(0, states.shape[0])\n",
    "        np.random.shuffle(indexes)\n",
    "        for i_epoch in range(states.shape[0]//self.bs):\n",
    "            indexes_batch = indexes[idx:idx+self.bs]\n",
    "\n",
    "            # Seleccionamos un batch de la trayectoria\n",
    "            states_batch = np_to_tensor(states[indexes_batch,:])\n",
    "            discounted_r_batch = np_to_tensor(discounted_r[indexes_batch])\n",
    "\n",
    "            # Hacemos forward al actor-critic dado el estado actual\n",
    "            _, values = self.model(states_batch)\n",
    "\n",
    "            # Obtenemos criterios de optimización\n",
    "            critic_loss = torch.mean((discounted_r_batch.detach() - values).pow(2))\n",
    "\n",
    "            # Computamos gradientes\n",
    "            critic_loss.backward()\n",
    "            # Actualizamos los pesos\n",
    "            self.optimizer.step()\n",
    "            # Limpiamos gradientes del modelo\n",
    "            self.optimizer.zero_grad()\n",
    "            # Actualizamos iterador de batch\n",
    "            idx += self.bs\n",
    "            track_loss += critic_loss.item()/(states.shape[0]//self.bs)\n",
    "            if verbose:\n",
    "                print(str(i_epoch) + \"/\" + str(states.shape[0]//self.bs) + \" - loss: \" + str(critic_loss.item()), end=\"\\r\")\n",
    "        if verbose:\n",
    "            print(str(i_epoch) + \"/\" + str(states.shape[0]//self.bs) + \" - loss: \" + str(track_loss), end=\"\\n\")\n",
    "\n",
    "        self.model.eval()\n",
    "\n",
    "    # Función principal de entrenamiento\n",
    "    def replay(self):\n",
    "        # Pasamos la memoria de listas a arrays\n",
    "        states = np.vstack(self.states)\n",
    "        actions = np.vstack(self.actions)\n",
    "        values = np.array(self.values)\n",
    "        actions_probs = np.array(self.actions_probs)\n",
    "\n",
    "        # Compute discounted rewards\n",
    "        discounted_r = self.discount_rewards(self.rewards)\n",
    "\n",
    "        # Compute advantages\n",
    "        advantages = discounted_r - values\n",
    "\n",
    "        print(\"Trajectory length: \", len(self.actions))\n",
    "        # Training Actor and Critic networks\n",
    "        for i in range(self.epochs):\n",
    "            print(f'Fit actor. Epoch:{i}',end='\\r')\n",
    "            self.fit_actor(states, actions, advantages, actions_probs)\n",
    "        print(\"\")\n",
    "        for i in range(self.epochs):\n",
    "            print(f\"Fit Critic. Epoch: \", i,end='\\r')\n",
    "            self.fit_critic(states, discounted_r)\n",
    "        print(\"\")\n",
    "\n",
    "        # reset training memory\n",
    "        self.states, self.actions, self.rewards, self.values, self.actions_probs = [], [], [], [], []\n",
    "\n",
    "    def load(self, model_name):\n",
    "        self.model.load_state_dict(torch.load(model_name))\n",
    "\n",
    "    def save(self,episode,score):\n",
    "        torch.save(self.model.state_dict(), self.Model_name + '_' + str(episode) + '_' + str(score) + '.pth')\n",
    "\n",
    "    # Función para visualizar la evolución del entrenamiento\n",
    "    plt.figure(figsize=(18, 9))\n",
    "    def PlotModel(self, score, episode):\n",
    "        self.scores.append(score)\n",
    "        self.episodes.append(episode)\n",
    "        self.average.append(sum(self.scores[-50:]) / len(self.scores[-50:]))\n",
    "        if str(episode)[-2:] == \"00\":# much faster than episode % 100\n",
    "            plt.plot(self.episodes, self.scores, 'b')\n",
    "            plt.plot(self.episodes, self.average, 'r')\n",
    "            plt.ylabel('Score', fontsize=18)\n",
    "            plt.xlabel('Steps', fontsize=18)\n",
    "            try:\n",
    "                plt.savefig(self.path+\".png\")\n",
    "            except OSError:\n",
    "                pass\n",
    "\n",
    "        return self.average[-1]\n",
    "\n",
    "    def process_state(self,state):\n",
    "        self.state_memory = np.concatenate((np.array(state) / 40, self.state_memory))[:self.state_size]\n",
    "        return self.state_memory\n",
    "\n",
    "        return np.array(state[:3] + state[6:9])\n",
    "\n",
    "    # Función de pre-procesamiento observación --> estado\n",
    "    def GetImage(self, frame):\n",
    "        # Cropeamos la información de la imagen\n",
    "        frame_cropped = frame[35:195:2, ::2,:]\n",
    "        # if frame_cropped.shape[0] != self.COLS or frame_cropped.shape[1] != self.ROWS:\n",
    "        #     # OpenCV resize function\n",
    "        #     frame_cropped = cv2.resize(frame, (self.COLS, self.ROWS), interpolation=cv2.INTER_CUBIC)\n",
    "\n",
    "        # Convertimos a rgb\n",
    "        frame_rgb = 0.299*frame_cropped[:,:,0] + 0.587*frame_cropped[:,:,1] + 0.114*frame_cropped[:,:,2]\n",
    "\n",
    "        # Convertimos a máscara binaria para acelerar el entrenamiento\n",
    "        frame_rgb[frame_rgb < 100] = 0\n",
    "        frame_rgb[frame_rgb >= 100] = 255\n",
    "\n",
    "        # Normalizamos los valroes de intensidad\n",
    "        new_frame = np.array(frame_rgb).astype(np.float32) / 255.0\n",
    "\n",
    "        # push our data by 1 frame, similar as deq() function work\n",
    "        self.image_memory = np.roll(self.image_memory, 1, axis = 0)\n",
    "\n",
    "        # inserting new frame to free space\n",
    "        self.image_memory[0,:,:] = new_frame\n",
    "\n",
    "        return np.expand_dims(self.image_memory, axis=0)\n",
    "\n",
    "    # Función para resetear el entorno tras acabar trayectoria\n",
    "    def reset(self):\n",
    "        frame = self.env.reset()\n",
    "        self.state_memory = np.zeros(self.state_size)\n",
    "        return self.process_state(frame)\n",
    "\n",
    "    # Función para interacción agente-entorno\n",
    "    def step(self, action):\n",
    "        next_state, reward, done = self.env.step(action)\n",
    "        next_state = self.process_state(next_state)\n",
    "        return next_state, reward, done\n",
    "\n",
    "    # Función principal de exploración + entrenamiento\n",
    "    def run(self):\n",
    "        # unique_actions_list = []\n",
    "        # Bucle de episodios de entrenamiento\n",
    "        for e in range(self.EPISODES):\n",
    "            self.model.eval()\n",
    "            state = self.reset()\n",
    "            done, score, SAVING = False, 0, ''\n",
    "            # Bucle para recopilar la trayectoria\n",
    "            while not done:\n",
    "                time.sleep(0.02)\n",
    "                # Elección de acción por parte del actor\n",
    "                action, value = self.model(np_to_tensor(state))\n",
    "                # Pasamos a distribución de probabilidad\n",
    "                action_prob = torch.softmax(action, -1).squeeze().detach().numpy()\n",
    "                # Hacemos un sampling de la acción - exploraicón\n",
    "                action = np.random.choice(self.action_size, p=action_prob)\n",
    "                value = value.squeeze().detach().numpy()\n",
    "                # Dada la acción seleccionado, interaccionar con el entorno y recibir nuevo estado y recompensa\n",
    "                # if action not in unique_actions_list:\n",
    "                #     unique_actions_list.append(action)\n",
    "                next_state, reward, done = self.step(action)\n",
    "                # print(next_state)\n",
    "                # Almacenamiento de memoria para posterior entrenamiento\n",
    "                self.remember(state, action, reward, value, action_prob)\n",
    "                # Update current state\n",
    "                state = next_state\n",
    "                score += reward\n",
    "\n",
    "                if done: # Estado terminal\n",
    "                    # Al acabar la trayectoria, almacenamos KPIs para curva de entrenamiento y almacenamos mejor modelo\n",
    "                    average = self.PlotModel(score, e)\n",
    "                    if average >= self.max_average:\n",
    "                        self.max_average = average\n",
    "                        self.save(e,score)\n",
    "                        SAVING = \"SAVING\"\n",
    "                    else:\n",
    "                        SAVING = \"\"\n",
    "                    print(\"episode: {}/{}, score: {}, average: {:.2f} {}\".format(e, self.EPISODES, score, average, SAVING))\n",
    "\n",
    "                    # print(unique_actions_list)\n",
    "                    # Entrenamos un modelo haciendo un replay de la trayectoria\n",
    "                    self.replay()\n",
    "                    print(\"End buffer replay\")\n",
    "\n",
    "        # close environemnt when finish training\n",
    "        self.env.close()\n",
    "\n",
    "    # Función de testeo\n",
    "    def test(self, Actor_name, Critic_name):\n",
    "        self.load(model_name)\n",
    "        self.model.eval()\n",
    "\n",
    "        for e in range(100):\n",
    "            state = self.reset()\n",
    "            done = False\n",
    "            score = 0\n",
    "            while not done:\n",
    "                # Al testear, seleccionamos la acción con mayor probabilidad (no muestreo)\n",
    "                action, value = self.model(np_to_tensor(state))\n",
    "                state, reward, done = self.step(action)\n",
    "                score += reward\n",
    "                if done:\n",
    "                    print(\"episode: {}/{}, score: {}\".format(e, self.EPISODES, score))\n",
    "                    break\n",
    "        self.env.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "agent = PPOAgent(\"iss_auto\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "agent.reset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "agent.run()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "miar_tfm",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import random\n",
    "from math import sqrt\n",
    "import torch\n",
    "from torchsummary import summary\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import random\n",
    "from math import sqrt\n",
    "import torch\n",
    "from torchsummary import summary\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from env2 import Env\n",
    "import os\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ActorCritic(torch.nn.Module):\n",
    "    def __init__(self,state_dims, n_actions):\n",
    "        super(ActorCritic, self).__init__()\n",
    "\n",
    "        self.fc1 = torch.nn.Linear(state_dims, 124)\n",
    "        torch.nn.init.kaiming_uniform_(self.fc1.weight)\n",
    "\n",
    "        self.fc2 = torch.nn.Linear(124, 64)\n",
    "        torch.nn.init.kaiming_uniform_(self.fc2.weight)\n",
    "\n",
    "        self.actor = torch.nn.Linear(64, n_actions)\n",
    "        torch.nn.init.kaiming_uniform_(self.actor.weight)\n",
    "        self.critic = torch.nn.Linear(64, 1)\n",
    "        torch.nn.init.kaiming_uniform_(self.critic.weight)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = torch.nn.functional.relu(self.fc1(x))\n",
    "        x = torch.nn.functional.relu(self.fc2(x))\n",
    "\n",
    "        policy = self.actor(x)\n",
    "        value = self.critic(x)\n",
    "\n",
    "        return policy, value\n",
    "\n",
    "def np_to_tensor(x):\n",
    "    return torch.tensor(x).to(torch.float32)\n",
    "    \n",
    "model = ActorCritic(state_dims=6, n_actions = 6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "summary(model, input_size=(1, 6))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PPOAgent:\n",
    "    # Algoritmo de optimización de PPO\n",
    "    def __init__(self, env_name):\n",
    "        # Initialization del entorno y parametros de PPO\n",
    "        self.env_name = env_name\n",
    "        self.env = Env()\n",
    "        self.env.run()\n",
    "        self.action_size = 6\n",
    "        self.EPISODES, self.max_average = 500, -500000 # specific for pong\n",
    "        self.lr = 0.001\n",
    "        self.bs = 10000\n",
    "        self.gamma = 0.99\n",
    "\n",
    "        # Hyperparametros para pre-procesamiento obs -> state\n",
    "        self.REM_STEP = 3\n",
    "\n",
    "        # Hyperparametros propios de PPO\n",
    "        self.LOSS_CLIPPING = 0.2\n",
    "        self.ENTROPY_LOSS = 5e-3\n",
    "        self.epochs = 10\n",
    "\n",
    "        # Inicialización de la memoria\n",
    "        self.states, self.actions, self.rewards, self.values, self.actions_probs = [], [], [], [], []\n",
    "        self.scores, self.episodes, self.average = [], [], []\n",
    "\n",
    "        # Preparar rutas de almacenamiento de estados y resultados\n",
    "        self.Save_Path = 'Models3'\n",
    "        self.state_size = 6 #3*self.REM_STEP\n",
    "        self.state_memory = np.zeros(self.state_size)\n",
    "\n",
    "        self.image_memory = np.zeros(self.state_size)\n",
    "\n",
    "        if not os.path.exists(self.Save_Path): os.makedirs(self.Save_Path)\n",
    "        self.path = '{}_PPO_{}'.format(self.env_name, self.lr)\n",
    "        self.Model_name = os.path.join(self.Save_Path, self.path)\n",
    "\n",
    "        # Crear el modelo\n",
    "        self.model = ActorCritic(state_dims=self.state_size,n_actions = self.action_size)\n",
    "        # Preparar optimizador.\n",
    "        self.optimizer = torch.optim.Adam(self.model.parameters(), lr=self.lr)\n",
    "\n",
    "\n",
    "    def remember(self, state, action, reward, value, action_prob):\n",
    "        # Almacenamiento de todas las variables que definen una transición para PPO\n",
    "        self.states.append(state)\n",
    "        action_onehot = np.zeros([self.action_size])\n",
    "        action_onehot[action] = 1\n",
    "        self.actions.append(action_onehot)\n",
    "        self.rewards.append(reward)\n",
    "        self.values.append(value)\n",
    "        self.actions_probs.append(action_prob)\n",
    "\n",
    "    def discount_rewards(self, reward):\n",
    "        # Computamos los gamma-discounted rewards sobre un episodio\n",
    "        running_add = 0\n",
    "        discounted_r = np.zeros_like(reward)\n",
    "        for i in reversed(range(0,len(reward))):\n",
    "            if reward[i] != 0: # reseteamos debido a re-inicio de partida (pong specific!)\n",
    "                running_add = 0\n",
    "            running_add = running_add * self.gamma + reward[i]\n",
    "            discounted_r[i] = running_add\n",
    "\n",
    "        # Normalización para reducir la varianza\n",
    "        discounted_r -= np.mean(discounted_r)\n",
    "        discounted_r /= np.std(discounted_r)\n",
    "        return discounted_r\n",
    "\n",
    "    def fit_actor(self, states, actions, advantages, actions_probs, verbose=False):\n",
    "        self.model.train()\n",
    "\n",
    "        idx, track_loss = 0, 0\n",
    "        indexes = np.arange(0, states.shape[0])\n",
    "        np.random.shuffle(indexes)\n",
    "        for i_epoch in range(states.shape[0]//self.bs):\n",
    "            indexes_batch = indexes[idx:idx+self.bs]\n",
    "\n",
    "            # Seleccionamos un batch de la trayectoria\n",
    "            states_batch = np_to_tensor(states[indexes_batch,:])\n",
    "            actions_batch = np_to_tensor(actions[indexes_batch,:].squeeze())\n",
    "            advantages_batch = np_to_tensor(advantages[indexes_batch])\n",
    "            actions_probs_batch = np_to_tensor(actions_probs[indexes_batch])\n",
    "\n",
    "            # Hacemos forward al actor-critic dado el estado actual\n",
    "            logits, _ = self.model(states_batch)\n",
    "\n",
    "            # Obtenemos las probabilidades de la acción a partir de los logits\n",
    "            prob = torch.nn.functional.softmax(logits, -1)\n",
    "\n",
    "            # Seleccionamos probabilidad de acción realizada y antigua\n",
    "            prob = torch.sum(prob * actions_batch, -1)\n",
    "            old_prob = torch.sum(actions_probs_batch * actions_batch, -1)\n",
    "\n",
    "            # Calculamos el ratio\n",
    "            r = prob/(old_prob + 1e-10)\n",
    "\n",
    "            # Ponderación por función ventaja\n",
    "            p1 = r * advantages_batch\n",
    "\n",
    "            # Clip\n",
    "            p2 = torch.clip(r, min=1 - self.LOSS_CLIPPING, max=1 + self.LOSS_CLIPPING) * advantages_batch\n",
    "\n",
    "            # Calculate loss\n",
    "            ppo_loss = -torch.mean(torch.minimum(p1, p2) + self.ENTROPY_LOSS * -(prob * torch.log(prob + 1e-10)))\n",
    "\n",
    "            # Computamos gradientes\n",
    "            ppo_loss.backward()\n",
    "            # Actualizamos los pesos\n",
    "            self.optimizer.step()\n",
    "            # Limpiamos gradientes del modelo\n",
    "            self.optimizer.zero_grad()\n",
    "            # Actualizamos iterador de batch\n",
    "            idx += self.bs\n",
    "            track_loss += ppo_loss.item()/(states.shape[0]//self.bs)\n",
    "\n",
    "            if verbose:\n",
    "                print(str(i_epoch) + \"/\" + str(states.shape[0]//self.bs) + \" - loss: \" + str(ppo_loss.item()), end=\"\\r\")\n",
    "        if verbose:\n",
    "            print(str(i_epoch) + \"/\" + str(states.shape[0]//self.bs) + \" - loss: \" + str(track_loss), end=\"\\n\")\n",
    "\n",
    "        self.model.eval()\n",
    "\n",
    "    def fit_critic(self, states, discounted_r, verbose=False):\n",
    "        self.model.train()\n",
    "\n",
    "        idx, track_loss = 0, 0\n",
    "        indexes = np.arange(0, states.shape[0])\n",
    "        np.random.shuffle(indexes)\n",
    "        for i_epoch in range(states.shape[0]//self.bs):\n",
    "            indexes_batch = indexes[idx:idx+self.bs]\n",
    "\n",
    "            # Seleccionamos un batch de la trayectoria\n",
    "            states_batch = np_to_tensor(states[indexes_batch,:])\n",
    "            discounted_r_batch = np_to_tensor(discounted_r[indexes_batch])\n",
    "\n",
    "            # Hacemos forward al actor-critic dado el estado actual\n",
    "            _, values = self.model(states_batch)\n",
    "\n",
    "            # Obtenemos criterios de optimización\n",
    "            critic_loss = torch.mean((discounted_r_batch.detach() - values).pow(2))\n",
    "\n",
    "            # Computamos gradientes\n",
    "            critic_loss.backward()\n",
    "            # Actualizamos los pesos\n",
    "            self.optimizer.step()\n",
    "            # Limpiamos gradientes del modelo\n",
    "            self.optimizer.zero_grad()\n",
    "            # Actualizamos iterador de batch\n",
    "            idx += self.bs\n",
    "            track_loss += critic_loss.item()/(states.shape[0]//self.bs)\n",
    "            if verbose:\n",
    "                print(str(i_epoch) + \"/\" + str(states.shape[0]//self.bs) + \" - loss: \" + str(critic_loss.item()), end=\"\\r\")\n",
    "        if verbose:\n",
    "            print(str(i_epoch) + \"/\" + str(states.shape[0]//self.bs) + \" - loss: \" + str(track_loss), end=\"\\n\")\n",
    "\n",
    "        self.model.eval()\n",
    "\n",
    "    # Función principal de entrenamiento\n",
    "    def replay(self):\n",
    "        # Pasamos la memoria de listas a arrays\n",
    "        states = np.vstack(self.states)\n",
    "        actions = np.vstack(self.actions)\n",
    "        values = np.array(self.values)\n",
    "        actions_probs = np.array(self.actions_probs)\n",
    "\n",
    "        # Compute discounted rewards\n",
    "        discounted_r = self.discount_rewards(self.rewards)\n",
    "\n",
    "        # Compute advantages\n",
    "        advantages = discounted_r - values\n",
    "\n",
    "        # print(\"Trajectory length: \", len(self.actions))\n",
    "        # Training Actor and Critic networks\n",
    "        for i in range(self.epochs):\n",
    "            # print(f'Fit actor. Epoch:{i}',end='\\r')\n",
    "            self.fit_actor(states, actions, advantages, actions_probs)\n",
    "        # print(\"\")\n",
    "        for i in range(self.epochs):\n",
    "            # print(f\"Fit Critic. Epoch: \", i,end='\\r')\n",
    "            self.fit_critic(states, discounted_r)\n",
    "        # print(\"\")\n",
    "\n",
    "        # reset training memory\n",
    "        self.states, self.actions, self.rewards, self.values, self.actions_probs = [], [], [], [], []\n",
    "\n",
    "    def load(self, model_name):\n",
    "        self.model.load_state_dict(torch.load(model_name))\n",
    "\n",
    "    def save(self,episode,score):\n",
    "        torch.save(self.model.state_dict(), self.Model_name + '_' + str(episode) + '_' + str(score) + '.pth')\n",
    "\n",
    "    # Función para visualizar la evolución del entrenamiento\n",
    "    plt.figure(figsize=(18, 9))\n",
    "    def PlotModel(self, score, episode):\n",
    "        self.scores.append(score)\n",
    "        self.episodes.append(episode)\n",
    "        self.average.append(sum(self.scores[-50:]) / len(self.scores[-50:]))\n",
    "        if episode % 10:#str(episode)[-2:] == \"00\":# much faster than episode % 100\n",
    "            plt.plot(self.episodes, self.scores, 'b')\n",
    "            plt.plot(self.episodes, self.average, 'r')\n",
    "            plt.ylabel('Score', fontsize=18)\n",
    "            plt.xlabel('Steps', fontsize=18)\n",
    "            try:\n",
    "                plt.savefig(self.path+\".png\")\n",
    "            except OSError:\n",
    "                pass\n",
    "\n",
    "        return self.average[-1]\n",
    "\n",
    "    def process_state(self,state):\n",
    "        self.state_memory = np.array(state)# / 40\n",
    "        self.state_memory[:3] = self.state_memory[:3] / 40 # Position array\n",
    "        #self.state_memory = np.concatenate((np.array(state) / 40, self.state_memory))[:self.state_size]\n",
    "        return self.state_memory\n",
    "\n",
    "        return np.array(state[:3] + state[6:9])\n",
    "\n",
    "    # Función para resetear el entorno tras acabar trayectoria\n",
    "    def reset(self):\n",
    "        next_state = self.env.reset()\n",
    "        self.state_memory = np.zeros(self.state_size)\n",
    "        return self.process_state(next_state)\n",
    "\n",
    "    # Función para interacción agente-entorno\n",
    "    def step(self, action):\n",
    "        next_state, reward, done = self.env.step(action)\n",
    "        next_state = self.process_state(next_state)\n",
    "        return next_state, reward, done\n",
    "\n",
    "    # Función principal de exploración + entrenamiento\n",
    "    def run(self):\n",
    "        # unique_actions_list = []\n",
    "        # Bucle de episodios de entrenamiento\n",
    "        for e in range(self.EPISODES):\n",
    "            self.model.eval()\n",
    "            state = self.reset()\n",
    "            done, score, SAVING = False, 0, ''\n",
    "            # Bucle para recopilar la trayectoria\n",
    "            while not done:\n",
    "                #time.sleep(0.02)\n",
    "                time.sleep(1.0)\n",
    "                # Elección de acción por parte del actor\n",
    "                action, value = self.model(np_to_tensor(state))\n",
    "                # Pasamos a distribución de probabilidad\n",
    "                action_prob = torch.softmax(action, -1).squeeze().detach().numpy()\n",
    "                # Hacemos un sampling de la acción - exploraicón\n",
    "                action = np.random.choice(self.action_size, p=action_prob)\n",
    "                value = value.squeeze().detach().numpy()\n",
    "                # Dada la acción seleccionado, interaccionar con el entorno y recibir nuevo estado y recompensa\n",
    "                # if action not in unique_actions_list:\n",
    "                #     unique_actions_list.append(action)\n",
    "                next_state, reward, done = self.step(action)\n",
    "                # print(next_state)\n",
    "                # Almacenamiento de memoria para posterior entrenamiento\n",
    "                self.remember(state, action, reward, value, action_prob)\n",
    "                # Update current state\n",
    "                state = next_state\n",
    "                score += reward\n",
    "\n",
    "                if done: # Estado terminal\n",
    "                    # Al acabar la trayectoria, almacenamos KPIs para curva de entrenamiento y almacenamos mejor modelo\n",
    "                    average = self.PlotModel(score, e)\n",
    "                    if score >= self.max_average:\n",
    "                        self.max_average = score\n",
    "                        self.save(e,score)\n",
    "                        SAVING = \"SAVING\"\n",
    "                    else:\n",
    "                        SAVING = \"\"\n",
    "                    print(\"\\n episode: {}/{}, score: {}, average: {:.2f} {}\".format(e, self.EPISODES, score, average, SAVING))\n",
    "\n",
    "                    # print(unique_actions_list)\n",
    "                    # Entrenamos un modelo haciendo un replay de la trayectoria\n",
    "                    self.replay()\n",
    "                    # print(\"End buffer replay\")\n",
    "\n",
    "        # close environemnt when finish training\n",
    "        self.save(e,score)\n",
    "        self.env.close() \n",
    "\n",
    "    # Función de testeo\n",
    "    def test(self, Actor_name, Critic_name):\n",
    "        self.load(model_name)\n",
    "        self.model.eval()\n",
    "\n",
    "        for e in range(100):\n",
    "            state = self.reset()\n",
    "            done = False\n",
    "            score = 0\n",
    "            while not done:\n",
    "                # Al testear, seleccionamos la acción con mayor probabilidad (no muestreo)\n",
    "                action, value = self.model(np_to_tensor(state))\n",
    "                state, reward, done = self.step(action)\n",
    "                score += reward\n",
    "                if done:\n",
    "                    print(\"episode: {}/{}, score: {}\".format(e, self.EPISODES, score))\n",
    "                    break\n",
    "        self.env.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "agent = PPOAgent(\"iss_auto\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "agent.reset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reset\n",
      " Steps: 38/200. Actions: [-2, 0, 2]. Reward: -0.55\n",
      " episode: 0/500, score: -2.5999999999999996, average: -2.60 SAVING\n",
      "Reset\n",
      " Steps: 96/200. Actions: [-2, 2, 2]. Reward: -0.33\n",
      " episode: 1/500, score: -8.499999999999995, average: -5.55 \n",
      "Reset\n",
      " Steps: 118/200. Actions: [-2, 0, 2]. Reward: -0.55\n",
      " episode: 2/500, score: -12.55000000000001, average: -7.88 \n",
      "Reset\n",
      " Steps: 44/200. Actions: [-2, 1, 0]. Reward: -0.255\n",
      " episode: 3/500, score: 7.050000000000001, average: -4.15 SAVING\n",
      "Reset\n",
      " Steps: 38/200. Actions: [-2, 1, 2]. Reward: 0.3353\n",
      " episode: 4/500, score: 4.199999999999999, average: -2.48 \n",
      "Reset\n",
      " Steps: 142/200. Actions: [-2, -2, 2]. Reward: -0.9\n",
      " episode: 5/500, score: -0.39999999999998237, average: -2.13 \n",
      "Reset\n",
      " Steps: 51/200. Actions: [-2, 2, 1]. Reward: -0.335\n",
      " episode: 6/500, score: -0.15000000000000152, average: -1.85 \n",
      "Reset\n",
      " Steps: 96/200. Actions: [0, -2, 2]. Reward: -0.935\n",
      " episode: 7/500, score: -21.600000000000023, average: -4.32 \n",
      "Reset\n",
      " Steps: 82/200. Actions: [-2, -1, 1]. Reward: -0.95\n",
      " episode: 8/500, score: -8.499999999999996, average: -4.78 \n",
      "Reset\n",
      " Steps: 24/200. Actions: [-2, -2, 2]. Reward: -0.9\n",
      " episode: 9/500, score: 0.050000000000000044, average: -4.30 \n",
      "Reset\n",
      " Steps: 145/200. Actions: [-2, -1, 2]. Reward: -0.35\n",
      " episode: 10/500, score: -12.300000000000013, average: -5.03 \n",
      "Reset\n",
      " Steps: 165/200. Actions: [-2, -2, 1]. Reward: -0.3\n",
      " episode: 11/500, score: -41.249999999999936, average: -8.05 \n",
      "Reset\n",
      " Steps: 68/200. Actions: [-2, 2, 0]. Reward: -0.95\n",
      " episode: 12/500, score: 3.3999999999999972, average: -7.17 \n",
      "Reset\n",
      " Steps: 114/200. Actions: [-2, -2, 2]. Reward: -0.9\n",
      " episode: 13/500, score: -35.05000000000002, average: -9.16 \n",
      "Reset\n",
      " Steps: 112/200. Actions: [-2, -2, 2]. Reward: -0.9\n",
      " episode: 14/500, score: -28.54999999999999, average: -10.45 \n",
      "Reset\n",
      " Steps: 87/200. Actions: [-2, 2, 1]. Reward: -0.33\n",
      " episode: 15/500, score: 0.7999999999999969, average: -9.75 \n",
      "Reset\n",
      " Steps: 69/200. Actions: [-2, 2, -2]. Reward: -0.93\n",
      " episode: 16/500, score: 7.800000000000004, average: -8.71 SAVING\n",
      "Reset\n",
      " Steps: 58/200. Actions: [-1, 0, -1]. Reward: 0.055\n",
      " episode: 17/500, score: 0.24999999999999978, average: -8.22 \n",
      "Reset\n",
      " Steps: 138/200. Actions: [0, -2, 2]. Reward: -0.33\n",
      " episode: 18/500, score: -22.900000000000016, average: -8.99 \n",
      "Reset\n",
      " Steps: 47/200. Actions: [-2, 0, 2]. Reward: -0.55\n",
      " episode: 19/500, score: -7.699999999999998, average: -8.92 \n",
      "Reset\n",
      " Steps: 111/200. Actions: [-2, -1, 2]. Reward: -0.3\n",
      " episode: 20/500, score: -15.45000000000001, average: -9.24 \n",
      "Reset\n",
      " Steps: 60/200. Actions: [-2, 1, 1]. Reward: -0.55\n",
      " episode: 21/500, score: 6.7499999999999964, average: -8.51 \n",
      "Reset\n",
      " Steps: 70/200. Actions: [-2, -1, 1]. Reward: -0.35\n",
      " episode: 22/500, score: 4.499999999999998, average: -7.94 \n",
      "Reset\n",
      " Steps: 129/200. Actions: [-2, 2, -1]. Reward: -0.55\n",
      " episode: 23/500, score: -6.400000000000001, average: -7.88 \n",
      "Reset\n",
      " Steps: 45/200. Actions: [-1, 1, 2]. Reward: -0.55\n",
      " episode: 24/500, score: -3.4000000000000012, average: -7.70 \n",
      "Reset\n",
      " Steps: 39/200. Actions: [-2, -1, 2]. Reward: -0.95\n",
      " episode: 25/500, score: 6.7500000000000036, average: -7.14 \n",
      "Reset\n",
      " Steps: 68/200. Actions: [-2, -1, 1]. Reward: -0.95\n",
      " episode: 26/500, score: -16.200000000000006, average: -7.48 \n",
      "Reset\n",
      " Steps: 200/200. Actions: [-1, 2, 2]. Reward: -0.35\n",
      " episode: 27/500, score: -60.049999999999855, average: -9.36 \n",
      "Reset\n",
      " Steps: 52/200. Actions: [-2, -2, 2]. Reward: -0.95\n",
      " episode: 28/500, score: -13.199999999999996, average: -9.49 \n",
      "Reset\n",
      " Steps: 151/200. Actions: [-2, -1, 1]. Reward: -0.95\n",
      " episode: 29/500, score: -49.999999999999915, average: -10.84 \n",
      "Reset\n",
      " Steps: 91/200. Actions: [-2, -2, 2]. Reward: -0.9\n",
      " episode: 30/500, score: -14.95, average: -10.97 \n",
      "Reset\n",
      " Steps: 108/200. Actions: [-2, 0, 2]. Reward: -0.555\n",
      " episode: 31/500, score: -7.000000000000004, average: -10.85 \n",
      "Reset\n",
      " Steps: 104/200. Actions: [-2, -2, 1]. Reward: -0.95\n",
      " episode: 32/500, score: -27.70000000000001, average: -11.36 \n",
      "Reset\n",
      " Steps: 37/200. Actions: [-2, 2, 1]. Reward: -0.33\n",
      " episode: 33/500, score: 6.949999999999998, average: -10.82 \n",
      "Reset\n",
      " Steps: 114/200. Actions: [-1, -2, 2]. Reward: -0.95\n",
      " episode: 34/500, score: -17.25000000000001, average: -11.00 \n",
      "Reset\n",
      " Steps: 29/200. Actions: [-1, -1, 2]. Reward: -0.95\n",
      " episode: 35/500, score: 1.0500000000000003, average: -10.67 \n",
      "Reset\n",
      " Steps: 36/200. Actions: [-2, -1, 2]. Reward: -0.9\n",
      " episode: 36/500, score: -1.2999999999999996, average: -10.42 \n",
      "Reset\n",
      " Steps: 151/200. Actions: [-2, 1, 2]. Reward: -0.95\n",
      " episode: 37/500, score: -10.650000000000011, average: -10.42 \n",
      "Reset\n",
      " Steps: 34/200. Actions: [-2, 0, 2]. Reward: -0.935\n",
      " episode: 38/500, score: -5.999999999999998, average: -10.31 \n",
      "Reset\n",
      " Steps: 39/200. Actions: [-2, 2, 2]. Reward: -0.333\n",
      " episode: 39/500, score: 5.599999999999999, average: -9.91 \n",
      "Reset\n",
      " Steps: 99/200. Actions: [-2, -2, 2]. Reward: -0.9\n",
      " episode: 40/500, score: -12.199999999999992, average: -9.97 \n",
      "Reset\n",
      " Steps: 200/200. Actions: [-1, -2, 1]. Reward: 0.355\n",
      " episode: 41/500, score: -30.450000000000053, average: -10.45 \n",
      "Reset\n",
      " Steps: 67/200. Actions: [0, 1, 2]. Reward: 0.335535\n",
      " episode: 42/500, score: -1.4499999999999975, average: -10.25 \n",
      "Reset\n",
      " Steps: 24/200. Actions: [-2, -2, 0]. Reward: -0.55\n",
      " episode: 43/500, score: 1.75, average: -9.97 \n",
      "Reset\n",
      " Steps: 83/200. Actions: [-1, -2, 2]. Reward: -0.95\n",
      " episode: 44/500, score: -31.200000000000035, average: -10.44 \n",
      "Reset\n",
      " Steps: 159/200. Actions: [-1, -2, -1]. Reward: -0.3\n",
      " episode: 45/500, score: -26.650000000000013, average: -10.80 \n",
      "Reset\n",
      " Steps: 58/200. Actions: [-2, 1, 2]. Reward: -0.95\n",
      " episode: 46/500, score: -15.000000000000012, average: -10.89 \n",
      "Reset\n",
      " Steps: 95/200. Actions: [-2, 1, 2]. Reward: -0.33\n",
      " episode: 47/500, score: -12.45, average: -10.92 \n",
      "Reset\n",
      " Steps: 108/200. Actions: [-2, 1, 2]. Reward: -0.9\n",
      " episode: 48/500, score: -36.79999999999998, average: -11.45 \n",
      "Reset\n",
      " Steps: 48/200. Actions: [-2, 1, 1]. Reward: -0.35\n",
      " episode: 49/500, score: 2.399999999999999, average: -11.17 \n",
      "Reset\n",
      " Steps: 64/200. Actions: [-1, -2, 2]. Reward: -0.95\n",
      " episode: 50/500, score: 2.5500000000000007, average: -11.07 \n",
      "Reset\n",
      " Steps: 192/200. Actions: [-2, 0, 1]. Reward: -0.33\n",
      " episode: 51/500, score: -38.350000000000065, average: -11.66 \n",
      "Reset\n",
      " Steps: 200/200. Actions: [-1, -2, -2]. Reward: 0.15\n",
      " episode: 52/500, score: 15.600000000000033, average: -11.10 SAVING\n",
      "Reset\n",
      " Steps: 200/200. Actions: [1, 2, 1]. Reward: 0.1555\n",
      " episode: 53/500, score: 30.599999999999888, average: -10.63 SAVING\n",
      "Reset\n",
      " Steps: 83/200. Actions: [-2, -2, 2]. Reward: -0.9\n",
      " episode: 54/500, score: -22.45000000000002, average: -11.16 \n",
      "Reset\n",
      " Steps: 66/200. Actions: [-2, -1, 2]. Reward: -0.9\n",
      " episode: 55/500, score: -9.100000000000003, average: -11.34 \n",
      "Reset\n",
      " Steps: 176/200. Actions: [-2, -2, 1]. Reward: -0.9\n",
      " episode: 56/500, score: -25.79999999999997, average: -11.85 \n",
      "Reset\n",
      " Steps: 200/200. Actions: [-1, 0, 1]. Reward: 0.1555\n",
      " episode: 57/500, score: 29.799999999999898, average: -10.82 \n",
      "Reset\n",
      " Steps: 200/200. Actions: [1, 0, 2]. Reward: -0.95555\n",
      " episode: 58/500, score: -10.749999999999984, average: -10.87 \n",
      "Reset\n",
      " Steps: 51/200. Actions: [-2, 1, 2]. Reward: -0.55\n",
      " episode: 59/500, score: -3.2500000000000018, average: -10.93 \n",
      "Reset\n",
      " Steps: 69/200. Actions: [-2, -2, 2]. Reward: -0.9\n",
      " episode: 60/500, score: 1.7499999999999991, average: -10.65 \n",
      "Reset\n",
      " Steps: 76/200. Actions: [-2, 0, 2]. Reward: -0.35\n",
      " episode: 61/500, score: -20.25000000000001, average: -10.23 \n",
      "Reset\n",
      " Steps: 200/200. Actions: [-2, -1, 2]. Reward: -0.25\n",
      " episode: 62/500, score: -35.999999999999986, average: -11.02 \n",
      "Reset\n",
      " Steps: 80/200. Actions: [-2, -1, 1]. Reward: -0.55\n",
      " episode: 63/500, score: -4.5500000000000025, average: -10.41 \n",
      "Reset\n",
      " Steps: 59/200. Actions: [-2, -2, 2]. Reward: -0.9\n",
      " episode: 64/500, score: -9.499999999999998, average: -10.03 \n",
      "Reset\n",
      " Steps: 40/200. Actions: [-2, 0, 1]. Reward: 0.0595\n",
      " episode: 65/500, score: 15.650000000000006, average: -9.73 \n",
      "Reset\n",
      " Steps: 200/200. Actions: [-2, -1, 2]. Reward: -0.35\n",
      " episode: 66/500, score: -43.04999999999995, average: -10.75 \n",
      "Reset\n",
      " Steps: 64/200. Actions: [-1, 1, -1]. Reward: 0.055\n",
      " episode: 67/500, score: 4.149999999999997, average: -10.67 \n",
      "Reset\n",
      " Steps: 125/200. Actions: [-2, 0, 2]. Reward: -0.55\n",
      " episode: 68/500, score: -20.59999999999999, average: -10.63 \n",
      "Reset\n",
      " Steps: 200/200. Actions: [-2, -2, 1]. Reward: 0.35\n",
      " episode: 69/500, score: -49.0, average: -11.45 \n",
      "Reset\n",
      " Steps: 33/200. Actions: [-2, 2, 1]. Reward: -0.93\n",
      " episode: 70/500, score: -2.3999999999999995, average: -11.19 \n",
      "Reset\n",
      " Steps: 42/200. Actions: [-2, -1, 2]. Reward: -0.9\n",
      " episode: 71/500, score: 3.249999999999997, average: -11.26 \n",
      "Reset\n",
      " Steps: 200/200. Actions: [-1, -2, 2]. Reward: 0.355\n",
      " episode: 72/500, score: -33.80000000000003, average: -12.03 \n",
      "Reset\n",
      " Steps: 48/200. Actions: [-2, -1, 2]. Reward: -0.3\n",
      " episode: 73/500, score: 7.799999999999996, average: -11.74 \n",
      "Reset\n",
      " Steps: 65/200. Actions: [-1, -1, 2]. Reward: -0.95\n",
      " episode: 74/500, score: 0.14999999999999303, average: -11.67 \n",
      "Reset\n",
      " Steps: 200/200. Actions: [-1, -1, 2]. Reward: -0.35\n",
      " episode: 75/500, score: -54.74999999999982, average: -12.90 \n",
      "Reset\n",
      " Steps: 124/200. Actions: [0, 1, 2]. Reward: -0.335\n",
      " episode: 76/500, score: -12.199999999999996, average: -12.82 \n",
      "Reset\n",
      " Steps: 133/200. Actions: [-1, 2, 2]. Reward: -0.95\n",
      " episode: 77/500, score: -20.799999999999997, average: -12.04 \n",
      "Reset\n",
      " Steps: 129/200. Actions: [-2, -2, 2]. Reward: -0.9\n",
      " episode: 78/500, score: -51.74999999999999, average: -12.81 \n",
      "Reset\n",
      " Steps: 47/200. Actions: [-2, -1, 2]. Reward: -0.9\n",
      " episode: 79/500, score: -13.050000000000004, average: -12.07 \n",
      "Reset\n",
      " Steps: 92/200. Actions: [-2, 0, 2]. Reward: -0.555\n",
      " episode: 80/500, score: -42.949999999999925, average: -12.63 \n",
      "Reset\n",
      " Steps: 19/200. Actions: [0, 1, 2]. Reward: -0.35\n",
      " episode: 81/500, score: 6.900000000000001, average: -12.35 \n",
      "Reset\n",
      " Steps: 149/200. Actions: [-2, -1, 0]. Reward: 0.395\n",
      " episode: 82/500, score: -22.900000000000016, average: -12.25 \n",
      "Reset\n",
      " Steps: 170/200. Actions: [-1, -1, 2]. Reward: -0.95\n",
      " episode: 83/500, score: -21.400000000000002, average: -12.82 \n",
      "Reset\n",
      " Steps: 124/200. Actions: [-1, 0, 2]. Reward: -0.55\n",
      " episode: 84/500, score: -16.4, average: -12.80 \n",
      "Reset\n",
      " Steps: 131/200. Actions: [-2, 1, 2]. Reward: -0.9\n",
      " episode: 85/500, score: -41.79999999999994, average: -13.66 \n",
      "Reset\n",
      " Steps: 75/200. Actions: [-1, 1, 2]. Reward: -0.335\n",
      " episode: 86/500, score: 4.100000000000004, average: -13.55 \n",
      "Reset\n",
      " Steps: 54/200. Actions: [-2, -1, 2]. Reward: -0.33\n",
      " episode: 87/500, score: 15.150000000000002, average: -13.04 \n",
      "Reset\n",
      " Steps: 121/200. Actions: [-1, 2, 2]. Reward: -0.95\n",
      " episode: 88/500, score: -47.49999999999996, average: -13.87 \n",
      "Reset\n",
      " Steps: 80/200. Actions: [-2, -1, 1]. Reward: -0.3\n",
      " episode: 89/500, score: 0.4500000000000031, average: -13.97 \n",
      "Reset\n",
      " Steps: 111/200. Actions: [-2, 0, 1]. Reward: -0.35\n",
      " episode: 90/500, score: -9.349999999999987, average: -13.91 \n",
      "Reset\n",
      " Steps: 120/200. Actions: [-2, -2, 1]. Reward: -0.9\n",
      " episode: 91/500, score: -17.000000000000018, average: -13.64 \n",
      "Reset\n",
      " Steps: 37/200. Actions: [-2, 0, 1]. Reward: -0.55\n",
      " episode: 92/500, score: -2.5, average: -13.66 \n",
      "Reset\n",
      " Steps: 39/200. Actions: [-2, 1, 2]. Reward: -0.335\n",
      " episode: 93/500, score: -4.25, average: -13.78 \n",
      "Reset\n",
      " Steps: 50/200. Actions: [-2, 1, 2]. Reward: -0.95\n",
      " episode: 94/500, score: -3.0500000000000003, average: -13.22 \n",
      "Reset\n",
      " Steps: 26/200. Actions: [-1, 0, 0]. Reward: -0.9\n",
      " episode: 95/500, score: -6.449999999999999, average: -12.82 \n",
      "Reset\n",
      " Steps: 87/200. Actions: [-1, 1, 1]. Reward: -0.955\n",
      " episode: 96/500, score: -21.350000000000016, average: -12.95 \n",
      "Reset\n",
      " Steps: 29/200. Actions: [-1, 1, 1]. Reward: 0.35\n",
      " episode: 97/500, score: 19.25, average: -12.31 \n",
      "Reset\n",
      " Steps: 72/200. Actions: [-1, 1, 2]. Reward: -0.333\n",
      " episode: 98/500, score: -25.85000000000002, average: -12.09 \n",
      "Reset\n",
      " Steps: 31/200. Actions: [-2, 1, 2]. Reward: -0.33\n",
      " episode: 99/500, score: -6.649999999999998, average: -12.27 \n",
      "Reset\n",
      " Steps: 200/200. Actions: [-2, 2, 2]. Reward: -0.355\n",
      " episode: 100/500, score: -63.69999999999981, average: -13.60 \n",
      "Reset\n",
      " Steps: 75/200. Actions: [-2, 2, 1]. Reward: -0.95\n",
      " episode: 101/500, score: -1.5999999999999965, average: -12.86 \n",
      "Reset\n",
      " Steps: 61/200. Actions: [-2, 2, 2]. Reward: -0.9\n",
      " episode: 102/500, score: -9.899999999999999, average: -13.37 \n",
      "Reset\n",
      " Steps: 43/200. Actions: [-1, -1, 0]. Reward: -0.3\n",
      " episode: 103/500, score: 11.700000000000006, average: -13.75 \n",
      "Reset\n",
      " Steps: 118/200. Actions: [-1, 0, 1]. Reward: -0.555\n",
      " episode: 104/500, score: -17.90000000000001, average: -13.66 \n",
      "Reset\n",
      " Steps: 39/200. Actions: [-1, 0, 2]. Reward: -0.55\n",
      " episode: 105/500, score: -1.5500000000000005, average: -13.51 \n",
      "Reset\n",
      " Steps: 51/200. Actions: [-1, 0, -1]. Reward: 0.05\n",
      " episode: 106/500, score: -2.7500000000000018, average: -13.05 \n",
      "Reset\n",
      " Steps: 46/200. Actions: [-2, -2, 2]. Reward: -0.95\n",
      " episode: 107/500, score: -3.7499999999999982, average: -13.72 \n",
      "Reset\n",
      " Steps: 198/200. Actions: [-1, 2, 2]. Reward: -0.955\n",
      " episode: 108/500, score: -20.500000000000025, average: -13.91 \n",
      "Reset\n",
      " Steps: 75/200. Actions: [-2, -1, 2]. Reward: -0.3\n",
      " episode: 109/500, score: -9.649999999999997, average: -14.04 \n",
      "Reset\n",
      " Steps: 31/200. Actions: [-2, -1, 2]. Reward: -0.55\n",
      " episode: 110/500, score: 1.949999999999999, average: -14.04 \n",
      "Reset\n",
      " Steps: 42/200. Actions: [-1, 1, -1]. Reward: -0.55\n",
      " episode: 111/500, score: -9.850000000000003, average: -13.83 \n",
      "Reset\n",
      " Steps: 171/200. Actions: [-2, 2, 2]. Reward: -0.95\n",
      " episode: 112/500, score: -53.449999999999825, average: -14.18 \n",
      "Reset\n",
      " Steps: 107/200. Actions: [-2, 0, 2]. Reward: -0.55\n",
      " episode: 113/500, score: 0.29999999999999716, average: -14.08 \n",
      "Reset\n",
      " Steps: 51/200. Actions: [-2, 1, 2]. Reward: -0.9\n",
      " episode: 114/500, score: -0.650000000000001, average: -13.90 \n",
      "Reset\n",
      " Steps: 82/200. Actions: [-1, 1, 2]. Reward: -0.95\n",
      " episode: 115/500, score: -14.65000000000001, average: -14.51 \n",
      "Reset\n",
      " Steps: 113/200. Actions: [-2, 1, 2]. Reward: -0.55\n",
      " episode: 116/500, score: 1.8000000000000067, average: -13.61 \n",
      "Reset\n",
      " Steps: 41/200. Actions: [-2, 0, 1]. Reward: -0.55\n",
      " episode: 117/500, score: -5.799999999999999, average: -13.81 \n",
      "Reset\n",
      " Steps: 52/200. Actions: [-2, 0, 2]. Reward: -0.93\n",
      " episode: 118/500, score: 12.400000000000004, average: -13.15 \n",
      "Reset\n",
      " Steps: 25/200. Actions: [-2, 0, 2]. Reward: -0.55\n",
      " episode: 119/500, score: -5.299999999999998, average: -12.28 \n",
      "Reset\n",
      " Steps: 152/200. Actions: [-2, 2, 1]. Reward: -0.55\n",
      " episode: 120/500, score: -33.25000000000003, average: -12.90 \n",
      "Reset\n",
      " Steps: 59/200. Actions: [-2, 0, 0]. Reward: -0.55\n",
      " episode: 121/500, score: 0.05000000000000138, average: -12.96 \n",
      "Reset\n",
      " Steps: 89/200. Actions: [-1, 1, 1]. Reward: -0.95\n",
      " episode: 122/500, score: -10.850000000000003, average: -12.50 \n",
      "Reset\n",
      " Steps: 91/200. Actions: [-2, -1, 2]. Reward: -0.3\n",
      " episode: 123/500, score: -29.950000000000028, average: -13.26 \n",
      "Reset\n",
      " Steps: 31/200. Actions: [-2, 0, 2]. Reward: -0.33\n",
      " episode: 124/500, score: -7.399999999999997, average: -13.41 \n",
      "Reset\n",
      " Steps: 176/200. Actions: [-2, -2, 2]. Reward: -0.3\n",
      " episode: 125/500, score: -38.99999999999992, average: -13.09 \n",
      "Reset\n",
      " Steps: 112/200. Actions: [-2, -2, 2]. Reward: -0.9\n",
      " episode: 126/500, score: -40.14999999999998, average: -13.65 \n",
      "Reset\n",
      " Steps: 40/200. Actions: [-2, 0, 0]. Reward: -0.555\n",
      " episode: 127/500, score: 0.8000000000000015, average: -13.22 \n",
      "Reset\n",
      " Steps: 86/200. Actions: [-1, -1, 2]. Reward: -0.95\n",
      " episode: 128/500, score: -11.500000000000004, average: -12.41 \n",
      "Reset\n",
      " Steps: 28/200. Actions: [-2, 0, 2]. Reward: -0.55\n",
      " episode: 129/500, score: -0.9499999999999993, average: -12.17 \n",
      "Reset\n",
      " Steps: 74/200. Actions: [-1, -1, 2]. Reward: -0.95\n",
      " episode: 130/500, score: -28.05000000000002, average: -11.87 \n",
      "Reset\n",
      " Steps: 69/200. Actions: [-2, 0, 2]. Reward: -0.935\n",
      " episode: 131/500, score: -18.05000000000002, average: -12.37 \n",
      "Reset\n",
      " Steps: 52/200. Actions: [-1, 0, 2]. Reward: -0.55\n",
      " episode: 132/500, score: -18.100000000000005, average: -12.28 \n",
      "Reset\n",
      " Steps: 54/200. Actions: [-2, -1, 1]. Reward: -0.3\n",
      " episode: 133/500, score: 9.75, average: -11.65 \n",
      "Reset\n",
      " Steps: 62/200. Actions: [-2, 1, 2]. Reward: -0.93\n",
      " episode: 134/500, score: -5.199999999999999, average: -11.43 \n",
      "Reset\n",
      " Steps: 72/200. Actions: [-2, -1, 2]. Reward: -0.3\n",
      " episode: 135/500, score: 2.349999999999999, average: -10.55 \n",
      "Reset\n",
      " Steps: 74/200. Actions: [-1, -1, 2]. Reward: -0.9\n",
      " episode: 136/500, score: -1.6500000000000017, average: -10.66 \n",
      "Reset\n",
      " Steps: 102/200. Actions: [-2, 0, 2]. Reward: -0.55\n",
      " episode: 137/500, score: -21.550000000000036, average: -11.40 \n",
      "Reset\n",
      " Steps: 66/200. Actions: [-1, -1, 0]. Reward: -0.55\n",
      " episode: 138/500, score: -14.650000000000004, average: -10.74 \n",
      "Reset\n",
      " Steps: 104/200. Actions: [-2, -1, 1]. Reward: -0.9\n",
      " episode: 139/500, score: -17.85, average: -11.10 \n",
      "Reset\n",
      " Steps: 75/200. Actions: [-2, 1, 2]. Reward: -0.955\n",
      " episode: 140/500, score: -1.7499999999999936, average: -10.95 \n",
      "Reset\n",
      " Steps: 94/200. Actions: [-2, 0, 2]. Reward: -0.55\n",
      " episode: 141/500, score: -22.75000000000002, average: -11.07 \n",
      "Reset\n",
      " Steps: 38/200. Actions: [-1, -2, 2]. Reward: -0.9\n",
      " episode: 142/500, score: 2.1500000000000017, average: -10.97 \n",
      "Reset\n",
      " Steps: 35/200. Actions: [-2, 1, 1]. Reward: -0.55\n",
      " episode: 143/500, score: 2.95, average: -10.83 \n",
      "Reset\n",
      " Steps: 188/200. Actions: [-2, -1, 2]. Reward: -0.3\n",
      " episode: 144/500, score: -41.400000000000055, average: -11.60 \n",
      "Reset\n",
      " Steps: 191/200. Actions: [-2, -2, 0]. Reward: -0.95\n",
      " episode: 145/500, score: -5.450000000000012, average: -11.58 \n",
      "Reset\n",
      " Steps: 59/200. Actions: [-2, -1, 2]. Reward: -0.935\n",
      " episode: 146/500, score: -7.449999999999999, average: -11.30 \n",
      "Reset\n",
      " Steps: 124/200. Actions: [-2, -1, 1]. Reward: -0.9\n",
      " episode: 147/500, score: -36.29999999999999, average: -12.41 \n",
      "Reset\n",
      " Steps: 85/200. Actions: [-2, -1, 2]. Reward: -0.3\n",
      " episode: 148/500, score: -9.049999999999976, average: -12.07 \n",
      "Reset\n",
      " Steps: 62/200. Actions: [-2, 0, 2]. Reward: -0.33\n",
      " episode: 149/500, score: -6.2499999999999964, average: -12.07 \n",
      "Reset\n",
      " Steps: 63/200. Actions: [-1, 1, 2]. Reward: -0.93\n",
      " episode: 150/500, score: -5.35, average: -10.90 \n",
      "Reset\n",
      " Steps: 83/200. Actions: [-1, -1, 1]. Reward: -0.55\n",
      " episode: 151/500, score: -5.199999999999999, average: -10.97 \n",
      "Reset\n",
      " Steps: 176/200. Actions: [-2, -2, 2]. Reward: -0.95\n",
      " episode: 152/500, score: -35.04999999999998, average: -11.47 \n",
      "Reset\n",
      " Steps: 98/200. Actions: [-1, -1, 2]. Reward: -0.3\n",
      " episode: 153/500, score: -12.79999999999999, average: -11.96 \n",
      "Reset\n",
      " Steps: 45/200. Actions: [-2, 0, -1]. Reward: 0.053\n",
      " episode: 154/500, score: -4.55, average: -11.70 \n",
      "Reset\n",
      " Steps: 41/200. Actions: [-2, -2, 2]. Reward: -0.9\n",
      " episode: 155/500, score: 5.850000000000007, average: -11.55 \n",
      "Reset\n",
      " Steps: 98/200. Actions: [-2, 0, 0]. Reward: -0.55\n",
      " episode: 156/500, score: -1.2499999999999973, average: -11.52 \n",
      "Reset\n",
      " Steps: 46/200. Actions: [-2, 1, 2]. Reward: -0.9\n",
      " episode: 157/500, score: -5.349999999999998, average: -11.55 \n",
      "Reset\n",
      " Steps: 127/200. Actions: [-2, -1, 2]. Reward: -0.95\n",
      " episode: 158/500, score: -12.800000000000011, average: -11.40 \n",
      "Reset\n",
      " Steps: 160/200. Actions: [-1, -2, 1]. Reward: -0.9\n",
      " episode: 159/500, score: -34.15, average: -11.89 \n",
      "Reset\n",
      " Steps: 51/200. Actions: [-1, -2, 2]. Reward: -0.95\n",
      " episode: 160/500, score: -5.65, average: -12.04 \n",
      "Reset\n",
      " Steps: 48/200. Actions: [0, -2, 2]. Reward: -0.955\n",
      " episode: 161/500, score: -14.25, average: -12.13 \n",
      "Reset\n",
      " Steps: 81/200. Actions: [-2, 0, 2]. Reward: -0.99\n",
      " episode: 162/500, score: -12.699999999999998, average: -11.31 \n",
      "Reset\n",
      " Steps: 98/200. Actions: [-1, -2, 1]. Reward: -0.55\n",
      " episode: 163/500, score: -30.750000000000025, average: -11.93 \n",
      "Reset\n",
      " Steps: 82/200. Actions: [-2, 1, 1]. Reward: -0.95\n",
      " episode: 164/500, score: -5.100000000000003, average: -12.02 \n",
      "Reset\n",
      " Steps: 70/200. Actions: [-2, -1, 2]. Reward: -0.55\n",
      " episode: 165/500, score: -9.099999999999989, average: -11.91 \n",
      "Reset\n",
      " Steps: 92/200. Actions: [-2, 1, 2]. Reward: -0.95\n",
      " episode: 166/500, score: -21.85000000000001, average: -12.39 \n",
      "Reset\n",
      " Steps: 71/200. Actions: [-2, 2, 2]. Reward: -0.335\n",
      " episode: 167/500, score: -13.40000000000001, average: -12.54 \n",
      "Reset\n",
      " Steps: 35/200. Actions: [-2, 0, 2]. Reward: 0.0555\n",
      " episode: 168/500, score: 8.750000000000002, average: -12.61 \n",
      "Reset\n",
      " Steps: 200/200. Actions: [1, -2, 2]. Reward: -0.555\n",
      " episode: 169/500, score: -33.099999999999994, average: -13.17 \n",
      "Reset\n",
      " Steps: 149/200. Actions: [-2, -2, 2]. Reward: -0.95\n",
      " episode: 170/500, score: -33.50000000000001, average: -13.17 \n",
      "Reset\n",
      " Steps: 94/200. Actions: [-2, 2, 1]. Reward: -0.335\n",
      " episode: 171/500, score: -23.300000000000026, average: -13.64 \n",
      "Reset\n",
      " Steps: 40/200. Actions: [-2, -1, 2]. Reward: -0.55\n",
      " episode: 172/500, score: 1.5499999999999996, average: -13.39 \n",
      "Reset\n",
      " Steps: 108/200. Actions: [-2, -1, 2]. Reward: -0.95\n",
      " episode: 173/500, score: -19.900000000000016, average: -13.19 \n",
      "Reset\n",
      " Steps: 123/200. Actions: [-2, -2, 2]. Reward: -0.95\n",
      " episode: 174/500, score: -25.950000000000017, average: -13.56 \n",
      "Reset\n",
      " Steps: 49/200. Actions: [-1, -2, 2]. Reward: -0.95\n",
      " episode: 175/500, score: -11.25, average: -13.01 \n",
      "Reset\n",
      " Steps: 133/200. Actions: [-2, 0, 2]. Reward: -0.35\n",
      " episode: 176/500, score: -18.699999999999992, average: -12.58 \n",
      "Reset\n",
      " Steps: 68/200. Actions: [-2, 2, 2]. Reward: -0.95\n",
      " episode: 177/500, score: -5.6999999999999975, average: -12.71 \n",
      "Reset\n",
      " Steps: 59/200. Actions: [-1, 1, 0]. Reward: -0.93\n",
      " episode: 178/500, score: 10.750000000000005, average: -12.26 \n",
      "Reset\n",
      " Steps: 43/200. Actions: [-2, -1, 2]. Reward: -0.95\n",
      " episode: 179/500, score: 6.650000000000004, average: -12.11 \n",
      "Reset\n",
      " Steps: 200/200. Actions: [1, 0, 2]. Reward: -0.335\n",
      " episode: 180/500, score: -46.84999999999998, average: -12.49 \n",
      "Reset\n",
      " Steps: 44/200. Actions: [-1, 1, 2]. Reward: -0.9\n",
      " episode: 181/500, score: 13.800000000000015, average: -11.85 \n",
      "Reset\n",
      " Steps: 200/200. Actions: [-2, -1, 2]. Reward: 0.35\n",
      " episode: 182/500, score: -27.25, average: -12.03 \n",
      "Reset\n",
      " Steps: 109/200. Actions: [-2, 0, 2]. Reward: -0.55\n",
      " episode: 183/500, score: -33.600000000000044, average: -12.90 \n",
      "Reset\n",
      " Steps: 162/200. Actions: [-2, -1, 2]. Reward: -0.95\n",
      " episode: 184/500, score: -41.599999999999916, average: -13.63 \n",
      "Reset\n",
      " Steps: 90/200. Actions: [-1, -2, 2]. Reward: -0.9\n",
      " episode: 185/500, score: -12.750000000000009, average: -13.93 \n",
      "Reset\n",
      " Steps: 57/200. Actions: [-2, 2, 2]. Reward: -0.95\n",
      " episode: 186/500, score: -18.35000000000002, average: -14.26 \n",
      "Reset\n",
      " Steps: 54/200. Actions: [-2, -1, 1]. Reward: -0.95\n",
      " episode: 187/500, score: -6.649999999999999, average: -13.96 \n",
      "Reset\n",
      " Steps: 88/200. Actions: [-1, -1, 0]. Reward: -0.3\n",
      " episode: 188/500, score: 13.55, average: -13.40 \n",
      "Reset\n",
      " Steps: 63/200. Actions: [-2, -2, 2]. Reward: -0.95\n",
      " episode: 189/500, score: -5.15, average: -13.15 \n",
      "Reset\n",
      " Steps: 102/200. Actions: [-2, -2, -1]. Reward: -0.3\n",
      " episode: 190/500, score: -17.2, average: -13.46 \n",
      "Reset\n",
      " Steps: 92/200. Actions: [-1, -2, 2]. Reward: -0.93\n",
      " episode: 191/500, score: -14.250000000000004, average: -13.29 \n",
      "Reset\n",
      " Steps: 68/200. Actions: [-2, -2, 2]. Reward: -0.9\n",
      " episode: 192/500, score: -7.649999999999997, average: -13.48 \n",
      "Reset\n",
      " Steps: 173/200. Actions: [-2, 0, 1]. Reward: -0.355\n",
      " episode: 193/500, score: -38.45000000000003, average: -14.31 \n",
      "Reset\n",
      " Steps: 96/200. Actions: [-2, -2, 2]. Reward: -0.9\n",
      " episode: 194/500, score: 9.2, average: -13.30 \n",
      "Reset\n",
      " Steps: 200/200. Actions: [-2, -1, 2]. Reward: 0.35\n",
      " episode: 195/500, score: -65.34999999999994, average: -14.49 \n",
      "Reset\n",
      " Steps: 99/200. Actions: [-2, 1, 2]. Reward: -0.335\n",
      " episode: 196/500, score: -14.600000000000005, average: -14.64 \n",
      "Reset\n",
      " Steps: 44/200. Actions: [-1, 0, 2]. Reward: 0.05535\n",
      " episode: 197/500, score: 9.650000000000002, average: -13.72 \n",
      "Reset\n",
      " Steps: 200/200. Actions: [-2, -1, 2]. Reward: -0.95\n",
      " episode: 198/500, score: -79.49999999999986, average: -15.13 \n",
      "Reset\n",
      " Steps: 200/200. Actions: [-2, 1, 2]. Reward: 0.3535\n",
      " episode: 199/500, score: -40.649999999999956, average: -15.82 \n",
      "Reset\n",
      " Steps: 69/200. Actions: [-2, 1, 2]. Reward: -0.935\n",
      " episode: 200/500, score: -1.6500000000000001, average: -15.74 \n",
      "Reset\n",
      " Steps: 23/200. Actions: [-2, -2, 2]. Reward: -0.95\n",
      " episode: 201/500, score: 0.8500000000000001, average: -15.62 \n",
      "Reset\n",
      " Steps: 40/200. Actions: [-2, -2, 2]. Reward: -0.95\n",
      " episode: 202/500, score: -4.549999999999999, average: -15.01 \n",
      "Reset\n",
      " Steps: 27/200. Actions: [-2, -2, 2]. Reward: -0.3\n",
      " episode: 203/500, score: 2.3, average: -14.71 \n",
      "Reset\n",
      " Steps: 80/200. Actions: [-2, 0, 1]. Reward: -0.333\n",
      " episode: 204/500, score: -1.850000000000002, average: -14.65 \n",
      "Reset\n",
      " Steps: 158/200. Actions: [-2, -2, 1]. Reward: -0.3\n",
      " episode: 205/500, score: -19.800000000000026, average: -15.17 \n",
      "Reset\n",
      " Steps: 61/200. Actions: [-2, 2, 0]. Reward: -0.33\n",
      " episode: 206/500, score: -6.700000000000005, average: -15.28 \n",
      "Reset\n",
      " Steps: 75/200. Actions: [-2, -1, 2]. Reward: -0.55\n",
      " episode: 207/500, score: 13.649999999999999, average: -14.90 \n",
      "Reset\n",
      " Steps: 84/200. Actions: [-2, 0, 2]. Reward: -0.95\n",
      " episode: 208/500, score: -24.450000000000028, average: -15.13 \n",
      "Reset\n",
      " Steps: 139/200. Actions: [-2, -2, 2]. Reward: -0.3\n",
      " episode: 209/500, score: -40.650000000000055, average: -15.26 \n",
      "Reset\n",
      " Steps: 97/200. Actions: [-2, 2, 1]. Reward: -0.335\n",
      " episode: 210/500, score: -11.24999999999999, average: -15.37 \n",
      "Reset\n",
      " Steps: 109/200. Actions: [-2, 1, 2]. Reward: -0.9\n",
      " episode: 211/500, score: -12.200000000000005, average: -15.33 \n",
      "Reset\n",
      " Steps: 96/200. Actions: [-2, -2, 1]. Reward: -0.3\n",
      " episode: 212/500, score: -30.05000000000001, average: -15.68 \n",
      "Reset\n",
      " Steps: 37/200. Actions: [-1, 0, 1]. Reward: -0.93\n",
      " episode: 213/500, score: 1.3000000000000034, average: -15.04 \n",
      "Reset\n",
      " Steps: 171/200. Actions: [-2, 2, 1]. Reward: -0.95\n",
      " episode: 214/500, score: -26.749999999999996, average: -15.47 \n",
      "Reset\n",
      " Steps: 90/200. Actions: [-2, 2, 2]. Reward: -0.93\n",
      " episode: 215/500, score: -25.75000000000002, average: -15.80 \n",
      "Reset\n",
      " Steps: 18/200. Actions: [-2, 2, 2]. Reward: -0.9\n",
      " episode: 216/500, score: 11.100000000000003, average: -15.14 \n",
      "Reset\n",
      " Steps: 25/200. Actions: [-2, 2, 1]. Reward: -0.3\n",
      " episode: 217/500, score: 13.000000000000004, average: -14.62 \n",
      "Reset\n",
      " Steps: 125/200. Actions: [-1, -1, 1]. Reward: -0.55\n",
      " episode: 218/500, score: -16.850000000000012, average: -15.13 \n",
      "Reset\n",
      " Steps: 178/200. Actions: [-2, -1, 2]. Reward: -0.95\n",
      " episode: 219/500, score: -65.14999999999993, average: -15.77 \n",
      "Reset\n",
      " Steps: 51/200. Actions: [-2, -2, 2]. Reward: -0.9\n",
      " episode: 220/500, score: -8.949999999999998, average: -15.28 \n",
      "Reset\n",
      " Steps: 106/200. Actions: [-2, 2, 2]. Reward: -0.9\n",
      " episode: 221/500, score: -17.500000000000014, average: -15.16 \n",
      "Reset\n",
      " Steps: 71/200. Actions: [-2, 0, 2]. Reward: -0.35\n",
      " episode: 222/500, score: -9.349999999999998, average: -15.38 \n",
      "Reset\n",
      " Steps: 84/200. Actions: [-2, -1, 2]. Reward: -0.9\n",
      " episode: 223/500, score: -1.3500000000000019, average: -15.01 \n",
      "Reset\n",
      " Steps: 200/200. Actions: [-2, -2, 2]. Reward: -0.35\n",
      " episode: 224/500, score: -51.59999999999995, average: -15.52 \n",
      "Reset\n",
      " Steps: 121/200. Actions: [-2, -2, 2]. Reward: -0.95\n",
      " episode: 225/500, score: -38.249999999999986, average: -16.06 \n",
      "Reset\n",
      " Steps: 41/200. Actions: [-2, 0, 2]. Reward: -0.93\n",
      " episode: 226/500, score: -8.549999999999999, average: -15.86 \n",
      "Reset\n",
      " Steps: 60/200. Actions: [-2, -1, 2]. Reward: -0.95\n",
      " episode: 227/500, score: -18.850000000000016, average: -16.12 \n",
      "Reset\n",
      " Steps: 74/200. Actions: [-1, 2, 1]. Reward: -0.935\n",
      " episode: 228/500, score: 10.500000000000004, average: -16.13 \n",
      "Reset\n",
      " Steps: 83/200. Actions: [-2, 0, 2]. Reward: -0.55\n",
      " episode: 229/500, score: 0.7499999999999987, average: -16.25 \n",
      "Reset\n",
      " Steps: 60/200. Actions: [-2, 2, 0]. Reward: -0.95\n",
      " episode: 230/500, score: 9.05, average: -15.13 \n",
      "Reset\n",
      " Steps: 200/200. Actions: [-1, 1, 1]. Reward: -0.95\n",
      " episode: 231/500, score: -57.64999999999989, average: -16.56 \n",
      "Reset\n",
      " Steps: 29/200. Actions: [-2, 2, 0]. Reward: -0.9\n",
      " episode: 232/500, score: 0.4999999999999991, average: -16.00 \n",
      "Reset\n",
      " Steps: 167/200. Actions: [-1, -2, 2]. Reward: -0.95\n",
      " episode: 233/500, score: -47.79999999999989, average: -16.28 \n",
      "Reset\n",
      " Steps: 56/200. Actions: [-2, 2, 2]. Reward: -0.33\n",
      " episode: 234/500, score: 1.850000000000002, average: -15.42 \n",
      "Reset\n",
      " Steps: 100/200. Actions: [-1, 2, 2]. Reward: -0.9\n",
      " episode: 235/500, score: -47.99999999999989, average: -16.12 \n",
      "Reset\n",
      " Steps: 69/200. Actions: [-2, -2, 2]. Reward: -0.95\n",
      " episode: 236/500, score: -5.25, average: -15.86 \n",
      "Reset\n",
      " Steps: 200/200. Actions: [-2, 0, 2]. Reward: 0.053\n",
      " episode: 237/500, score: -54.6, average: -16.82 \n",
      "Reset\n",
      " Steps: 78/200. Actions: [-1, -2, 2]. Reward: -0.95\n",
      " episode: 238/500, score: -19.350000000000005, average: -17.48 \n",
      "Reset\n",
      " Steps: 177/200. Actions: [-1, 2, 2]. Reward: -0.95\n",
      " episode: 239/500, score: -33.850000000000016, average: -18.05 \n",
      "Reset\n",
      " Steps: 98/200. Actions: [-2, -2, 2]. Reward: -0.9\n",
      " episode: 240/500, score: 3.6999999999999855, average: -17.63 \n",
      "Reset\n",
      " Steps: 44/200. Actions: [-2, -2, 2]. Reward: -0.93\n",
      " episode: 241/500, score: -12.950000000000005, average: -17.61 \n",
      "Reset\n",
      " Steps: 155/200. Actions: [-2, -2, 2]. Reward: -0.95\n",
      " episode: 242/500, score: -31.550000000000004, average: -18.08 \n",
      "Reset\n",
      " Steps: 63/200. Actions: [-2, 1, 2]. Reward: -0.355\n",
      " episode: 243/500, score: 8.650000000000004, average: -17.14 \n",
      "Reset\n",
      " Steps: 70/200. Actions: [-2, -1, 0]. Reward: -0.35\n",
      " episode: 244/500, score: 1.3499999999999954, average: -17.30 \n",
      "Reset\n",
      " Steps: 82/200. Actions: [-2, 1, 2]. Reward: -0.555\n",
      " episode: 245/500, score: -17.599999999999998, average: -16.34 \n",
      "Reset\n",
      " Steps: 188/200. Actions: [-1, 0, 2]. Reward: -0.55\n",
      " episode: 246/500, score: -29.650000000000052, average: -16.64 \n",
      "Reset\n",
      " Steps: 61/200. Actions: [-2, 0, 2]. Reward: -0.33\n",
      " episode: 247/500, score: 0.6500000000000008, average: -16.82 \n",
      "Reset\n",
      " Steps: 78/200. Actions: [-2, 1, 2]. Reward: -0.95\n",
      " episode: 248/500, score: -1.800000000000002, average: -15.27 \n",
      "Reset\n",
      " Steps: 70/200. Actions: [-2, 2, 2]. Reward: -0.95\n",
      " episode: 249/500, score: -17.100000000000012, average: -14.80 \n",
      "Reset\n",
      " Steps: 135/200. Actions: [-1, -2, 2]. Reward: -0.9\n",
      " episode: 250/500, score: -9.19999999999997, average: -14.95 \n",
      "Reset\n",
      " Steps: 42/200. Actions: [-1, 2, 1]. Reward: -0.33\n",
      " episode: 251/500, score: 5.550000000000001, average: -14.86 \n",
      "Reset\n",
      " Steps: 57/200. Actions: [-1, -2, 2]. Reward: -0.3\n",
      " episode: 252/500, score: 1.1, average: -14.74 \n",
      "Reset\n",
      " Steps: 74/200. Actions: [-1, 2, 1]. Reward: -0.955\n",
      " episode: 253/500, score: -2.6500000000000004, average: -14.84 \n",
      "Reset\n",
      " Steps: 136/200. Actions: [-1, 0, 2]. Reward: -0.555\n",
      " episode: 254/500, score: -32.35000000000001, average: -15.45 \n",
      "Reset\n",
      " Steps: 69/200. Actions: [-2, 2, 2]. Reward: -0.95\n",
      " episode: 255/500, score: 4.650000000000002, average: -14.96 \n",
      "Reset\n",
      " Steps: 65/200. Actions: [-2, -1, 2]. Reward: -0.9\n",
      " episode: 256/500, score: -0.10000000000000098, average: -14.83 \n",
      "Reset\n",
      " Steps: 46/200. Actions: [-1, -1, 2]. Reward: -0.9\n",
      " episode: 257/500, score: 7.549999999999988, average: -14.95 \n",
      "Reset\n",
      " Steps: 139/200. Actions: [-1, -2, 2]. Reward: -0.9\n",
      " episode: 258/500, score: -16.249999999999975, average: -14.79 \n",
      "Reset\n",
      " Steps: 81/200. Actions: [-2, 1, 0]. Reward: -0.353\n",
      " episode: 259/500, score: -6.949999999999998, average: -14.12 \n",
      "Reset\n",
      " Steps: 76/200. Actions: [-2, 0, 2]. Reward: -0.55\n",
      " episode: 260/500, score: -21.249999999999993, average: -14.32 \n",
      "Reset\n",
      " Steps: 72/200. Actions: [-2, 2, 2]. Reward: -0.95\n",
      " episode: 261/500, score: -21.050000000000015, average: -14.49 \n",
      "Reset\n",
      " Steps: 157/200. Actions: [-2, -2, 1]. Reward: -0.95\n",
      " episode: 262/500, score: -25.649999999999988, average: -14.40 \n",
      "Reset\n",
      " Steps: 200/200. Actions: [-2, 2, 2]. Reward: -0.335\n",
      " episode: 263/500, score: -21.199999999999996, average: -14.86 \n",
      "Reset\n",
      " Steps: 49/200. Actions: [-2, 1, 2]. Reward: -0.95\n",
      " episode: 264/500, score: -2.3500000000000023, average: -14.37 \n",
      "Reset\n",
      " Steps: 41/200. Actions: [-2, 1, 2]. Reward: -0.33\n",
      " episode: 265/500, score: 0.40000000000000063, average: -13.84 \n",
      "Reset\n",
      " Steps: 37/200. Actions: [-1, -1, 1]. Reward: -0.33\n",
      " episode: 266/500, score: -5.199999999999999, average: -14.17 \n",
      "Reset\n",
      " Steps: 36/200. Actions: [-1, 1, 2]. Reward: 0.3353\n",
      " episode: 267/500, score: 3.8, average: -14.35 \n",
      "Reset\n",
      " Steps: 100/200. Actions: [-2, 2, 1]. Reward: -0.35\n",
      " episode: 268/500, score: -19.400000000000013, average: -14.41 \n",
      "Reset\n",
      " Steps: 69/200. Actions: [-2, 2, 1]. Reward: -0.335\n",
      " episode: 269/500, score: -4.549999999999995, average: -13.19 \n",
      "Reset\n",
      " Steps: 54/200. Actions: [-2, -1, 2]. Reward: -0.95\n",
      " episode: 270/500, score: -5.899999999999999, average: -13.13 \n",
      "Reset\n",
      " Steps: 56/200. Actions: [-2, 1, 2]. Reward: -0.95\n",
      " episode: 271/500, score: -13.899999999999999, average: -13.06 \n",
      "Reset\n",
      " Steps: 124/200. Actions: [-2, -1, 2]. Reward: -0.9\n",
      " episode: 272/500, score: -34.80000000000003, average: -13.57 \n",
      "Reset\n",
      " Steps: 46/200. Actions: [-2, 0, 0]. Reward: -0.553\n",
      " episode: 273/500, score: 4.949999999999997, average: -13.44 \n",
      "Reset\n",
      " Steps: 183/200. Actions: [-2, 0, 2]. Reward: -0.955\n",
      " episode: 274/500, score: -24.300000000000015, average: -12.90 \n",
      "Reset\n",
      " Steps: 129/200. Actions: [-2, -2, 2]. Reward: -0.3\n",
      " episode: 275/500, score: -30.650000000000023, average: -12.74 \n",
      "Reset\n",
      " Steps: 91/200. Actions: [-2, 2, 2]. Reward: -0.95\n",
      " episode: 276/500, score: -4.049999999999997, average: -12.65 \n",
      "Reset\n",
      " Steps: 29/200. Actions: [-2, 2, 2]. Reward: -0.33\n",
      " episode: 277/500, score: -1.4, average: -12.31 \n",
      "Reset\n",
      " Steps: 98/200. Actions: [-2, 0, 2]. Reward: -0.55\n",
      " episode: 278/500, score: 2.849999999999996, average: -12.46 \n",
      "Reset\n",
      " Steps: 126/200. Actions: [-2, 0, 2]. Reward: -0.55\n",
      " episode: 279/500, score: -5.349999999999985, average: -12.58 \n",
      "Reset\n",
      " Steps: 93/200. Actions: [-1, -2, 0]. Reward: -0.35\n",
      " episode: 280/500, score: -1.4999999999999911, average: -12.79 \n",
      "Reset\n",
      " Steps: 43/200. Actions: [-2, -1, 1]. Reward: -0.9\n",
      " episode: 281/500, score: 10.450000000000008, average: -11.43 \n",
      "Reset\n",
      " Steps: 47/200. Actions: [-1, -2, 2]. Reward: -0.9\n",
      " episode: 282/500, score: -0.4000000000000003, average: -11.45 \n",
      "Reset\n",
      " Steps: 99/200. Actions: [-2, 2, 2]. Reward: -0.355\n",
      " episode: 283/500, score: -27.999999999999993, average: -11.05 \n",
      "Reset\n",
      " Steps: 151/200. Actions: [-2, -2, 2]. Reward: -0.9\n",
      " episode: 284/500, score: -9.449999999999994, average: -11.28 \n",
      "Reset\n",
      " Steps: 57/200. Actions: [-1, -2, 2]. Reward: -0.3\n",
      " episode: 285/500, score: 6.6999999999999975, average: -10.18 \n",
      "Reset\n",
      " Steps: 31/200. Actions: [0, 2, 1]. Reward: -0.333\n",
      " episode: 286/500, score: 4.6999999999999975, average: -9.99 \n",
      "Reset\n",
      " Steps: 109/200. Actions: [-2, -1, 2]. Reward: -0.9\n",
      " episode: 287/500, score: -21.900000000000016, average: -9.33 \n",
      "Reset\n",
      " Steps: 44/200. Actions: [-2, 1, 2]. Reward: -0.93\n",
      " episode: 288/500, score: 1.6500000000000004, average: -8.91 \n",
      "Reset\n",
      " Steps: 54/200. Actions: [-1, -2, 1]. Reward: -0.9\n",
      " episode: 289/500, score: -7.549999999999999, average: -8.39 \n",
      "Reset\n",
      " Steps: 75/200. Actions: [-2, -1, 2]. Reward: -0.3\n",
      " episode: 290/500, score: -17.400000000000013, average: -8.81 \n",
      "Reset\n",
      " Steps: 74/200. Actions: [-2, -1, 0]. Reward: 0.05\n",
      " episode: 291/500, score: -7.099999999999998, average: -8.69 \n",
      "Reset\n",
      " Steps: 90/200. Actions: [-2, 2, 0]. Reward: -0.555\n",
      " episode: 292/500, score: 16.10000000000002, average: -7.74 \n",
      "Reset\n",
      " Steps: 140/200. Actions: [-2, 2, 1]. Reward: -0.35\n",
      " episode: 293/500, score: -21.10000000000002, average: -8.33 \n",
      "Reset\n",
      " Steps: 143/200. Actions: [-2, 1, 2]. Reward: -0.95\n",
      " episode: 294/500, score: -21.599999999999984, average: -8.79 \n",
      "Reset\n",
      " Steps: 129/200. Actions: [-2, -1, 2]. Reward: -0.95\n",
      " episode: 295/500, score: -4.400000000000006, average: -8.53 \n",
      "Reset\n",
      " Steps: 95/200. Actions: [-2, -2, -1]. Reward: 0.35\n",
      " episode: 296/500, score: -7.700000000000002, average: -8.09 \n",
      "Reset\n",
      " Steps: 128/200. Actions: [-2, -2, 2]. Reward: -0.9\n",
      " episode: 297/500, score: -4.950000000000001, average: -8.20 \n",
      "Reset\n",
      " Steps: 77/200. Actions: [-1, 0, 1]. Reward: -0.935\n",
      " episode: 298/500, score: -4.6, average: -8.26 \n",
      "Reset\n",
      " Steps: 35/200. Actions: [-1, 2, 2]. Reward: -0.9\n",
      " episode: 299/500, score: 2.2500000000000013, average: -7.87 \n",
      "Reset\n",
      " Steps: 126/200. Actions: [0, -2, 1]. Reward: -0.99\n",
      " episode: 300/500, score: -30.300000000000015, average: -8.29 \n",
      "Reset\n",
      " Steps: 71/200. Actions: [-2, -1, 2]. Reward: -0.3\n",
      " episode: 301/500, score: -23.450000000000024, average: -8.87 \n",
      "Reset\n",
      " Steps: 69/200. Actions: [-2, -1, 1]. Reward: 0.33\n",
      " episode: 302/500, score: 5.899999999999998, average: -8.78 \n",
      "Reset\n",
      " Steps: 101/200. Actions: [-2, -2, 2]. Reward: -0.9\n",
      " episode: 303/500, score: -26.000000000000025, average: -9.24 \n",
      "Reset\n",
      " Steps: 70/200. Actions: [-1, 1, 2]. Reward: -0.95\n",
      " episode: 304/500, score: -10.249999999999996, average: -8.80 \n",
      "Reset\n",
      " Steps: 98/200. Actions: [-2, 2, 2]. Reward: -0.95\n",
      " episode: 305/500, score: -16.250000000000007, average: -9.22 \n",
      "Reset\n",
      " Steps: 71/200. Actions: [-2, 2, 2]. Reward: -0.93\n",
      " episode: 306/500, score: -13.250000000000005, average: -9.48 \n",
      "Reset\n",
      " Steps: 184/200. Actions: [-2, -1, 0]. Reward: -0.33\n",
      " episode: 307/500, score: 11.450000000000003, average: -9.40 \n",
      "Reset\n",
      " Steps: 24/200. Actions: [0, -2, 1]. Reward: -0.99\n",
      " episode: 308/500, score: 3.049999999999999, average: -9.02 \n",
      "Reset\n",
      " Steps: 36/200. Actions: [-1, 1, 2]. Reward: -0.35\n",
      " episode: 309/500, score: 0.7999999999999989, average: -8.86 \n",
      "Reset\n",
      " Steps: 82/200. Actions: [-2, -2, 2]. Reward: -0.9\n",
      " episode: 310/500, score: -4.550000000000003, average: -8.53 \n",
      "Reset\n",
      " Steps: 190/200. Actions: [-2, 1, 2]. Reward: -0.955\n",
      " episode: 311/500, score: -22.500000000000014, average: -8.56 \n",
      "Reset\n",
      " Steps: 42/200. Actions: [-1, 2, 2]. Reward: -0.95\n",
      " episode: 312/500, score: -4.85, average: -8.14 \n",
      "Reset\n",
      " Steps: 200/200. Actions: [0, 0, 1]. Reward: -0.5555\n",
      " episode: 313/500, score: -73.09999999999995, average: -9.18 \n",
      "Reset\n",
      " Steps: 31/200. Actions: [-1, 1, 1]. Reward: -0.95\n",
      " episode: 314/500, score: 0.900000000000001, average: -9.11 \n",
      "Reset\n",
      " Steps: 89/200. Actions: [-2, 1, 2]. Reward: -0.95\n",
      " episode: 315/500, score: -24.400000000000023, average: -9.61 \n",
      "Reset\n",
      " Steps: 57/200. Actions: [-1, 2, 2]. Reward: -0.95\n",
      " episode: 316/500, score: -16.50000000000001, average: -9.84 \n",
      "Reset\n",
      " Steps: 121/200. Actions: [-2, -1, 1]. Reward: -0.9\n",
      " episode: 317/500, score: -21.8, average: -10.35 \n",
      "Reset\n",
      " Steps: 117/200. Actions: [-2, -2, 2]. Reward: -0.3\n",
      " episode: 318/500, score: -13.299999999999972, average: -10.23 \n",
      "Reset\n",
      " Steps: 64/200. Actions: [-2, 2, 2]. Reward: -0.95\n",
      " episode: 319/500, score: -6.849999999999998, average: -10.27 \n",
      "Reset\n",
      " Steps: 67/200. Actions: [-2, 2, 2]. Reward: -0.335\n",
      " episode: 320/500, score: -11.350000000000001, average: -10.38 \n",
      "Reset\n",
      " Steps: 41/200. Actions: [-2, -2, 2]. Reward: -0.93\n",
      " episode: 321/500, score: -5.25, average: -10.21 \n",
      "Reset\n",
      " Steps: 129/200. Actions: [-1, -2, 2]. Reward: -0.3\n",
      " episode: 322/500, score: -4.849999999999999, average: -9.61 \n",
      "Reset\n",
      " Steps: 120/200. Actions: [-1, -2, 1]. Reward: -0.95\n",
      " episode: 323/500, score: -11.650000000000007, average: -9.94 \n",
      "Reset\n",
      " Steps: 116/200. Actions: [-2, -1, 2]. Reward: -0.9\n",
      " episode: 324/500, score: -31.60000000000003, average: -10.09 \n",
      "Reset\n",
      " Steps: 51/200. Actions: [-2, 2, 0]. Reward: -0.95\n",
      " episode: 325/500, score: 1.000000000000001, average: -9.45 \n",
      "Reset\n",
      " Steps: 118/200. Actions: [-2, 2, 0]. Reward: 0.0555\n",
      " episode: 326/500, score: 12.05000000000001, average: -9.13 \n",
      "Reset\n",
      " Steps: 78/200. Actions: [-2, -2, 2]. Reward: -0.9\n",
      " episode: 327/500, score: 0.7999999999999989, average: -9.09 \n",
      "Reset\n",
      " Steps: 42/200. Actions: [-2, -2, 2]. Reward: -0.3\n",
      " episode: 328/500, score: -0.7499999999999998, average: -9.16 \n",
      "Reset\n",
      " Steps: 54/200. Actions: [-2, 2, 2]. Reward: -0.35\n",
      " episode: 329/500, score: 1.299999999999997, average: -9.03 \n",
      "Reset\n",
      " Steps: 167/200. Actions: [-1, -2, 2]. Reward: -0.95\n",
      " episode: 330/500, score: 5.350000000000007, average: -8.89 \n",
      "Reset\n",
      " Steps: 44/200. Actions: [-2, 2, 2]. Reward: -0.355\n",
      " episode: 331/500, score: -1.1000000000000034, average: -9.12 \n",
      "Reset\n",
      " Steps: 18/200. Actions: [-1, 2, 2]. Reward: -0.3\n",
      " episode: 332/500, score: 11.900000000000002, average: -8.88 \n",
      "Reset\n",
      " Steps: 61/200. Actions: [-2, -2, 2]. Reward: -0.9\n",
      " episode: 333/500, score: -0.19999999999999907, average: -8.32 \n",
      "Reset\n",
      " Steps: 112/200. Actions: [-2, 1, 2]. Reward: -0.35\n",
      " episode: 334/500, score: -5.8999999999999755, average: -8.25 \n",
      "Reset\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[9], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43magent\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[7], line 235\u001b[0m, in \u001b[0;36mPPOAgent.run\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    233\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m e \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mEPISODES):\n\u001b[1;32m    234\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel\u001b[38;5;241m.\u001b[39meval()\n\u001b[0;32m--> 235\u001b[0m     state \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mreset\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    236\u001b[0m     done, score, SAVING \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m, \u001b[38;5;241m0\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[1;32m    237\u001b[0m     \u001b[38;5;66;03m# Bucle para recopilar la trayectoria\u001b[39;00m\n",
      "Cell \u001b[0;32mIn[7], line 219\u001b[0m, in \u001b[0;36mPPOAgent.reset\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    218\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mreset\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m--> 219\u001b[0m     next_state \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43menv\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mreset\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    220\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstate_memory \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mzeros(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstate_size)\n\u001b[1;32m    221\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mprocess_state(next_state)\n",
      "File \u001b[0;32m/media/saul/Datos/Asignaturas/09MIAR  - TFM/auto-docking-iss/env2.py:136\u001b[0m, in \u001b[0;36mEnv.reset\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    134\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstopped \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[1;32m    135\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mReset\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m--> 136\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mwait_until_stopped\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    137\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mderivative_timer \u001b[38;5;241m=\u001b[39m time\u001b[38;5;241m.\u001b[39mtime() \u001b[38;5;241m-\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m    138\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstate_curr \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msim_data\u001b[38;5;241m.\u001b[39mcopy()\n",
      "File \u001b[0;32m/media/saul/Datos/Asignaturas/09MIAR  - TFM/auto-docking-iss/env2.py:123\u001b[0m, in \u001b[0;36mEnv.wait_until_stopped\u001b[0;34m(self, sleep_time)\u001b[0m\n\u001b[1;32m    121\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m: \n\u001b[1;32m    122\u001b[0m         count \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m\n\u001b[0;32m--> 123\u001b[0m     \u001b[43mtime\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msleep\u001b[49m\u001b[43m(\u001b[49m\u001b[43msleep_time\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    125\u001b[0m \u001b[38;5;66;03m# print(\"Stopped\")\u001b[39;00m\n\u001b[1;32m    126\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstopped \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "agent.run()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "miar_tfm",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "from math import sqrt\n",
    "import matplotlib.pyplot as plt\n",
    "import random\n",
    "import env_sim3\n",
    "import env6\n",
    "from liquid_net import LiquidNet\n",
    "import os\n",
    "import numpy as np\n",
    "import torch\n",
    "from torchsummary import summary\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from copy import deepcopy\n",
    "\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "\n",
    "from ncps.wirings import AutoNCP \n",
    "from ncps.torch import LTC\n",
    "import seaborn as sns\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ExperienceDataset(Dataset):\n",
    "    def __init__(self, data):\n",
    "        super().__init__()\n",
    "        self.data = data\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data[0])\n",
    "\n",
    "    def __getitem__(self, ind):\n",
    "        return tuple(map(lambda t: t[ind], self.data))\n",
    "\n",
    "def create_shuffled_dataloader(data, batch_size, shuffle = True):\n",
    "    ds = ExperienceDataset(data)\n",
    "    return DataLoader(ds, batch_size = batch_size, shuffle = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PolicyNetwork(torch.nn.Module):\n",
    "    def __init__(self,state_dims, n_actions):\n",
    "        super(PolicyNetwork, self).__init__()\n",
    "        self.fc1 = torch.nn.Linear(state_dims, 64)\n",
    "        self.fc2 = torch.nn.Linear(64, 32)\n",
    "        self.liquid_net = LiquidNet(32)\n",
    "        self.fc3 = torch.nn.Linear(32, n_actions)\n",
    "\n",
    "        torch.nn.init.kaiming_uniform_(self.fc1.weight)\n",
    "        torch.nn.init.kaiming_uniform_(self.fc2.weight)\n",
    "        torch.nn.init.kaiming_uniform_(self.fc3.weight)\n",
    "\n",
    "        self.hidden_state = None\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Initialize the hidden state if it is the first pass\n",
    "        if self.hidden_state is None:\n",
    "            self.hidden_state = torch.zeros(x.size(0), self.liquid_net.state_size).to(\n",
    "                x.device\n",
    "            )\n",
    "        # Forward pass through the liquidnet\n",
    "        x = torch.tanh(self.fc1(x))\n",
    "        x = torch.tanh(self.fc2(x))\n",
    "        x, self.hidden_state = self.liquid_net(x, self.hidden_state)\n",
    "        x = self.fc3(x)\n",
    "        policy = torch.nn.functional.softmax(x, dim=-1) \n",
    "\n",
    "        return policy\n",
    "    \n",
    "class ValueNetwork(torch.nn.Module):\n",
    "    def __init__(self,state_dims):\n",
    "        super(ValueNetwork, self).__init__()\n",
    "        self.fc1 = torch.nn.Linear(state_dims, 64)\n",
    "        self.fc2 = torch.nn.Linear(64, 32)\n",
    "        self.liquid_net = LiquidNet(32)\n",
    "        self.fc3 = torch.nn.Linear(32, 1)\n",
    "\n",
    "        torch.nn.init.kaiming_uniform_(self.fc1.weight)\n",
    "        torch.nn.init.kaiming_uniform_(self.fc2.weight)\n",
    "        torch.nn.init.kaiming_uniform_(self.fc3.weight)\n",
    "\n",
    "        self.hidden_state = None\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Initialize the hidden state if it is the first pass\n",
    "        if self.hidden_state is None:\n",
    "            self.hidden_state = torch.zeros(x.size(0), self.liquid_net.state_size).to(\n",
    "                x.device\n",
    "            )\n",
    "        # Forward pass through the liquidnet\n",
    "        x = torch.tanh(self.fc1(x))\n",
    "        x = torch.tanh(self.fc2(x))\n",
    "        x, self.hidden_state = self.liquid_net(x, self.hidden_state)\n",
    "        value = self.fc3(x)\n",
    "        return value\n",
    "\n",
    "        \n",
    "def weights_init(m):\n",
    "    if isinstance(m,torch.nn.Linear):\n",
    "        m.bias.data.fill_(0)\n",
    "        torch.nn.init.kaiming_uniform_(m.weight)\n",
    "\n",
    "def np_to_tensor(x):\n",
    "    return torch.tensor(x).to(torch.float32)\n",
    "\n",
    "    \n",
    "policy_model = PolicyNetwork(state_dims=6, n_actions = 6)\n",
    "value_model = ValueNetwork(state_dims=6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PolicyNetwork(\n",
      "  (fc1): Linear(in_features=6, out_features=64, bias=True)\n",
      "  (fc2): Linear(in_features=64, out_features=32, bias=True)\n",
      "  (liquid_net): LiquidNet()\n",
      "  (fc3): Linear(in_features=32, out_features=6, bias=True)\n",
      ") ValueNetwork(\n",
      "  (fc1): Linear(in_features=6, out_features=64, bias=True)\n",
      "  (fc2): Linear(in_features=64, out_features=32, bias=True)\n",
      "  (liquid_net): LiquidNet()\n",
      "  (fc3): Linear(in_features=32, out_features=1, bias=True)\n",
      ")\n",
      "----------------------------------------------------------------\n",
      "        Layer (type)               Output Shape         Param #\n",
      "================================================================\n",
      "            Linear-1                [-1, 1, 64]             448\n",
      "            Linear-2                [-1, 1, 32]           2,080\n",
      "         LiquidNet-3       [[-1, 32], [-1, 32]]               0\n",
      "            Linear-4                    [-1, 6]             198\n",
      "================================================================\n",
      "Total params: 2,726\n",
      "Trainable params: 2,726\n",
      "Non-trainable params: 0\n",
      "----------------------------------------------------------------\n",
      "Input size (MB): 0.00\n",
      "Forward/backward pass size (MB): 0.01\n",
      "Params size (MB): 0.01\n",
      "Estimated Total Size (MB): 0.02\n",
      "----------------------------------------------------------------\n",
      "----------------------------------------------------------------\n",
      "        Layer (type)               Output Shape         Param #\n",
      "================================================================\n",
      "            Linear-1                [-1, 1, 64]             448\n",
      "            Linear-2                [-1, 1, 32]           2,080\n",
      "         LiquidNet-3       [[-1, 32], [-1, 32]]               0\n",
      "            Linear-4                    [-1, 1]              33\n",
      "================================================================\n",
      "Total params: 2,561\n",
      "Trainable params: 2,561\n",
      "Non-trainable params: 0\n",
      "----------------------------------------------------------------\n",
      "Input size (MB): 0.00\n",
      "Forward/backward pass size (MB): 0.01\n",
      "Params size (MB): 0.01\n",
      "Estimated Total Size (MB): 0.02\n",
      "----------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "print(policy_model,value_model)\n",
    "summary(policy_model, input_size=(1, 6))\n",
    "summary(value_model, input_size=(1, 6))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PPO:\n",
    "    def __init__(self, state_dim=6, action_dim=6, policy_lr=0.0005, value_lr=0.001, \n",
    "                 gamma=0.99, lam=0.95, beta_s=0.01,epsilon_clip=0.2, value_clip=0.4, \n",
    "                 epochs=5,batch_size=50,save_path='Models',env=None):\n",
    "        \n",
    "        self.policy_net = PolicyNetwork(state_dim, action_dim).to(device)\n",
    "        self.value_net = ValueNetwork(state_dim).to(device)\n",
    "\n",
    "        self.policy_optimizer = torch.optim.Adam(self.policy_net.parameters(), lr=policy_lr)\n",
    "        self.value_optimizer = torch.optim.Adam(self.value_net.parameters(), lr=value_lr)\n",
    "\n",
    "        self.gamma = gamma\n",
    "        self.lamda = lam\n",
    "        self.epsilon_clip = epsilon_clip\n",
    "        self.value_clip = value_clip\n",
    "        self.beta_s = beta_s\n",
    "        self.epochs = epochs\n",
    "        self.action_dim = action_dim\n",
    "        self.batch_size = batch_size\n",
    "\n",
    "        self.env = env\n",
    "        self.env.run()\n",
    "\n",
    "        self.save_path = save_path\n",
    "        self.env_name = \"auto-docking-iss\"\n",
    "\n",
    "        if not os.path.exists(self.save_path): os.makedirs(self.save_path)\n",
    "        self.path = str(self.env_name)+'_PPO_'\n",
    "        self.model_name = os.path.join(self.save_path, self.path)\n",
    "\n",
    "        self.trajectory = {'states':[],'actions':[], 'rewards':[], 'action_logs':[],\n",
    "                           'values':[],'done':[]}\n",
    "        \n",
    "        self.scores, self.episodes, self.average = [],[],[]\n",
    "        self.value_loss_mean, self.policy_loss_mean = [], []\n",
    "\n",
    "    def save(self,episode,score):\n",
    "        torch.save({\n",
    "            'actor': self.policy_net.state_dict(),\n",
    "            'critic': self.value_net.state_dict()\n",
    "        }, f'./'+ self.model_name + '_' + str(episode) + '_' + str(round(score,4)) + '.pt')\n",
    "\n",
    "    def load(self,name):\n",
    "        print(f'./{name}')\n",
    "        try:\n",
    "            data = torch.load(f'./{name}')\n",
    "            self.policy_net.fc1.weight.data = data['actor']['fc1.weight']\n",
    "            self.policy_net.fc1.bias.data = data['actor']['fc1.bias']\n",
    "            self.policy_net.fc2.weight.data = data['actor']['fc2.weight']\n",
    "            self.policy_net.fc2.bias.data = data['actor']['fc2.bias']\n",
    "            self.policy_net.fc3.weight.data = data['actor']['fc3.weight']\n",
    "            self.policy_net.fc3.bias.data = data['actor']['fc3.bias']\n",
    "            self.policy_net.liquid_net.sensory_mu = torch.nn.Parameter(data['actor']['liquid_net.sensory_mu'])\n",
    "            self.policy_net.liquid_net.sensory_sigma = torch.nn.Parameter(data['actor']['liquid_net.sensory_sigma'])\n",
    "            self.policy_net.liquid_net.sensory_W = torch.nn.Parameter(data['actor']['liquid_net.sensory_W'])\n",
    "            self.policy_net.liquid_net.sensory_erev = torch.nn.Parameter(data['actor']['liquid_net.sensory_erev'])\n",
    "            self.policy_net.liquid_net.mu = torch.nn.Parameter(data['actor']['liquid_net.mu'])\n",
    "            self.policy_net.liquid_net.sigma = torch.nn.Parameter(data['actor']['liquid_net.sigma'])\n",
    "            self.policy_net.liquid_net.W = torch.nn.Parameter(data['actor']['liquid_net.W'])\n",
    "            self.policy_net.liquid_net.erev = torch.nn.Parameter(data['actor']['liquid_net.erev'])\n",
    "            self.policy_net.liquid_net.vleak = torch.nn.Parameter(data['actor']['liquid_net.vleak'])\n",
    "            self.policy_net.liquid_net.gleak = torch.nn.Parameter(data['actor']['liquid_net.gleak'])\n",
    "            self.policy_net.liquid_net.cm_t = torch.nn.Parameter(data['actor']['liquid_net.cm_t'])\n",
    "\n",
    "            self.value_net.fc1.weight.data = data['critic']['fc1.weight']\n",
    "            self.value_net.fc1.bias.data = data['critic']['fc1.bias']\n",
    "            self.value_net.fc2.weight.data = data['critic']['fc2.weight']\n",
    "            self.value_net.fc2.bias.data = data['critic']['fc2.bias']\n",
    "            self.value_net.fc3.weight.data = data['critic']['fc3.weight']\n",
    "            self.value_net.fc3.bias.data = data['critic']['fc3.bias']\n",
    "            self.value_net.liquid_net.sensory_mu = torch.nn.Parameter(data['critic']['liquid_net.sensory_mu'])\n",
    "            self.value_net.liquid_net.sensory_sigma = torch.nn.Parameter(data['critic']['liquid_net.sensory_sigma'])\n",
    "            self.value_net.liquid_net.sensory_W = torch.nn.Parameter(data['critic']['liquid_net.sensory_W'])\n",
    "            self.value_net.liquid_net.sensory_erev = torch.nn.Parameter(data['critic']['liquid_net.sensory_erev'])\n",
    "            self.value_net.liquid_net.mu = torch.nn.Parameter(data['critic']['liquid_net.mu'])\n",
    "            self.value_net.liquid_net.sigma = torch.nn.Parameter(data['critic']['liquid_net.sigma'])\n",
    "            self.value_net.liquid_net.W = torch.nn.Parameter(data['critic']['liquid_net.W'])\n",
    "            self.value_net.liquid_net.erev = torch.nn.Parameter(data['critic']['liquid_net.erev'])\n",
    "            self.value_net.liquid_net.vleak = torch.nn.Parameter(data['critic']['liquid_net.vleak'])\n",
    "            self.value_net.liquid_net.gleak = torch.nn.Parameter(data['critic']['liquid_net.gleak'])\n",
    "            self.value_net.liquid_net.cm_t = torch.nn.Parameter(data['critic']['liquid_net.cm_t'])\n",
    "\n",
    "            # self.policy_net.load_state_dict(data['actor'])\n",
    "            # self.value_net.load_state_dict(data['critic'])\n",
    "        except Exception as e:\n",
    "            print(e)\n",
    "\n",
    "    # Función para visualizar la evolución del entrenamiento\n",
    "    def plot_model(self, score, episode,num_episodes):\n",
    "        self.scores.append(score)\n",
    "        self.episodes.append(episode)\n",
    "        self.average.append(sum(self.scores[-10:]) / len(self.scores[-10:]))\n",
    "        if (episode % 10 == 0 and episode > 0) or episode == num_episodes:#str(episode)[-2:] == \"00\":# much faster than episode % 100\n",
    "            plt.figure(1,figsize=(18, 9))\n",
    "            plt.plot(self.episodes, self.scores, 'b')\n",
    "            plt.plot(self.episodes, self.average, 'r')\n",
    "            plt.ylabel('Score', fontsize=18)\n",
    "            plt.xlabel('Steps', fontsize=18)\n",
    "            try:\n",
    "                plt.savefig(self.model_name+\"scores.png\")\n",
    "            except OSError:\n",
    "                pass\n",
    "\n",
    "        return self.average[-1]\n",
    "    \n",
    "    def plot_loss(self,episode,num_episodes):        \n",
    "        if (episode % 10 == 0 and episode > 0) or episode == num_episodes:#str(episode)[-2:] == \"00\":# much faster than episode % 100\n",
    "            plt.figure(2,figsize=(18, 9))\n",
    "            plt.plot(self.episodes, self.policy_loss_mean, 'b')\n",
    "            plt.plot(self.episodes, self.value_loss_mean, 'r')\n",
    "            plt.ylabel('Loss', fontsize=18)\n",
    "            plt.xlabel('Steps', fontsize=18)\n",
    "            try:\n",
    "                plt.savefig(self.model_name+\"loss.png\")\n",
    "            except OSError:\n",
    "                pass\n",
    "    \n",
    "\n",
    "    #Almacenamiento de todas las variables que definen una transición para PPO\n",
    "    def collect_trayectory(self,state,action,action_logs,reward,done,value):\n",
    "        self.trajectory['states'].append(state)\n",
    "        self.trajectory['actions'].append(action)\n",
    "        self.trajectory['rewards'].append(reward)\n",
    "        self.trajectory['values'].append(value)\n",
    "        self.trajectory['action_logs'].append(action_logs)\n",
    "        self.trajectory['done'].append(done)\n",
    "    \n",
    "    def clear_trayectory(self):\n",
    "        for key in self.trajectory.keys():\n",
    "            self.trajectory[key].clear()\n",
    "    \n",
    "    def generalized_advantage_estimation(self,next_state):\n",
    "        advantages = np.zeros_like(self.trajectory['rewards'])\n",
    "        last_gae_lambda = 0\n",
    "        self.trajectory['values'].append(self.value_net(next_state.unsqueeze(0)).squeeze().detach())\n",
    "\n",
    "        for t in reversed(range(len(self.trajectory['rewards']))):\n",
    "            delta = self.trajectory['rewards'][t]+ self.gamma * self.trajectory['values'][t + 1].item() - self.trajectory['values'][t].item()\n",
    "            advantages[t] = last_gae_lambda = delta + self.gamma * self.lamda * last_gae_lambda\n",
    "        \n",
    "        return np_to_tensor(advantages)\n",
    "\n",
    "\n",
    "    def learn_from_episode_trajectory(self,next_state):\n",
    "        self.policy_net.train()\n",
    "        self.value_net.train()\n",
    "\n",
    "        states = deepcopy(self.trajectory['states'])\n",
    "        actions = deepcopy(self.trajectory['actions'])\n",
    "        # rewards = deepcopy(self.trajectory['rewards'])\n",
    "        values = deepcopy(self.trajectory['values'])\n",
    "        action_logs = deepcopy(self.trajectory['action_logs'])\n",
    "        # done = deepcopy(self.trajectory['done'])\n",
    "\n",
    "        advantages = self.generalized_advantage_estimation(next_state).to(device)\n",
    "\n",
    "        # prepare dataloader for policy phase training\n",
    "        dl = create_shuffled_dataloader([states, actions, action_logs, advantages, values], self.batch_size,False)\n",
    "\n",
    "        policy_loss_array = []\n",
    "        value_loss_array = []\n",
    "\n",
    "        for i in range(self.epochs):\n",
    "            # self.policy_net.hidden_state = None\n",
    "            # self.value_net.hidden_state = None\n",
    "\n",
    "            for states, actions, action_log, advantages, values in dl:\n",
    "                #Update policy_net\n",
    "                new_action_probs = self.policy_net(states).squeeze()\n",
    "                dist = torch.distributions.Categorical(new_action_probs)\n",
    "\n",
    "                new_action_logs = dist.log_prob(actions)\n",
    "                entropy = dist.entropy()\n",
    "\n",
    "                # compute PPO-Clip\n",
    "                ratios = torch.exp(new_action_logs - action_log)\n",
    "                surr1 = ratios * advantages\n",
    "                surr2 = ratios.clamp(1 - self.epsilon_clip, 1 + self.epsilon_clip) * advantages\n",
    "                policy_loss = -torch.mean(torch.min(surr1, surr2) - self.beta_s * entropy)\n",
    "                policy_loss_array.append(policy_loss.detach().numpy())\n",
    "\n",
    "                self.policy_optimizer.zero_grad() # Clean gradients\n",
    "                policy_loss.mean().backward() # Compute gradients\n",
    "                self.policy_optimizer.step() #Update weights\n",
    "\n",
    "                #Update value_net\n",
    "                new_values = self.value_net(states).squeeze()\n",
    "                value_clipped = values + (new_values - values).clamp(-self.value_clip, self.value_clip)\n",
    "                value_loss_1 = (value_clipped.flatten() - advantages) ** 2\n",
    "                value_loss_2 = (values.flatten() - advantages) ** 2\n",
    "                value_loss = torch.mean(torch.max(value_loss_1, value_loss_2))\n",
    "                value_loss_array.append(value_loss.detach().numpy())\n",
    "\n",
    "                self.value_optimizer.zero_grad() # Clean gradients\n",
    "                value_loss.backward() # Compute gradients \n",
    "                self.value_optimizer.step() #Update weights\n",
    "\n",
    "                self.policy_net.hidden_state = self.policy_net.hidden_state.detach()\n",
    "                self.value_net.hidden_state = self.value_net.hidden_state.detach()\n",
    "            \n",
    "        self.value_loss_mean.append(sum(value_loss_array)/self.epochs)\n",
    "        self.policy_loss_mean.append(sum(policy_loss_array)/self.epochs)\n",
    "\n",
    "        # print(f\"\\n Mean Policy loss: {self.value_loss_mean[-1]:.4f}. Mean Value loss: {self.policy_loss_mean[-1]:.4f}\")\n",
    "        \n",
    "        self.policy_net.eval()\n",
    "        self.value_net.eval()\n",
    "\n",
    "\n",
    "    # Función para resetear el entorno tras acabar trayectoria\n",
    "    def reset(self):\n",
    "        state = self.env.reset()\n",
    "        return np_to_tensor(self.process_state(state))\n",
    "    \n",
    "     # Función para interacción agente-entorno\n",
    "    def step(self, action):\n",
    "        next_state, reward, done = self.env.step(action)\n",
    "        next_state = self.process_state(next_state)\n",
    "        return np_to_tensor(next_state), reward, done\n",
    "\n",
    "    def process_state(self,state):\n",
    "        state_processed = np.array(state)\n",
    "        state_processed[0] = state_processed[0] / 30 # Position array\n",
    "        state_processed[1] = state_processed[1] / 3 # Position array\n",
    "        state_processed[2] = state_processed[2] / 3 # Position array\n",
    "        return state_processed\n",
    "    \n",
    "    def train(self,init_episode=1,num_episodes=200,max_steps=200,dt=0.01):\n",
    "        average = 0\n",
    "        self.policy_net.eval()\n",
    "        self.value_net.eval()\n",
    "\n",
    "        max_score_average = -5000\n",
    "\n",
    "        num_episodes += init_episode\n",
    "\n",
    "        for episode in range(init_episode,num_episodes):\n",
    "            self.clear_trayectory()\n",
    "\n",
    "            state = self.reset()\n",
    "            score = 0\n",
    "\n",
    "            SAVING = \"\"\n",
    "            self.policy_net.hidden_state = None\n",
    "            self.value_net.hidden_state = None\n",
    "\n",
    "            for step in range(1,max_steps+1):\n",
    "                action_probs = self.policy_net(state.unsqueeze(0)).squeeze().detach()\n",
    "                dist = torch.distributions.Categorical(action_probs)\n",
    "                action = dist.sample()\n",
    "                action_log = dist.log_prob(action)\n",
    "                # print(action,action_probs,action_log)\n",
    "\n",
    "                value = self.value_net(state.unsqueeze(0)).squeeze().detach()\n",
    "                \n",
    "                next_state,reward,done = self.step(action.item())    \n",
    "                self.collect_trayectory(state,action,action_log,reward,done,value)\n",
    "\n",
    "                state = next_state\n",
    "\n",
    "                score += reward\n",
    "\n",
    "                # print(state)\n",
    "                # print(f'\\rSteps: {step}/{max_steps}. Action: {action}. Reward/Value: {round(reward,2)}/{round(value.item(),4)}. Done: {done}        ', end='', flush=True)\n",
    "\n",
    "                if done:\n",
    "                    break\n",
    "                \n",
    "                # time.sleep(dt)\n",
    "\n",
    "            self.learn_from_episode_trajectory(next_state)\n",
    "            self.clear_trayectory()\n",
    "\n",
    "            average = self.plot_model(score, episode,num_episodes)\n",
    "            self.plot_loss(episode,num_episodes)\n",
    "            \n",
    "            if episode > 200 and average >= max_score_average:\n",
    "                max_score_average = average\n",
    "                self.save(episode,average)\n",
    "                SAVING = \"SAVING\"\n",
    "            # elif episode % 10 == 0:\n",
    "            #     self.save(episode,average)\n",
    "            #     SAVING = \"SAVING\"\n",
    "            else:\n",
    "                SAVING = \"\"\n",
    "            \n",
    "            print(f\"\\rEpisode: {episode}/{num_episodes-1}, score/average: {score:.4f}/{average:.4f} {SAVING}           \", end='', flush=True)\n",
    "\n",
    "\n",
    "        # close environemnt when finish training\n",
    "        self.save(episode,average)\n",
    "\n",
    "    def test(self,max_steps,dt=0.01):\n",
    "        self.policy_net.eval()\n",
    "        self.value_net.eval()\n",
    "\n",
    "        self.clear_trayectory()\n",
    "        state = self.reset()\n",
    "        score = 0\n",
    "\n",
    "        self.policy_net.hidden_state = None\n",
    "        self.value_net.hidden_state = None\n",
    "\n",
    "        for step in range(1,max_steps+1):\n",
    "            action_probabilities = self.policy_net(state.unsqueeze(0))\n",
    "            action = torch.argmax(action_probabilities, dim=-1).item()#np.argmax(self.policy_net(state).squeeze().detach().numpy())\n",
    "            value = self.value_net(state.unsqueeze(0)).squeeze().detach()\n",
    "            \n",
    "            next_state,reward,done = self.step(action)    \n",
    "\n",
    "            state = next_state\n",
    "\n",
    "            score += reward\n",
    "\n",
    "            print(f'\\rSteps: {step}/{max_steps}. Action: {action}. Score/Value: {round(score,2)}/{round(value.item(),4)}. Done: {done}', end='', flush=True)\n",
    "\n",
    "            if self.env.done or step > max_steps:\n",
    "                break\n",
    "                \n",
    "            time.sleep(dt)\n",
    "        \n",
    "        print(f'\\nNext state: {state}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "dt = 0.5\n",
    "env = env_sim3.Env(dt=dt)\n",
    "agent = PPO(save_path='Models15',env=env,batch_size=500)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode: 4/1000, score/average: -503.2919/-464.8653            "
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[17], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m max_steps \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m250\u001b[39m\n\u001b[0;32m----> 2\u001b[0m \u001b[43magent\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43minit_episode\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43mnum_episodes\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1000\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43mmax_steps\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mint\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mmax_steps\u001b[49m\u001b[38;5;241;43m/\u001b[39;49m\u001b[43mdt\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[5], line 235\u001b[0m, in \u001b[0;36mPPO.train\u001b[0;34m(self, init_episode, num_episodes, max_steps, dt)\u001b[0m\n\u001b[1;32m    231\u001b[0m         \u001b[38;5;28;01mbreak\u001b[39;00m\n\u001b[1;32m    233\u001b[0m     \u001b[38;5;66;03m# time.sleep(dt)\u001b[39;00m\n\u001b[0;32m--> 235\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlearn_from_episode_trajectory\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnext_state\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    236\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mclear_trayectory()\n\u001b[1;32m    238\u001b[0m average \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mplot_model(score, episode,num_episodes)\n",
      "Cell \u001b[0;32mIn[5], line 160\u001b[0m, in \u001b[0;36mPPO.learn_from_episode_trajectory\u001b[0;34m(self, next_state)\u001b[0m\n\u001b[1;32m    158\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mvalue_optimizer\u001b[38;5;241m.\u001b[39mzero_grad() \u001b[38;5;66;03m# Clean gradients\u001b[39;00m\n\u001b[1;32m    159\u001b[0m value_loss\u001b[38;5;241m.\u001b[39mbackward() \u001b[38;5;66;03m# Compute gradients \u001b[39;00m\n\u001b[0;32m--> 160\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mvalue_optimizer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstep\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;66;03m#Update weights\u001b[39;00m\n\u001b[1;32m    162\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpolicy_net\u001b[38;5;241m.\u001b[39mhidden_state \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpolicy_net\u001b[38;5;241m.\u001b[39mhidden_state\u001b[38;5;241m.\u001b[39mdetach()\n\u001b[1;32m    163\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mvalue_net\u001b[38;5;241m.\u001b[39mhidden_state \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mvalue_net\u001b[38;5;241m.\u001b[39mhidden_state\u001b[38;5;241m.\u001b[39mdetach()\n",
      "File \u001b[0;32m~/miniconda3/envs/miar_tfm/lib/python3.10/site-packages/torch/optim/optimizer.py:391\u001b[0m, in \u001b[0;36mOptimizer.profile_hook_step.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    386\u001b[0m         \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    387\u001b[0m             \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\n\u001b[1;32m    388\u001b[0m                 \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfunc\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m must return None or a tuple of (new_args, new_kwargs), but got \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mresult\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    389\u001b[0m             )\n\u001b[0;32m--> 391\u001b[0m out \u001b[38;5;241m=\u001b[39m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    392\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_optimizer_step_code()\n\u001b[1;32m    394\u001b[0m \u001b[38;5;66;03m# call optimizer step post hooks\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/envs/miar_tfm/lib/python3.10/site-packages/torch/optim/optimizer.py:76\u001b[0m, in \u001b[0;36m_use_grad_for_differentiable.<locals>._use_grad\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m     74\u001b[0m     torch\u001b[38;5;241m.\u001b[39mset_grad_enabled(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdefaults[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdifferentiable\u001b[39m\u001b[38;5;124m'\u001b[39m])\n\u001b[1;32m     75\u001b[0m     torch\u001b[38;5;241m.\u001b[39m_dynamo\u001b[38;5;241m.\u001b[39mgraph_break()\n\u001b[0;32m---> 76\u001b[0m     ret \u001b[38;5;241m=\u001b[39m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     77\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m     78\u001b[0m     torch\u001b[38;5;241m.\u001b[39m_dynamo\u001b[38;5;241m.\u001b[39mgraph_break()\n",
      "File \u001b[0;32m~/miniconda3/envs/miar_tfm/lib/python3.10/site-packages/torch/optim/adam.py:168\u001b[0m, in \u001b[0;36mAdam.step\u001b[0;34m(self, closure)\u001b[0m\n\u001b[1;32m    157\u001b[0m     beta1, beta2 \u001b[38;5;241m=\u001b[39m group[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mbetas\u001b[39m\u001b[38;5;124m'\u001b[39m]\n\u001b[1;32m    159\u001b[0m     has_complex \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_init_group(\n\u001b[1;32m    160\u001b[0m         group,\n\u001b[1;32m    161\u001b[0m         params_with_grad,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    165\u001b[0m         max_exp_avg_sqs,\n\u001b[1;32m    166\u001b[0m         state_steps)\n\u001b[0;32m--> 168\u001b[0m     \u001b[43madam\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    169\u001b[0m \u001b[43m        \u001b[49m\u001b[43mparams_with_grad\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    170\u001b[0m \u001b[43m        \u001b[49m\u001b[43mgrads\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    171\u001b[0m \u001b[43m        \u001b[49m\u001b[43mexp_avgs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    172\u001b[0m \u001b[43m        \u001b[49m\u001b[43mexp_avg_sqs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    173\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmax_exp_avg_sqs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    174\u001b[0m \u001b[43m        \u001b[49m\u001b[43mstate_steps\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    175\u001b[0m \u001b[43m        \u001b[49m\u001b[43mamsgrad\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mamsgrad\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    176\u001b[0m \u001b[43m        \u001b[49m\u001b[43mhas_complex\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mhas_complex\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    177\u001b[0m \u001b[43m        \u001b[49m\u001b[43mbeta1\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbeta1\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    178\u001b[0m \u001b[43m        \u001b[49m\u001b[43mbeta2\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbeta2\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    179\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlr\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mlr\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    180\u001b[0m \u001b[43m        \u001b[49m\u001b[43mweight_decay\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mweight_decay\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    181\u001b[0m \u001b[43m        \u001b[49m\u001b[43meps\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43meps\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    182\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmaximize\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mmaximize\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    183\u001b[0m \u001b[43m        \u001b[49m\u001b[43mforeach\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mforeach\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    184\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcapturable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mcapturable\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    185\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdifferentiable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mdifferentiable\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    186\u001b[0m \u001b[43m        \u001b[49m\u001b[43mfused\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mfused\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    187\u001b[0m \u001b[43m        \u001b[49m\u001b[43mgrad_scale\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mgetattr\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mgrad_scale\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    188\u001b[0m \u001b[43m        \u001b[49m\u001b[43mfound_inf\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mgetattr\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mfound_inf\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    189\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    191\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m loss\n",
      "File \u001b[0;32m~/miniconda3/envs/miar_tfm/lib/python3.10/site-packages/torch/optim/adam.py:318\u001b[0m, in \u001b[0;36madam\u001b[0;34m(params, grads, exp_avgs, exp_avg_sqs, max_exp_avg_sqs, state_steps, foreach, capturable, differentiable, fused, grad_scale, found_inf, has_complex, amsgrad, beta1, beta2, lr, weight_decay, eps, maximize)\u001b[0m\n\u001b[1;32m    315\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    316\u001b[0m     func \u001b[38;5;241m=\u001b[39m _single_tensor_adam\n\u001b[0;32m--> 318\u001b[0m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43mparams\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    319\u001b[0m \u001b[43m     \u001b[49m\u001b[43mgrads\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    320\u001b[0m \u001b[43m     \u001b[49m\u001b[43mexp_avgs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    321\u001b[0m \u001b[43m     \u001b[49m\u001b[43mexp_avg_sqs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    322\u001b[0m \u001b[43m     \u001b[49m\u001b[43mmax_exp_avg_sqs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    323\u001b[0m \u001b[43m     \u001b[49m\u001b[43mstate_steps\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    324\u001b[0m \u001b[43m     \u001b[49m\u001b[43mamsgrad\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mamsgrad\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    325\u001b[0m \u001b[43m     \u001b[49m\u001b[43mhas_complex\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mhas_complex\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    326\u001b[0m \u001b[43m     \u001b[49m\u001b[43mbeta1\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbeta1\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    327\u001b[0m \u001b[43m     \u001b[49m\u001b[43mbeta2\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbeta2\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    328\u001b[0m \u001b[43m     \u001b[49m\u001b[43mlr\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlr\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    329\u001b[0m \u001b[43m     \u001b[49m\u001b[43mweight_decay\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mweight_decay\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    330\u001b[0m \u001b[43m     \u001b[49m\u001b[43meps\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43meps\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    331\u001b[0m \u001b[43m     \u001b[49m\u001b[43mmaximize\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmaximize\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    332\u001b[0m \u001b[43m     \u001b[49m\u001b[43mcapturable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcapturable\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    333\u001b[0m \u001b[43m     \u001b[49m\u001b[43mdifferentiable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdifferentiable\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    334\u001b[0m \u001b[43m     \u001b[49m\u001b[43mgrad_scale\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgrad_scale\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    335\u001b[0m \u001b[43m     \u001b[49m\u001b[43mfound_inf\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfound_inf\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/miar_tfm/lib/python3.10/site-packages/torch/optim/adam.py:393\u001b[0m, in \u001b[0;36m_single_tensor_adam\u001b[0;34m(params, grads, exp_avgs, exp_avg_sqs, max_exp_avg_sqs, state_steps, grad_scale, found_inf, amsgrad, has_complex, beta1, beta2, lr, weight_decay, eps, maximize, capturable, differentiable)\u001b[0m\n\u001b[1;32m    390\u001b[0m     param \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mview_as_real(param)\n\u001b[1;32m    392\u001b[0m \u001b[38;5;66;03m# Decay the first and second moment running average coefficient\u001b[39;00m\n\u001b[0;32m--> 393\u001b[0m \u001b[43mexp_avg\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlerp_\u001b[49m\u001b[43m(\u001b[49m\u001b[43mgrad\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mbeta1\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    394\u001b[0m exp_avg_sq\u001b[38;5;241m.\u001b[39mmul_(beta2)\u001b[38;5;241m.\u001b[39maddcmul_(grad, grad\u001b[38;5;241m.\u001b[39mconj(), value\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m \u001b[38;5;241m-\u001b[39m beta2)\n\u001b[1;32m    396\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m capturable \u001b[38;5;129;01mor\u001b[39;00m differentiable:\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "max_steps = 250\n",
    "agent.train(init_episode=1,num_episodes=1000,max_steps=int(max_steps/dt))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "./Models15/auto-docking-iss_PPO__747_582.3387.pt\n"
     ]
    }
   ],
   "source": [
    "agent.load(\"Models15/auto-docking-iss_PPO__747_582.3387.pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Steps: 1000/1000. Action: 4. Score/Value: 910.25/4.1595. Done: 0\n",
      "Next state: tensor([ 0.5785, -0.0259,  0.0065,  0.0000,  0.0000,  0.0597])\n"
     ]
    }
   ],
   "source": [
    "agent.test(500)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "(17992) wsgi starting up on http://127.0.0.1:5555\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "(17992) accepted ('127.0.0.1', 40082)\n",
      "(17992) accepted ('127.0.0.1', 40084)\n",
      "127.0.0.1 - - [12/Aug/2024 12:20:20] \"GET / HTTP/1.1\" 304 243 0.004293\n",
      "127.0.0.1 - - [12/Aug/2024 12:20:20] \"GET /css/normalize.css HTTP/1.1\" 304 201 0.001192\n",
      "(17992) accepted ('127.0.0.1', 40092)\n",
      "(17992) accepted ('127.0.0.1', 40100)\n",
      "(17992) accepted ('127.0.0.1', 40110)\n",
      "(17992) accepted ('127.0.0.1', 40114)\n",
      "(17992) accepted ('127.0.0.1', 40122)\n",
      "(17992) accepted ('127.0.0.1', 40132)\n",
      "127.0.0.1 - - [12/Aug/2024 12:20:20] \"GET /css/layout.css HTTP/1.1\" 304 198 0.000809\n",
      "127.0.0.1 - - [12/Aug/2024 12:20:20] \"GET /js/three/three.js HTTP/1.1\" 304 196 0.000621\n",
      "127.0.0.1 - - [12/Aug/2024 12:20:20] \"GET /js/three/WebGL.js HTTP/1.1\" 304 196 0.001609\n",
      "127.0.0.1 - - [12/Aug/2024 12:20:20] \"GET /js/three/GLTFLoader.js HTTP/1.1\" 304 201 0.000556\n",
      "127.0.0.1 - - [12/Aug/2024 12:20:20] \"GET /js/three/CSS2DRenderer.js HTTP/1.1\" 304 204 0.000529\n",
      "127.0.0.1 - - [12/Aug/2024 12:20:20] \"GET /js/gsap3/gsap.js HTTP/1.1\" 304 195 0.000562\n",
      "(17992) accepted ('127.0.0.1', 40134)\n",
      "(17992) accepted ('127.0.0.1', 40148)\n",
      "127.0.0.1 - - [12/Aug/2024 12:20:20] \"GET /js/gsap3/DrawSVGPlugin.js HTTP/1.1\" 304 204 0.000613\n",
      "127.0.0.1 - - [12/Aug/2024 12:20:20] \"GET /js/gsap3/ScrambleTextPlugin.js HTTP/1.1\" 304 209 0.000489\n",
      "(17992) accepted ('127.0.0.1', 40158)\n",
      "(17992) accepted ('127.0.0.1', 40160)\n",
      "127.0.0.1 - - [12/Aug/2024 12:20:20] \"GET /img/hud-darken.png HTTP/1.1\" 304 202 0.000599\n",
      "127.0.0.1 - - [12/Aug/2024 12:20:20] \"GET /img/hud-ring.png HTTP/1.1\" 304 200 0.000641\n",
      "(17992) accepted ('127.0.0.1', 40166)\n",
      "(17992) accepted ('127.0.0.1', 40172)\n",
      "127.0.0.1 - - [12/Aug/2024 12:20:20] \"GET /img/hud-ring-inner.png HTTP/1.1\" 304 206 0.000662\n",
      "127.0.0.1 - - [12/Aug/2024 12:20:20] \"GET /img/shadow.png HTTP/1.1\" 304 198 0.000637\n",
      "(17992) accepted ('127.0.0.1', 40174)\n",
      "127.0.0.1 - - [12/Aug/2024 12:20:20] \"GET /js/threed.js HTTP/1.1\" 304 197 0.000993\n",
      "(17992) accepted ('127.0.0.1', 40186)\n",
      "127.0.0.1 - - [12/Aug/2024 12:20:20] \"GET /img/instructions/step1_6.jpg HTTP/1.1\" 304 199 0.001227\n",
      "(17992) accepted ('127.0.0.1', 40202)\n",
      "(17992) accepted ('127.0.0.1', 40216)\n",
      "(17992) accepted ('127.0.0.1', 40222)\n",
      "(17992) accepted ('127.0.0.1', 40232)\n",
      "(17992) accepted ('127.0.0.1', 40236)\n",
      "127.0.0.1 - - [12/Aug/2024 12:20:20] \"GET /img/instructions/step2.jpg HTTP/1.1\" 304 197 0.000542\n",
      "127.0.0.1 - - [12/Aug/2024 12:20:20] \"GET /img/instructions/step3.jpg HTTP/1.1\" 304 197 0.000410\n",
      "127.0.0.1 - - [12/Aug/2024 12:20:20] \"GET /img/instructions/step4.jpg HTTP/1.1\" 304 197 0.000402\n",
      "127.0.0.1 - - [12/Aug/2024 12:20:20] \"GET /img/instructions/step5.jpg HTTP/1.1\" 304 197 0.000481\n",
      "127.0.0.1 - - [12/Aug/2024 12:20:20] \"GET /img/instructions/step7.jpg HTTP/1.1\" 304 197 0.000395\n",
      "(17992) accepted ('127.0.0.1', 40242)\n",
      "127.0.0.1 - - [12/Aug/2024 12:20:20] \"GET /socket.io/?EIO=4&transport=polling&t=P566cGw HTTP/1.1\" 200 278 0.000598\n",
      "127.0.0.1 - - [12/Aug/2024 12:20:20] \"GET /img/hud/ForwardViewSprites.json HTTP/1.1\" 304 211 0.000956\n",
      "(17992) accepted ('127.0.0.1', 40248)\n",
      "127.0.0.1 - - [12/Aug/2024 12:20:20] \"GET /3d/iss.glb HTTP/1.1\" 304 195 0.000810\n",
      "(17992) accepted ('127.0.0.1', 40258)\n",
      "127.0.0.1 - - [12/Aug/2024 12:20:20] \"GET /img/earth.jpg HTTP/1.1\" 304 197 0.000874\n",
      "(17992) accepted ('127.0.0.1', 40260)\n",
      "(17992) accepted ('127.0.0.1', 40266)\n",
      "(17992) accepted ('127.0.0.1', 40272)\n",
      "(17992) accepted ('127.0.0.1', 40280)\n",
      "127.0.0.1 - - [12/Aug/2024 12:20:20] \"GET /img/navball.png HTTP/1.1\" 304 199 0.002620\n",
      "127.0.0.1 - - [12/Aug/2024 12:20:20] \"GET /img/texture_fire.jpg HTTP/1.1\" 304 204 0.000977\n",
      "127.0.0.1 - - [12/Aug/2024 12:20:20] \"GET /img/texture_wormhole.jpg HTTP/1.1\" 304 208 0.000733\n",
      "127.0.0.1 - - [12/Aug/2024 12:20:20] \"GET /img/texture_star.jpg HTTP/1.1\" 304 204 0.000680\n",
      "(17992) accepted ('127.0.0.1', 40288)\n",
      "127.0.0.1 - - [12/Aug/2024 12:20:20] \"GET /img/hud/ForwardViewSprites2.png HTTP/1.1\" 304 211 0.000638\n",
      "(17992) accepted ('127.0.0.1', 40296)\n",
      "127.0.0.1 - - [12/Aug/2024 12:20:20] \"POST /socket.io/?EIO=4&transport=polling&t=P566cLK&sid=ouW20LrKSSUlpi96AAAA HTTP/1.1\" 200 219 0.000831\n",
      "(17992) accepted ('127.0.0.1', 40310)\n",
      "(17992) accepted ('127.0.0.1', 40322)\n",
      "127.0.0.1 - - [12/Aug/2024 12:20:20] \"GET /socket.io/?EIO=4&transport=polling&t=P566cLL&sid=ouW20LrKSSUlpi96AAAA HTTP/1.1\" 200 181 0.000160\n",
      "127.0.0.1 - - [12/Aug/2024 12:20:20] \"GET /socket.io/?EIO=4&transport=polling&t=P566cPn&sid=ouW20LrKSSUlpi96AAAA HTTP/1.1\" 200 181 0.000202\n",
      "127.0.0.1 - - [12/Aug/2024 12:20:21] \"GET /socket.io/?EIO=4&transport=polling&t=P566cQU&sid=ouW20LrKSSUlpi96AAAA HTTP/1.1\" 200 181 0.000227\n",
      "127.0.0.1 - - [12/Aug/2024 12:20:21] \"GET /img/favicon.ico HTTP/1.1\" 304 199 0.000665\n",
      "127.0.0.1 - - [12/Aug/2024 12:20:21] \"GET /img/target-iss.png HTTP/1.1\" 304 202 0.001158\n"
     ]
    }
   ],
   "source": [
    "env2 = env6.Env(port=5555)\n",
    "agent2 = PPO(env=env2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "./Models15/auto-docking-iss_PPO__545_410.917.pt\n",
      "Error(s) in loading state_dict for PolicyNetwork:\n",
      "\tUnexpected key(s) in state_dict: \"liquid_net.sensory_mu\", \"liquid_net.sensory_sigma\", \"liquid_net.sensory_W\", \"liquid_net.sensory_erev\", \"liquid_net.mu\", \"liquid_net.sigma\", \"liquid_net.W\", \"liquid_net.erev\", \"liquid_net.vleak\", \"liquid_net.gleak\", \"liquid_net.cm_t\". \n"
     ]
    }
   ],
   "source": [
    "agent2.load(\"Models15/auto-docking-iss_PPO__545_410.917.pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "agent2.test(max_steps=500,dt=0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = torch.load(f'./{\"Models15/auto-docking-iss_PPO__545_410.917.pt\"}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "odict_keys(['fc1.weight', 'fc1.bias', 'fc2.weight', 'fc2.bias', 'liquid_net.sensory_mu', 'liquid_net.sensory_sigma', 'liquid_net.sensory_W', 'liquid_net.sensory_erev', 'liquid_net.mu', 'liquid_net.sigma', 'liquid_net.W', 'liquid_net.erev', 'liquid_net.vleak', 'liquid_net.gleak', 'liquid_net.cm_t', 'fc3.weight', 'fc3.bias'])\n"
     ]
    }
   ],
   "source": [
    "# print(data['actor']['fc1.weight'])\n",
    "\n",
    "agent.policy_net.fc1.weight.data = data['actor']['fc1.weight']\n",
    "agent.policy_net.fc1.bias.data = data['actor']['fc1.bias']\n",
    "agent.policy_net.fc2.weight.data = data['actor']['fc2.weight']\n",
    "agent.policy_net.fc2.bias.data = data['actor']['fc2.bias']\n",
    "agent.policy_net.fc3.weight.data = data['actor']['fc3.weight']\n",
    "agent.policy_net.fc3.bias.data = data['actor']['fc3.bias']\n",
    "\n",
    "agent.policy_net.liquid_net.sensory_mu = torch.nn.Parameter(data['actor']['liquid_net.sensory_mu'])\n",
    "agent.policy_net.liquid_net.sensory_sigma = torch.nn.Parameter(data['actor']['liquid_net.sensory_sigma'])\n",
    "agent.policy_net.liquid_net.sensory_W = torch.nn.Parameter(data['actor']['liquid_net.sensory_W'])\n",
    "agent.policy_net.liquid_net.sensory_erev = torch.nn.Parameter(data['actor']['liquid_net.sensory_erev'])\n",
    "agent.policy_net.liquid_net.mu = torch.nn.Parameter(data['actor']['liquid_net.mu'])\n",
    "agent.policy_net.liquid_net.sigma = torch.nn.Parameter(data['actor']['liquid_net.sigma'])\n",
    "agent.policy_net.liquid_net.W = torch.nn.Parameter(data['actor']['liquid_net.W'])\n",
    "agent.policy_net.liquid_net.erev = torch.nn.Parameter(data['actor']['liquid_net.erev'])\n",
    "agent.policy_net.liquid_net.vleak = torch.nn.Parameter(data['actor']['liquid_net.vleak'])\n",
    "agent.policy_net.liquid_net.gleak = torch.nn.Parameter(data['actor']['liquid_net.gleak'])\n",
    "agent.policy_net.liquid_net.cm_t = torch.nn.Parameter(data['actor']['liquid_net.cm_t'])\n",
    "\n",
    "\n",
    "#data['actor']['liquid_net.sensory_mu']\n",
    "\n",
    "# data['actor']['liquid_net.sensory_mu']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Parameter containing:\n",
       "tensor([[0.5646, 0.7707, 0.4789,  ..., 0.7558, 0.4982, 0.3348],\n",
       "        [0.4882, 0.7766, 0.6540,  ..., 0.6880, 0.5127, 0.4809],\n",
       "        [0.6894, 0.3052, 0.6115,  ..., 0.5713, 0.3007, 0.3115],\n",
       "        ...,\n",
       "        [0.3324, 0.7808, 0.7234,  ..., 0.3549, 0.6742, 0.5760],\n",
       "        [0.4970, 0.7467, 0.5483,  ..., 0.7669, 0.4636, 0.3357],\n",
       "        [0.3701, 0.5748, 0.7015,  ..., 0.6184, 0.3825, 0.4185]],\n",
       "       requires_grad=True)"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "agent.policy_net.liquid_net.sensory_mu"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "miar_tfm",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
